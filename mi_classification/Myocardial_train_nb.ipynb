{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saranpan/ECG-Heartbeat-classifier/blob/main/mi_classification/Myocardial_train_nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hardware will be used on this notebook\n",
        "\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r1ZHd5FtTdH",
        "outputId": "248618c8-3837-4b47-e2e8-58555b760007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-98533401-ca0b-ecf0-2601-7f3c57b9f287)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🌏 Business Understanding "
      ],
      "metadata": {
        "id": "3m1HhrJVux7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The healthcare industry is experiencing significant growth with the integration of advanced technologies to improve patient outcomes and ease the burden on doctors. One of the most important technologies is the electrocardiogram (ECG), which can accurately check the electrical activity and rhythm of the heart. Early detection and diagnosis of myocardial infarction (MI)  (กล้ามเนื้อหัวใจตาย) is crucial to prevent life-threatening complications.\n",
        "\n",
        "Myocardial infarction occurs when there is a sudden interruption of blood supply to the heart, which can cause serious damage to the heart muscle. Identifying patients with MI requires timely and accurate diagnosis, which can be challenging for doctors.\n",
        "\n",
        "To overcome this challenge, we aim to develop a product that uses AI to classify ECG signals as either indicative of MI or not. The goal is to accurately identify patients with MI and provide appropriate treatment. Our model will be trained on a labeled dataset of ECG signals and corresponding MI classifications, and the performance of the model will be evaluated using relevant metrics such as weighted F1 score.\n",
        "\n",
        "In summary, our goal is to develop an AI-based solution that can accurately classify ECG signals to determine the presence of MI. This will aid in timely and accurate diagnosis, resulting in improved patient outcomes."
      ],
      "metadata": {
        "id": "JfheQVPqvB38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📰 Data Understanding"
      ],
      "metadata": {
        "id": "XXUqnd2QL-VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset for this project consists of ECG heartbeat signals, which will be used as input to train the machine learning model. The dataset includes labeled data, with each ECG heartbeat signal corresponding to either the presence of myocardial infarction (MI) or not. This task is a binary classification problem, where the model will predict whether a patient has MI or not.\n",
        "\n",
        "The objective of this project is to develop a machine learning approach that accurately classifies ECG signals as indicative of MI or not, which can aid in the early detection and diagnosis of MI. The labeled dataset of ECG signals and corresponding MI classifications will be used to train the machine learning model. The model's performance will be evaluated using the binary F1 score as the criterion for selecting the best model.\n",
        "\n",
        "In summary, the data understanding phase of this project involves analyzing the labeled dataset of ECG signals and corresponding MI classifications. The goal is to develop a machine learning model that accurately classifies ECG signals as indicative of MI or not, which can aid in the early detection and diagnosis of MI. The binary F1 score will be used to evaluate the model's performance.\n"
      ],
      "metadata": {
        "id": "pdwqkU0aMAu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To develop a machine learning model for ECG analysis, we needed to collect a dataset with ECG heartbeat signals as predictors and binary labels indicating the presence or absence of Myocardial Infarction as the target. One such dataset we found on [Kaggle](https://www.kaggle.com/datasets/shayanfazeli/heartbeat) was scraped from a [research paper](https://arxiv.org/abs/1805.00794) , which in turn was scraped from the [PTB Diagnostic ECG Database](https://www.physionet.org/content/ptbdb/1.0.0/). However, the dataset was downsampled, resulting in a reduced number of rows. It is important to note that the trained model cannot be applied universally as it has limitations arising from the dataset used to train it. Therefore, it is worth reviewing some metadata of the PTB Diagnostic ECG Database, which we used to train the model.\n",
        "\n",
        "The summary of the limitations of this dataset are as follows:\n",
        "\n",
        "> Small sample size: The dataset has only 549 records from 290 subjects, which may not be representative of the general population.\n",
        "\n",
        "> Imbalanced classes: The dataset is imbalanced, with the majority of records being from patients with Myocardial Infarction, which may affect the performance of machine learning algorithms in detecting other heart conditions.\n",
        "\n",
        "> Limited clinical information: Although the dataset includes a detailed clinical summary for most records, it is not available for 22 subjects. Additionally, the dataset does not include information on the severity of the heart conditions or other factors that may impact the diagnosis.\n",
        "\n",
        "> Limited recording conditions: The ECG recordings were obtained using a non-commercial, PTB prototype recorder with specific specifications, which may not be representative of other recording conditions. Additionally, the dataset only includes recordings at a sampling rate of 1000 samples per second, which may not capture all relevant information in the ECG signal.\n",
        "\n",
        "> Limited diversity: The dataset was collected from patients in Berlin, Germany, and may not be representative of other populations or geographic regions. Additionally, the dataset has a majority of male participants, which may impact the generalizability of the results to female populations.\n",
        "\n",
        "Although the PTB Diagnostic ECG Database is a valuable resource for ECG analysis research and algorithmic benchmarking, the limitations of the dataset must be considered when interpreting the results.\n",
        "\n",
        "Moreover, it is essential to understand the limitations of the model's inference. For instance, the model may not accurately diagnose heart conditions for patients in 2023 as the dataset used to train the model may not include recent cases of heart conditions and may not be representative of the current population. Additionally, the model's performance may be affected by factors such as the quality of the ECG signal, variations in recording conditions, and limitations in the algorithms used for analysis. Therefore, caution should be exercised when applying the model's results to real-world scenarios, considering both the dataset and the model's limitations."
      ],
      "metadata": {
        "id": "CufCg_42woHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moreover, the data from the [PTB Diagnostic ECG Database](https://www.physionet.org/content/ptbdb/1.0.0/) is not exactly the same used for modeling in the [research paper](https://arxiv.org/abs/1805.00794), the author of paper have preprocessed in the following steps\n",
        "\n",
        "1. Splitting the continuous ECG signal to 10s windows and select a 10s window from an ECG signal. \n",
        "\n",
        "2. Normalizing the amplitude values to the range of between zero and one. \n",
        "\n",
        "3. Finding the set of all local maximums based on zerocrossings of the first derivative. \n",
        "\n",
        "4. Finding the set of ECG R-peak candidates by applying a threshold of 0.9 on the normalized value of the local maximums. \n",
        "\n",
        "5. Finding the median of R-R time intervals as the nominal heartbeat period of that window (T). \n",
        "\n",
        "6. For each R-peak, selecting a signal part with the length equal to 1.2T. \n",
        "\n",
        "7. Padding each selected part with zeros to make its length equal to a predefined fixed length."
      ],
      "metadata": {
        "id": "oAMplj4CU3yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the whole dataset is approximately 10,000 rows, That is an overview of the metadata for this dataset. Moving forward, we will import the dataset and conduct exploratory analysis to gain a more comprehensive understanding of its contents."
      ],
      "metadata": {
        "id": "aCjRtQAF0QTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The method conducting in this notebook is similar to [Arhythmia](https://colab.research.google.com/drive/1rqp2jdPIHLjDu3xOC1Clc5S9Xb5D4BtP?usp=sharing)). The only things are \n",
        "\n",
        "1. multi-class classification -> binary classification\n",
        "2. performing threshold optimization"
      ],
      "metadata": {
        "id": "g7VXqOhNoLVK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2oSa4se9Wlv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install kora kaggle # API for import dataset from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NpKVD8GzDEFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd29e2d-cf01-4ee3-b254-60594472601f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print('setup libraries completed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJxdmnsbDa_c",
        "outputId": "962c7d2e-b011-48d5-ea43-eaf2903c3e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setup libraries completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kora import kaggle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def import_kaggle_dataset():\n",
        "    # If failed, download Kaggle API to your drive first\n",
        "    kaggle.download('shayanfazeli/heartbeat')\n",
        "    print('The Kaggle Dataset was downloaded into your directory')\n",
        "\n",
        "def create_myocardial_infarction_dataset(seed=42):\n",
        "    negative = pd.read_csv('/content/ptbdb_normal.csv', header=None)\n",
        "    positive = pd.read_csv('/content/ptbdb_abnormal.csv', header=None)\n",
        "\n",
        "    # split the negative and positive datasets into training and validation sets separately\n",
        "    neg_train, neg_test = train_test_split(negative, test_size=0.2, random_state=seed)\n",
        "    pos_train, pos_test = train_test_split(positive, test_size=0.2, random_state=seed)\n",
        "\n",
        "    # concatenate the training and validation sets\n",
        "    train = pd.concat([neg_train,pos_train])\n",
        "    test = pd.concat([neg_test,pos_test])\n",
        "\n",
        "    train.rename(columns={187: \"class\"}, inplace=True)\n",
        "    test.rename(columns={187: \"class\"}, inplace=True)\n",
        "\n",
        "    train['class'] = train['class'].astype(int)\n",
        "    test['class'] = test['class'].astype(int)\n",
        "\n",
        "    # return the training and validation sets\n",
        "    return train, test\n",
        "\n",
        "def check_null(df):\n",
        "    if df.isnull().values.any():\n",
        "        print(\"The dataset contains null values\")\n",
        "    else:\n",
        "        print(\"The dataset contains NO null values\")\n",
        "\n",
        "def get_proportion_train_test(train,test):\n",
        "    total = len(train) + len(test)\n",
        "\n",
        "    train_prop = len(train) / total \n",
        "    test_prop = len(test) / total \n",
        "\n",
        "    return f'train:test proportion = {train_prop:.2f} : {test_prop:.2f}'"
      ],
      "metadata": {
        "id": "UvNQqDxQ_TWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import_kaggle_dataset()\n",
        "#  <- Check the data files in your directory\n",
        "train, test = create_myocardial_infarction_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F5k34uX57Wo",
        "outputId": "a31843c6-fdf6-4b42-b05a-49f6451ca212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading heartbeat.zip to /content\n",
            "100% 98.8M/98.8M [00:04<00:00, 24.4MB/s]\n",
            "100% 98.8M/98.8M [00:04<00:00, 23.3MB/s]\n",
            "The Kaggle Dataset was downloaded into your directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( f' Arrhythmia : {get_proportion_train_test(train,test)}' )"
      ],
      "metadata": {
        "id": "Lcn_zmAHfsJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc77c1e6-0517-4c26-dfde-c35c007efd08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Arrhythmia : train:test proportion = 0.80 : 0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check null\n",
        "check_null(train)\n",
        "check_null(test)"
      ],
      "metadata": {
        "id": "zviUvjpTDUdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5654069c-d45d-44ee-9e0f-4337e6f88511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset contains NO null values\n",
            "The dataset contains NO null values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Originally, Only train and test dataset were already splitted by kaggle, \n",
        "# we need to further split train into train and val where the size is equal to test\n",
        "# goal : train-test : 80-20 into train-val-test : 60-20-20\n",
        "# since 25% of 80% is 20%, hence, we will split the train into train-val by 75:25\n",
        "\n",
        "seed = 42\n",
        "train, val = train_test_split(train, test_size=0.25, random_state=seed, stratify=train['class'])"
      ],
      "metadata": {
        "id": "QkqIHiRpGPpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_class(train,val,title='Value counts for each class of ..'):\n",
        "    # Create a bar plot for train and val and test set for each class\n",
        "    train_value_counts = train['class'].value_counts()\n",
        "    val_value_counts = val['class'].value_counts()\n",
        "    test_value_counts = test['class'].value_counts()\n",
        "\n",
        "    fig = go.Figure(go.Bar(\n",
        "                x=train_value_counts.index,\n",
        "                y=train_value_counts.values,\n",
        "                text=train_value_counts.values,\n",
        "                textposition='auto',\n",
        "                name='Train Set'\n",
        "            ))\n",
        "    \n",
        "    fig.add_trace(go.Bar(\n",
        "            x=val_value_counts.index,\n",
        "            y=val_value_counts.values,\n",
        "            text=val_value_counts.values,\n",
        "            textposition='auto',\n",
        "            name='Validation Set'\n",
        "        ))\n",
        "    \n",
        "    fig.add_trace(go.Bar(\n",
        "            x=test_value_counts.index,\n",
        "            y=test_value_counts.values,\n",
        "            text=test_value_counts.values,\n",
        "            textposition='auto',\n",
        "            name='Test Set'\n",
        "        ))\n",
        "    \n",
        "    # Set plot title and axis labels\n",
        "    fig.update_layout(title=title,\n",
        "                    xaxis_title=\"Class\",\n",
        "                    yaxis_title=\"Count\")\n",
        "\n",
        "    # Display the plot\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "roT_YDaHDRbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_class(train,test, title='Value counts for each class  in train/test set of PTBDB dataset')"
      ],
      "metadata": {
        "id": "p_Hfn7PLD6Ts",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "22cf1d07-8699-49e1-b238-0df2facde103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"e0e40bc0-91ac-4935-b8b3-57c48326d73e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e0e40bc0-91ac-4935-b8b3-57c48326d73e\")) {                    Plotly.newPlot(                        \"e0e40bc0-91ac-4935-b8b3-57c48326d73e\",                        [{\"name\":\"Train Set\",\"text\":[6303.0,2427.0],\"textposition\":\"auto\",\"x\":[1,0],\"y\":[6303,2427],\"type\":\"bar\"},{\"name\":\"Validation Set\",\"text\":[2102.0,810.0],\"textposition\":\"auto\",\"x\":[1,0],\"y\":[2102,810],\"type\":\"bar\"},{\"name\":\"Test Set\",\"text\":[2102.0,810.0],\"textposition\":\"auto\",\"x\":[1,0],\"y\":[2102,810],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Value counts for each class  in train/test set of PTBDB dataset\"},\"xaxis\":{\"title\":{\"text\":\"Class\"}},\"yaxis\":{\"title\":{\"text\":\"Count\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e0e40bc0-91ac-4935-b8b3-57c48326d73e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The minority class with no Myocardial Infarction {2427 / 6303 : .2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-fCpyDYp7u6",
        "outputId": "5fe4520b-4ed7-4869-dd0e-c642b81b1c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minority class with no Myocardial Infarction  0.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, it's an imbalanced dataset, this will lead us to be more carefully on dealing with them\n",
        "\n",
        "In this notebook, I (Run) will use weighted binary cross-entropy over plain binary cross-entropy which treat every class with the same weight"
      ],
      "metadata": {
        "id": "M4K9SCqjqfOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we wouldl like to observe the random heartbeat of each class of myocardial infarction"
      ],
      "metadata": {
        "id": "c_S5pv4p5nzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "def plot_individual_class(df):\n",
        "\n",
        "    # randomly sample one row per class\n",
        "    df_sampled = df.groupby('class').apply(lambda x: x.sample(n=1)).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    # melt the dataframe to convert it from wide to long format\n",
        "    df_melted = df_sampled.melt(id_vars='class', var_name='timestep')\n",
        "\n",
        "    # create the line plot with Plotly\n",
        "    fig = px.line(df_melted, x='timestep', y='value', color='class')\n",
        "\n",
        "    # update the layout of the plot\n",
        "    fig.update_layout(title='1-beat ECG for patient with Myocardial Infarction (1) and not (0)',\n",
        "                    xaxis_title='Timestep',\n",
        "                    yaxis_title='Normalized value')\n",
        "\n",
        "    # display the plot\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "GcBhDZL0RG0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_individual_class(train)\n",
        "\n",
        "# Re-run to see new random observations for each class "
      ],
      "metadata": {
        "id": "TLgFgWjSRa4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "218771e6-cdb0-4aa2-cb72-9b3cf01e381b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"a14718b2-7db0-4fc8-aaa9-a8362b9bde7b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a14718b2-7db0-4fc8-aaa9-a8362b9bde7b\")) {                    Plotly.newPlot(                        \"a14718b2-7db0-4fc8-aaa9-a8362b9bde7b\",                        [{\"hovertemplate\":\"class=0<br>timestep=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186],\"xaxis\":\"x\",\"y\":[1.0,0.5126935243606567,0.2130030989646912,0.0,0.040866874158382416,0.21981424093246457,0.2501547932624817,0.26377707719802856,0.29721361398696894,0.3065015375614166,0.31083592772483826,0.3151702880859375,0.3300309479236603,0.3343653380870819,0.3263157904148102,0.34365326166152954,0.34551084041595453,0.35913312435150146,0.3634674847126007,0.3678018450737,0.38452011346817017,0.4068111479282379,0.43777090311050415,0.46563467383384705,0.5071207284927368,0.5492259860038757,0.5913312435150145,0.6235294342041016,0.6489164233207704,0.6600618958473206,0.6594427227973938,0.6328173279762268,0.575232207775116,0.5108358860015869,0.4383901059627533,0.38513931632041926,0.3417956531047821,0.30959752202034,0.2934984564781189,0.2873064875602722,0.2718266248703003,0.27368420362472534,0.27616098523139954,0.2780185639858246,0.2705882489681244,0.27120742201805115,0.26996904611587524,0.26996904611587524,0.26996904611587524,0.27368420362472534,0.26501548290252686,0.27120742201805115,0.27616098523139954,0.2767801880836487,0.2693498432636261,0.2804953455924988,0.28854489326477045,0.28421053290367126,0.2947368323802948,0.2848297357559204,0.2860681116580963,0.2860681116580963,0.2860681116580963,0.2804953455924988,0.28421053290367126,0.2718266248703003,0.2687306404113769,0.27492260932922363,0.27120742201805115,0.26749226450920105,0.27120742201805115,0.27368420362472534,0.2755417823791504,0.26625385880470276,0.26996904611587524,0.26749226450920105,0.2668730616569519,0.26501548290252686,0.27244582772254944,0.27368420362472534,0.27863776683807373,0.2718266248703003,0.26253870129585266,0.2613002955913544,0.2811145484447479,0.2928792536258697,0.31950464844703674,0.32693499326705927,0.31826624274253845,0.2922600507736206,0.3052631616592407,0.2922600507736206,0.3034055829048157,0.28421053290367126,0.2743034064769745,0.2613002955913544,0.27306500077247614,0.2792569696903229,0.2792569696903229,0.2860681116580963,0.29721361398696894,0.28668731451034546,0.2947368323802948,0.3250773847103119,0.43900927901268005,0.4687306582927704,0.6352941393852234,0.9832817316055296,0.7857584953308105,0.3993808031082153,0.20371517539024356,0.04148606956005096,0.19195047020912168,0.311455100774765,0.3226006329059601,0.352321982383728,0.37275540828704834,0.38452011346817017,0.3913312554359436,0.3863777220249176,0.3981424272060394,0.4068111479282379,0.4061919450759888,0.41795665025711054,0.4235294163227081,0.4291021525859833,0.4315789341926575,0.46130031347274786,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"class=1<br>timestep=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"1\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186],\"xaxis\":\"x\",\"y\":[0.9611901640892029,0.4463130533695221,0.010349288582801819,0.0,0.11772315949201584,0.06338939070701599,0.11772315949201584,0.10090555995702744,0.10866753011941908,0.11513583362102509,0.13324709236621857,0.13971538841724396,0.13454075157642365,0.06727037578821182,0.1371280699968338,0.14747735857963562,0.16300129890441895,0.13842172920703888,0.16429495811462402,0.21345406770706177,0.23156532645225525,0.20051746070384976,0.2289780080318451,0.21992237865924835,0.2858991026878357,0.24320827424526212,0.27813711762428284,0.2846054434776306,0.19534282386302948,0.24320827424526212,0.17852522432804108,0.2069857716560364,0.19922380149364471,0.15912030637264252,0.20957309007644653,0.18240621685981753,0.16688227653503415,0.16300129890441895,0.1940491646528244,0.17981888353824615,0.23156532645225525,0.18240621685981753,0.20181111991405487,0.2522639036178589,0.19793014228343964,0.2341526448726654,0.20957309007644653,0.177231565117836,0.2121604084968567,0.21345406770706177,0.24967658519744876,0.2341526448726654,0.1849935352802277,0.19275550544261927,0.22509703040122983,0.19534282386302948,0.17593790590763092,0.23156532645225525,0.19146183133125305,0.21474774181842804,0.19146183133125305,0.2108667492866516,0.22250969707965848,0.37774902582168574,0.3221215903759002,0.37386804819107056,0.4566623568534851,0.37774902582168574,0.4838292300701141,0.3712807297706604,0.31824061274528503,0.33505821228027344,0.23803363740444183,0.1655886173248291,0.10349288582801817,0.09573091566562653,0.11254851520061492,0.107373870909214,0.0802069827914238,0.09702458232641219,0.05950840935111046,0.07115136086940765,0.11125484853982924,0.18240621685981753,0.2988356947898865,0.47865459322929377,0.8111254572868347,1.0,0.508408784866333,0.12160413712263109,0.042690813541412354,0.17852522432804108,0.16429495811462402,0.14747735857963562,0.16429495811462402,0.20957309007644653,0.1849935352802277,0.17852522432804108,0.18887451291084287,0.20957309007644653,0.2238033562898636,0.19146183133125305,0.22121603786945343,0.23803363740444183,0.28072443604469294,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Timestep\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Normalized value\"}},\"legend\":{\"title\":{\"text\":\"class\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"1-beat ECG for patient with Myocardial Infarction (1) and not (0)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a14718b2-7db0-4fc8-aaa9-a8362b9bde7b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like this dataset was zero padded until timestep 187.\n",
        "\n",
        "Let's check if any beat is fully completed in recording to timestep 187"
      ],
      "metadata": {
        "id": "ppxKBjFZra6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[train[186] != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "fuAfL7hmq3kz",
        "outputId": "50fd4d55-cc19-4e87-ab33-b84e7e3468ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (188) exceeds max_columns (20). Falling back to pandas display.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 188 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0116a69-4b27-4edc-a1c8-5c26f564c31c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 188 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0116a69-4b27-4edc-a1c8-5c26f564c31c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a0116a69-4b27-4edc-a1c8-5c26f564c31c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a0116a69-4b27-4edc-a1c8-5c26f564c31c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val[val[186] != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "7D1RBynlq9Ij",
        "outputId": "f6dd41b2-fe56-47f4-a387-0367b6f51b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (188) exceeds max_columns (20). Falling back to pandas display.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 188 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a51cf43b-c150-4c6e-8651-32d6e1c751ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 188 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a51cf43b-c150-4c6e-8651-32d6e1c751ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a51cf43b-c150-4c6e-8651-32d6e1c751ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a51cf43b-c150-4c6e-8651-32d6e1c751ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test[test[186] != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "icyRtcFdq-v6",
        "outputId": "a4739ebf-b720-43b4-f11c-33a25b2aae3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (188) exceeds max_columns (20). Falling back to pandas display.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 188 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5bbe3fd1-cec4-420f-9ee4-5d6ffc57ca2d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 188 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5bbe3fd1-cec4-420f-9ee4-5d6ffc57ca2d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5bbe3fd1-cec4-420f-9ee4-5d6ffc57ca2d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5bbe3fd1-cec4-420f-9ee4-5d6ffc57ca2d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, we do not see any beats reach timestep 187, then why do they zero-padding? This is because the author of the paper aims to do transfer learning from the arhythmic which have the maximum recording at timestep 187.\n",
        "\n",
        "Before using this dataset let's truncate into the max length for just this task and create new dataset "
      ],
      "metadata": {
        "id": "gqfFsyKSrxSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the max length\n",
        "\n",
        "for col in reversed(range(187)):\n",
        "    train_len = len(train[train[col] != 0])\n",
        "    val_len = len(val[val[col] != 0])\n",
        "    test_len = len(test[test[col] != 0])\n",
        "\n",
        "    if any([train_len, val_len, test_len]):\n",
        "        print(f'last timestep of MI. dataset is {col}')\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHrdSSg_sS_b",
        "outputId": "ec9d78cc-15df-449a-f909-736d44f3aa3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last timestep of MI. dataset is 185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, the max length of MI. dataset is 186 timestep\n",
        "\n",
        "Although, this is not significantly different much, and adding one more zero padding should not hurt performance. When we try the architecture which is not from the pretrained model of Arhythmic. We won't truncate"
      ],
      "metadata": {
        "id": "EeTZUNXTtTk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔍 Data Preparation"
      ],
      "metadata": {
        "id": "K6-dqxuYPOVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before doing anything, it's good to set deteministic seed, so, we can reproduce the result\n",
        "\n",
        "Although, it's not possible to fully reproduce, but we can minimize the variance of the result"
      ],
      "metadata": {
        "id": "Vb86wdQQ0lsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set up reproducible seed\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "ckfHRynt0k38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c269ab78773e4b5a960a5e55d2b48c53d5f9c446",
        "id": "Qgn4MREd5R73"
      },
      "cell_type": "markdown",
      "source": [
        "## Data augmentation\n",
        "\n",
        "Since I'm not currently an domain expert about heartbeat signal, I'm not confident enough to do data augmentation. I think there will be many source discussed about this, we will further read in the future"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip"
      ],
      "metadata": {
        "id": "3lsE3v6_FClA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Torch Dataset\n",
        "\n",
        "In this notebook, I (Run) decided to use Pytorch Framework, so, we need to transform train, val, test into torch dataset object"
      ],
      "metadata": {
        "id": "kY4AT_W6DVpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas to numpy \n",
        "\n",
        "X_train = train.values[:, :-1] \n",
        "y_train = train.values[:, -1].astype(int)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "X_val = val.values[:, :-1]\n",
        "y_val = val.values[:, -1].astype(int)\n",
        "y_val = y_val.reshape(-1, 1)\n",
        "\n",
        "X_test = test.values[:, :-1]\n",
        "y_test = test.values[:, -1].astype(int)\n",
        "y_test = y_test.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "F_NP4Zee_Dk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MI_Dataset(Dataset):\n",
        "    def __init__(self, X, y, truncate_to_max = False, transforms=None):\n",
        "        \"\"\"\n",
        "        truncate_to_max == True : will truncate the X shape from 187 into 186\n",
        "        \"\"\"\n",
        "        assert y.shape[1] == 1\n",
        "        assert X.shape[1] == 187\n",
        "        assert X.ndim == 2\n",
        "                \n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.transforms = transforms\n",
        "        self.truncate_to_max = False\n",
        "        self._process()\n",
        "\n",
        "    def _process(self):\n",
        "        if self.truncate_to_max:\n",
        "            self.X = self.X[:,:186]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        X = torch.tensor( self.X[index] , dtype=torch.float32)\n",
        "        X = X.unsqueeze(0)\n",
        "        y = torch.tensor( self.y[index] , dtype = torch.float32)#.long()\n",
        "\n",
        "        if self.transforms:\n",
        "            X = self.transforms(X)\n",
        "\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "aMmHjwbdDkQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "\n",
        "train_set = MI_Dataset(X_train,y_train)\n",
        "val_set = MI_Dataset(X_val,y_val)\n",
        "test_set = MI_Dataset(X_test,y_test)"
      ],
      "metadata": {
        "id": "HuggPz76ER6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxxmn6FIvPy6",
        "outputId": "eb283fc4-2da9-416a-a89f-2a91d6c1d916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DON\"T forget threshold optimization"
      ],
      "metadata": {
        "id": "gJQpYfLWi_PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As well as dataloader, since we decided to do mini-batches GD. in order to overcome the out-of-memory issue in RAM. Batch_size for val_set is arbitrary, but cannot be full-batch size, otherwise, it will be out-of-memory \n",
        "\n",
        "\n",
        "Doing Mini-batch also make a set of parameter of the model converges faster, and generalize better than batch GD."
      ],
      "metadata": {
        "id": "Hnz-ElUalGyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_loader(train_set,val_set,test_set,train_batch_size = 256):\n",
        "    \"\"\"\n",
        "    Return train_loader and val_loader given wandb.config\n",
        "    !Warning : Make sure to declare p_train, p_val as global variable before\n",
        "\n",
        "    \"\"\"\n",
        "    # Dependence Config : architecture, batch_size, transform\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=1024, shuffle=False) # Val set should not be fine tuned, since they always give the same score regardless the batch size\n",
        "    test_loader = DataLoader(test_set, batch_size=1024, shuffle=False) # Val set should not be fine tuned, since they always give the same score regardless the batch size\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "jN0G5ObRHMJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = get_loader(train_set,val_set,test_set,train_batch_size = 256) # Train batch_size"
      ],
      "metadata": {
        "id": "jAtdOvUxH2HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 👉 Modelling"
      ],
      "metadata": {
        "id": "0HlLtpOrP0B4"
      }
    },
    {
      "metadata": {
        "_uuid": "c4de23b85abe34a726eab268171da0e827bafa35",
        "id": "IPqcx5xa5R76"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup all architecture\n",
        "\n",
        "Although sequence is not arbitrary due to zero padding, but order still need to be preserved, so we will give up on non-sequence model due to bad representation of human intelligence. Hence, all candidates we choose will be the most common architectures that are appropriate with our task; sequence model\n",
        "\n",
        "I would like to divide categories of architecture into 2 groups: RNN-based and CNN-based\n",
        "\n",
        "RNN-based is the default choice for sequence data,\n",
        "\n",
        "- GRU\n",
        "- LSTM \n",
        "\n",
        "The thing I worry is that the zero padding may reduce the performance of them. In future, If possible, we will mask those zero padding, so, it could show the true performance.\n",
        "\n",
        "However, CNN with conv1d layer still is able to do so. Moreover, it can be used as transfer learning due to the feature extraction layer. We may transfer them into another task on Myocardial infarction which might suffer from relatively low data \n",
        "\n",
        "Due to a limited time, I plan to implement a reliable convolutional neural network (CNN) architecture for our task. We will base our model on a research paper that we obtained the data from, which uses a Conv1d-based architecture with residual blocks.\n",
        "\n",
        "- Deep_ResCNN (Pretrained = False)\n",
        "\n",
        "The proposed CNN architecture will have multiple convolutional layers, each with 32 channels, a stride of 5, and same padding. This ensures that the addition in the residual blocks can be performed on units with the same number of channels without having to map them to the same unit using weight dot products. We will incorporate 5 residual connection blocks over the plain convolutional layers, which helps to accelerate learning by addressing the vanishing gradient problem. Finally, we will add three fully connected layers with the ReLU activation function for the hidden layers and softmax for the output layer.\n",
        "\n",
        "!! Note that These above architecture can have more than 1 layers\n",
        "Also Note that the above 4 architectures we used will all be set \n",
        "`truncate_to_max = True` to avoid unnecessary zero pad\n",
        "\n",
        "- Deep_ResCNN (Pretrained = True)\n",
        "\n",
        "Like the previous Deep_ResCNN, but this time, we will transfer the parameter of the model from Arrhythmia classification instead. \n",
        "\n",
        "This CNN model will be `truncate_to_max = False`\n",
        "\n",
        "\n",
        "We expect the ensemble model to perform better than just a single model, if we need to increase the performance (but slower in inference time), we recommend you to do that, but In this notebook, we don't do that due to a limited time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "mQi3DCbkbv0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# GRU model # Many-to-one\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, hidden_size, num_layers, device = device):\n",
        "        super(GRUModel,self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(1, hidden_size, num_layers, batch_first=True).to(device) # batch_first=True : Optional, but I prefer the first axis of data shape to be batch\n",
        "        self.fc = nn.Linear(hidden_size,1).to(device)\n",
        "\n",
        "        self.device = device\n",
        "        self._init_weights()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(self.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = out[:,-1,:] # Take only the last timestep of the output layer\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "            elif isinstance(m, nn.LSTM):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight' in name:\n",
        "                        nn.init.xavier_normal_(param)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x = torch.rand(32,1,187).to(device)\n",
        "\n",
        "    # Test 1 layer of GRU with hidden size 32x32\n",
        "    gru_model = GRUModel(hidden_size=32,num_layers = 1)\n",
        "    gru_model2 = GRUModel(hidden_size=32,num_layers = 2)\n",
        "    out = gru_model(x)\n",
        "    out2 = gru_model2(x)\n",
        "    print(out.shape)\n",
        "    print(out2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maSBaOWl0fUi",
        "outputId": "427ff5e6-29cb-42b1-85f3-9aeaa18ad2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1])\n",
            "torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model # Many-to-one\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, hidden_size, num_layers, device = device):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(1, hidden_size, num_layers, batch_first=True).to(device) # batch_first=True : Optional, but I prefer the first axis of data shape to be batch\n",
        "        self.fc = nn.Linear(hidden_size,1).to(device)\n",
        "        self._init_weights()\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device) #short term\n",
        "        c0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device) #long term\n",
        "\n",
        "        out, _ = self.lstm(x, (h0,c0))\n",
        "        out = out[:,-1,:] # Take only the last timestep of the output layer\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "            elif isinstance(m, nn.LSTM):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight' in name:\n",
        "                        nn.init.xavier_normal_(param)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x = torch.rand(32,1,187).to(device)\n",
        "\n",
        "    # Test 1 layer of GRU with hidden size 32x32\n",
        "    lstm_model = LSTMModel(hidden_size=32,num_layers = 1)\n",
        "    lstm_model2 = LSTMModel(hidden_size=32,num_layers = 2)\n",
        "    out = lstm_model(x)\n",
        "    out2 = lstm_model2(x)\n",
        "    print(out.shape)\n",
        "    print(out2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDK3QK5U7SPZ",
        "outputId": "375dcf37-6c83-4476-bf1e-0fa4946376f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1])\n",
            "torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep_ResCNN is a special case which would support both pretrain=True and pretrain=False. First, we will import the state_dict of that model"
      ],
      "metadata": {
        "id": "acMUnX2-vpwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Deep_ResCNN(nn.Module):\n",
        "    def __init__(self, num_classes = 1):\n",
        "        super(Deep_ResCNN, self).__init__()\n",
        "        # kernel size : (5,), number of channel : 32\n",
        "        self.conv1 = nn.Conv1d(1, 32, 5)\n",
        "        \n",
        "        self.conv2_1 = nn.Conv1d(32, 32, 5, padding=2)\n",
        "        self.conv2_2 = nn.Conv1d(32, 32, 5, padding=2)\n",
        "        \n",
        "        self.conv3_1 = nn.Conv1d(32, 32, 5, padding=2)\n",
        "        self.conv3_2 = nn.Conv1d(32, 32, 5, padding=2)\n",
        "        \n",
        "        self.conv4_1 = nn.Conv1d(32, 32, 5, padding=2)\n",
        "        self.conv4_2 = nn.Conv1d(32, 32, 5, padding=2)\n",
        "        \n",
        "        self.conv5_1 = nn.Conv1d(32, 32, 5, padding=2)\n",
        "        self.conv5_2 = nn.Conv1d(32, 32, 5, padding=2)\n",
        "        \n",
        "        self.fc1 = nn.Linear(32*8, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.fc3 = nn.Linear(32, num_classes)\n",
        "        \n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        x1 = F.relu(self.conv2_1(x))\n",
        "        x1 = self.conv2_2(x1)\n",
        "        x = F.relu(x + x1)\n",
        "        x = F.max_pool1d(x, kernel_size=5, stride=2)\n",
        "        \n",
        "        x1 = F.relu(self.conv3_1(x))\n",
        "        x1 = self.conv3_2(x1)\n",
        "        x = F.relu(x + x1)\n",
        "        x = F.max_pool1d(x, kernel_size=5, stride=2)\n",
        "        \n",
        "        x1 = F.relu(self.conv4_1(x))\n",
        "        x1 = self.conv4_2(x1)\n",
        "        x = F.relu(x + x1)\n",
        "        x = F.max_pool1d(x, kernel_size=5, stride=2)\n",
        "        \n",
        "        x1 = F.relu(self.conv5_1(x))\n",
        "        x1 = self.conv5_2(x1)\n",
        "        x = F.relu(x + x1)\n",
        "        x = F.max_pool1d(x, kernel_size=5, stride=2)\n",
        "        \n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x = torch.rand(32,1,187)\n",
        "    res_model = Deep_ResCNN(num_classes=1)\n",
        "    print(res_model)\n",
        "    out = res_model(x)\n",
        "    print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrGrbMhe7pU8",
        "outputId": "f2e9a806-efaa-4f1b-f215-68322983bcf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep_ResCNN(\n",
            "  (conv1): Conv1d(1, 32, kernel_size=(5,), stride=(1,))\n",
            "  (conv2_1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv2_2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv3_1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv3_2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv4_1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv4_2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv5_1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv5_2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n",
            "torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_for_deep_rescnn(pretrained=True,start_layer='conv4_1.weight', end_layer='fc3.bias', device = device):\n",
        "    # Load\n",
        "    global Deep_ResCNN\n",
        "\n",
        "    pretrain_path = '/content/drive/MyDrive/arrhythmia_classification/best_deep_rescnn_model_state_dict.pt'\n",
        "    model = Deep_ResCNN(num_classes=5).to(device) # The pretrained model \n",
        "    model.load_state_dict(torch.load(pretrain_path))\n",
        "\n",
        "    # Freeze all layers except the first 3 residual blocks\n",
        "    if pretrained:\n",
        "        start_freeze = False\n",
        "        for name, param in model.named_parameters():\n",
        "            if name == start_layer:\n",
        "                start_freeze = True\n",
        "            if start_freeze:\n",
        "                param.requires_grad = True\n",
        "\n",
        "            if name in end_layer:\n",
        "                break\n",
        "            if not start_freeze:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Replace for binary classification\n",
        "    model.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    if not pretrained:\n",
        "        model._init_weights()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Z84-Prk-78GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the require_grad of pretrain and non-pretrain\n",
        "\n",
        "model = setup_for_deep_rescnn(pretrained=True)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I31Ec-rO_Za0",
        "outputId": "268345a0-8414-401f-9b88-56bf1cbd04a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight False\n",
            "conv1.bias False\n",
            "conv2_1.weight False\n",
            "conv2_1.bias False\n",
            "conv2_2.weight False\n",
            "conv2_2.bias False\n",
            "conv3_1.weight False\n",
            "conv3_1.bias False\n",
            "conv3_2.weight False\n",
            "conv3_2.bias False\n",
            "conv4_1.weight True\n",
            "conv4_1.bias True\n",
            "conv4_2.weight True\n",
            "conv4_2.bias True\n",
            "conv5_1.weight True\n",
            "conv5_1.bias True\n",
            "conv5_2.weight True\n",
            "conv5_2.bias True\n",
            "fc1.weight True\n",
            "fc1.bias True\n",
            "fc2.weight True\n",
            "fc2.bias True\n",
            "fc3.weight True\n",
            "fc3.bias True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the require_grad of pretrain and non-pretrain\n",
        "\n",
        "model = setup_for_deep_rescnn(pretrained=False)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noITo5OtBz3S",
        "outputId": "6c7c666c-de6f-4586-b57e-ae488ba322f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight True\n",
            "conv1.bias True\n",
            "conv2_1.weight True\n",
            "conv2_1.bias True\n",
            "conv2_2.weight True\n",
            "conv2_2.bias True\n",
            "conv3_1.weight True\n",
            "conv3_1.bias True\n",
            "conv3_2.weight True\n",
            "conv3_2.bias True\n",
            "conv4_1.weight True\n",
            "conv4_1.bias True\n",
            "conv4_2.weight True\n",
            "conv4_2.bias True\n",
            "conv5_1.weight True\n",
            "conv5_1.bias True\n",
            "conv5_2.weight True\n",
            "conv5_2.bias True\n",
            "fc1.weight True\n",
            "fc1.bias True\n",
            "fc2.weight True\n",
            "fc2.bias True\n",
            "fc3.weight True\n",
            "fc3.bias True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_architecture(name, device ,**kwargs):\n",
        "\n",
        "    if name in ['gru','lstm']:\n",
        "        hidden_size = kwargs.get('hidden_size')\n",
        "        num_layers = kwargs.get('num_layers')\n",
        "        assert hidden_size, 'gru or lstm requires argument hidden_size'\n",
        "        assert num_layers, 'gru or lstm requires argument num_layers'\n",
        "    \n",
        "    elif name in ['deep_rescnn']:\n",
        "        pretrained = kwargs.get('pretrained') #expect bool\n",
        "        assert pretrained is not None, 'deep_rescnn requires argument pretrained'\n",
        "\n",
        "        start_layer = kwargs.get('start_layer') #expect bool\n",
        "        if start_layer in [1,2,3,4,5]: #Expect the model to freeze between residual block 1 to 5\n",
        "            start_layer = f'conv{start_layer}_1.weight'\n",
        "        else:\n",
        "            start_layer = f'conv4_1.weight'\n",
        "\n",
        "    if name == 'gru':\n",
        "        model = GRUModel(hidden_size=hidden_size,num_layers = num_layers).to(device)\n",
        "    elif name == 'lstm':\n",
        "        model = LSTMModel(hidden_size=hidden_size,num_layers=num_layers).to(device)\n",
        "    elif name == 'deep_rescnn':\n",
        "        model = setup_for_deep_rescnn(pretrained,start_layer=start_layer).to(device)\n",
        "    else:\n",
        "        raise ValueError('unknown architecture')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JkRRYEX_IcRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_architecture('deep_rescnn',device, pretrained = True, start_layer = 2)"
      ],
      "metadata": {
        "id": "5mXh8qt3vHi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if our sample model function normally \n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBibMk42Ae8W",
        "outputId": "4e54fad5-36cc-470d-c964-b85e508f8b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight False\n",
            "conv1.bias False\n",
            "conv2_1.weight True\n",
            "conv2_1.bias True\n",
            "conv2_2.weight True\n",
            "conv2_2.bias True\n",
            "conv3_1.weight True\n",
            "conv3_1.bias True\n",
            "conv3_2.weight True\n",
            "conv3_2.bias True\n",
            "conv4_1.weight True\n",
            "conv4_1.bias True\n",
            "conv4_2.weight True\n",
            "conv4_2.bias True\n",
            "conv5_1.weight True\n",
            "conv5_1.bias True\n",
            "conv5_2.weight True\n",
            "conv5_2.bias True\n",
            "fc1.weight True\n",
            "fc1.bias True\n",
            "fc2.weight True\n",
            "fc2.bias True\n",
            "fc3.weight True\n",
            "fc3.bias True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Baseline\n",
        "\n",
        "We never know Is our proposed model actually better than random-guessing model, or predict everything as majority class.\n",
        "\n",
        "To prove that I would like to introduce ZeroR as guessing everything as majority class. In this case, we guess all as abnormal class since it's the majority class"
      ],
      "metadata": {
        "id": "7Eil2FydX4vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def get_zero_baseline(train_set,val_set,test_set): \n",
        "    \"\"\"\n",
        "    Baseline model where we predict every heartbeat signal as majority class\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    train_targets = torch.tensor(train_set.y).long()\n",
        "    val_targets = torch.tensor(val_set.y).long()\n",
        "    test_targets = torch.tensor(test_set.y).long()\n",
        "\n",
        "    num_classes = train_targets.shape[1]\n",
        "\n",
        "    train_preds = torch.ones_like(train_targets)\n",
        "    val_preds = torch.ones_like(val_targets)\n",
        "    test_preds = torch.ones_like(test_targets)\n",
        "\n",
        "\n",
        "    sets = ['train', 'val', 'test']\n",
        "    preds = [train_preds, val_preds, test_preds]\n",
        "    targets = [train_targets, val_targets, test_targets]\n",
        "\n",
        "    metric = {}\n",
        "    for set_, pred, target in zip(sets, preds, targets):\n",
        "\n",
        "        recall = recall_score(target, pred, average='binary')\n",
        "        precision = precision_score(target, pred, average='binary')\n",
        "        f1 = f1_score(target, pred, average='binary')\n",
        "\n",
        "        metric.update({f'best_{set_}_recall': recall,\n",
        "                       f'best_{set_}_precision': precision,\n",
        "                       f'best_{set_}_f1': f1}\n",
        "                      )\n",
        "\n",
        "    return metric"
      ],
      "metadata": {
        "id": "lvPehs1rzMK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_zero_baseline(train_set,val_set,test_set)"
      ],
      "metadata": {
        "id": "bWkEEF5PViLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter new user, if new, then initiate new benchmark\n",
        "\n",
        "PATH = '/content/drive/MyDrive/mi_classification'\n",
        "\n",
        "try:\n",
        "    benchmark = pd.read_csv(PATH + '/benchmark.csv')\n",
        "except FileNotFoundError:\n",
        "    benchmark = pd.DataFrame({'model_name':'zero_baseline', 'hyperparameter' : 'seed=42','best_epoch' : 1,\n",
        "                                'best_train_cost' : None, 'best_val_cost' : None } | result,\n",
        "                                index=[0])"
      ],
      "metadata": {
        "id": "NMPILDDTORx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3828
        },
        "id": "NPNUWffI5IhY",
        "outputId": "b318ec76-2374-4c49-e394-677573441b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       model_name                                     hyperparameter  \\\n",
              "0   zero_baseline                                            seed=42   \n",
              "1             gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "2             gru  {'weight_decay': 0.01, 'learning_rate': 0.0005...   \n",
              "3             gru  {'weight_decay': 0.01, 'learning_rate': 0.0001...   \n",
              "4             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "..            ...                                                ...   \n",
              "91    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              "92    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              "93    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              "94    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              "95    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              "\n",
              "    best_epoch  best_train_cost  best_val_cost  best_train_recall  \\\n",
              "0            1              NaN            NaN           1.000000   \n",
              "1            1         0.237551       0.228324           1.000000   \n",
              "2            1         0.247199       0.235428           1.000000   \n",
              "3            2         0.268154       0.262874           0.384359   \n",
              "4            1         0.237496       0.228500           1.000000   \n",
              "..         ...              ...            ...                ...   \n",
              "91          71         0.019795       0.029031           0.993196   \n",
              "92          68         0.043721       0.051469           0.979579   \n",
              "93          64         0.026478       0.035204           0.987589   \n",
              "94          67         0.019459       0.030119           0.994543   \n",
              "95          62         0.019648       0.029903           0.994247   \n",
              "\n",
              "    best_train_precision  best_train_f1  best_val_recall  best_val_precision  \\\n",
              "0               0.721993       0.838555         1.000000            0.721993   \n",
              "1               0.721993       0.837994         1.000000            0.721993   \n",
              "2               0.721993       0.837963         1.000000            0.721993   \n",
              "3               0.293013       0.322972         1.000000            0.721993   \n",
              "4               0.721993       0.837926         1.000000            0.721993   \n",
              "..                   ...            ...              ...                 ...   \n",
              "91              0.990810       0.991912         0.991923            0.982528   \n",
              "92              0.966299       0.972732         0.982887            0.957762   \n",
              "93              0.983912       0.985649         0.991955            0.972927   \n",
              "94              0.991456       0.992958         0.982439            0.986138   \n",
              "95              0.991731       0.992952         0.995702            0.979381   \n",
              "\n",
              "    best_val_f1  best_test_recall  best_test_precision  best_test_f1  \n",
              "0      0.838555               1.0             0.721841      0.838452  \n",
              "1      0.838520               NaN                  NaN           NaN  \n",
              "2      0.838520               NaN                  NaN           NaN  \n",
              "3      0.838520               NaN                  NaN           NaN  \n",
              "4      0.838520               NaN                  NaN           NaN  \n",
              "..          ...               ...                  ...           ...  \n",
              "91     0.987198               NaN                  NaN           NaN  \n",
              "92     0.970154               NaN                  NaN           NaN  \n",
              "93     0.982331               NaN                  NaN           NaN  \n",
              "94     0.984278               NaN                  NaN           NaN  \n",
              "95     0.987473               NaN                  NaN           NaN  \n",
              "\n",
              "[96 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87fc6712-889f-48b2-97b4-955fb82cc6f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>hyperparameter</th>\n",
              "      <th>best_epoch</th>\n",
              "      <th>best_train_cost</th>\n",
              "      <th>best_val_cost</th>\n",
              "      <th>best_train_recall</th>\n",
              "      <th>best_train_precision</th>\n",
              "      <th>best_train_f1</th>\n",
              "      <th>best_val_recall</th>\n",
              "      <th>best_val_precision</th>\n",
              "      <th>best_val_f1</th>\n",
              "      <th>best_test_recall</th>\n",
              "      <th>best_test_precision</th>\n",
              "      <th>best_test_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_baseline</td>\n",
              "      <td>seed=42</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838555</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721841</td>\n",
              "      <td>0.838452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237551</td>\n",
              "      <td>0.228324</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837994</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0005...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.247199</td>\n",
              "      <td>0.235428</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837963</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0001...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.268154</td>\n",
              "      <td>0.262874</td>\n",
              "      <td>0.384359</td>\n",
              "      <td>0.293013</td>\n",
              "      <td>0.322972</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237496</td>\n",
              "      <td>0.228500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837926</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0023...</td>\n",
              "      <td>71</td>\n",
              "      <td>0.019795</td>\n",
              "      <td>0.029031</td>\n",
              "      <td>0.993196</td>\n",
              "      <td>0.990810</td>\n",
              "      <td>0.991912</td>\n",
              "      <td>0.991923</td>\n",
              "      <td>0.982528</td>\n",
              "      <td>0.987198</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0023...</td>\n",
              "      <td>68</td>\n",
              "      <td>0.043721</td>\n",
              "      <td>0.051469</td>\n",
              "      <td>0.979579</td>\n",
              "      <td>0.966299</td>\n",
              "      <td>0.972732</td>\n",
              "      <td>0.982887</td>\n",
              "      <td>0.957762</td>\n",
              "      <td>0.970154</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0023...</td>\n",
              "      <td>64</td>\n",
              "      <td>0.026478</td>\n",
              "      <td>0.035204</td>\n",
              "      <td>0.987589</td>\n",
              "      <td>0.983912</td>\n",
              "      <td>0.985649</td>\n",
              "      <td>0.991955</td>\n",
              "      <td>0.972927</td>\n",
              "      <td>0.982331</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0023...</td>\n",
              "      <td>67</td>\n",
              "      <td>0.019459</td>\n",
              "      <td>0.030119</td>\n",
              "      <td>0.994543</td>\n",
              "      <td>0.991456</td>\n",
              "      <td>0.992958</td>\n",
              "      <td>0.982439</td>\n",
              "      <td>0.986138</td>\n",
              "      <td>0.984278</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0023...</td>\n",
              "      <td>62</td>\n",
              "      <td>0.019648</td>\n",
              "      <td>0.029903</td>\n",
              "      <td>0.994247</td>\n",
              "      <td>0.991731</td>\n",
              "      <td>0.992952</td>\n",
              "      <td>0.995702</td>\n",
              "      <td>0.979381</td>\n",
              "      <td>0.987473</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>96 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87fc6712-889f-48b2-97b4-955fb82cc6f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87fc6712-889f-48b2-97b4-955fb82cc6f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87fc6712-889f-48b2-97b4-955fb82cc6f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/99dac6621f6ae8c4/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_baseline\",\n\"seed=42\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385551786070644,\n            'f': \"0.8385551786070644\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385551786070644,\n            'f': \"0.8385551786070644\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7218406593406593,\n            'f': \"0.7218406593406593\",\n        },\n{\n            'v': 0.8384523334662944,\n            'f': \"0.8384523334662944\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2375509515285218,\n            'f': \"0.2375509515285218\",\n        },\n{\n            'v': 0.2283238747890053,\n            'f': \"0.2283238747890053\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379937470948515,\n            'f': \"0.8379937470948515\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0005, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2471994655477768,\n            'f': \"0.2471994655477768\",\n        },\n{\n            'v': 0.235427502913983,\n            'f': \"0.235427502913983\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379629078268853,\n            'f': \"0.8379629078268853\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0.2681543786569422,\n            'f': \"0.2681543786569422\",\n        },\n{\n            'v': 0.2628738533180604,\n            'f': \"0.2628738533180604\",\n        },\n{\n            'v': 0.3843590544621472,\n            'f': \"0.3843590544621472\",\n        },\n{\n            'v': 0.2930126002290951,\n            'f': \"0.2930126002290951\",\n        },\n{\n            'v': 0.3229722700122437,\n            'f': \"0.3229722700122437\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23749594646426,\n            'f': \"0.23749594646426\",\n        },\n{\n            'v': 0.2284998590491481,\n            'f': \"0.2284998590491481\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379264789299304,\n            'f': \"0.8379264789299304\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2376953307500819,\n            'f': \"0.2376953307500819\",\n        },\n{\n            'v': 0.2283079446898293,\n            'f': \"0.2283079446898293\",\n        },\n{\n            'v': 0.9266895761741124,\n            'f': \"0.9266895761741124\",\n        },\n{\n            'v': 0.6705612829324169,\n            'f': \"0.6705612829324169\",\n        },\n{\n            'v': 0.777512641968225,\n            'f': \"0.777512641968225\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 6,\n            'f': \"6\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2327026935309205,\n            'f': \"0.2327026935309205\",\n        },\n{\n            'v': 0.2275912654051666,\n            'f': \"0.2275912654051666\",\n        },\n{\n            'v': 0.9853379152348224,\n            'f': \"0.9853379152348224\",\n        },\n{\n            'v': 0.7120274914089347,\n            'f': \"0.7120274914089347\",\n        },\n{\n            'v': 0.8261833872750113,\n            'f': \"0.8261833872750113\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 7,\n            'f': \"7\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2308624370094017,\n            'f': \"0.2308624370094017\",\n        },\n{\n            'v': 0.2279899197345746,\n            'f': \"0.2279899197345746\",\n        },\n{\n            'v': 0.9853379152348224,\n            'f': \"0.9853379152348224\",\n        },\n{\n            'v': 0.711340206185567,\n            'f': \"0.711340206185567\",\n        },\n{\n            'v': 0.8256718950601964,\n            'f': \"0.8256718950601964\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 8,\n            'f': \"8\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2459691150355038,\n            'f': \"0.2459691150355038\",\n        },\n{\n            'v': 0.2284881673625244,\n            'f': \"0.2284881673625244\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5950744558991982,\n            'f': \"0.5950744558991982\",\n        },\n{\n            'v': 0.6906552425563023,\n            'f': \"0.6906552425563023\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 9,\n            'f': \"9\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2387216530168452,\n            'f': \"0.2387216530168452\",\n        },\n{\n            'v': 0.2290092765363221,\n            'f': \"0.2290092765363221\",\n        },\n{\n            'v': 0.8680412371134021,\n            'f': \"0.8680412371134021\",\n        },\n{\n            'v': 0.6282932416953035,\n            'f': \"0.6282932416953035\",\n        },\n{\n            'v': 0.7283860077496157,\n            'f': \"0.7283860077496157\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 10,\n            'f': \"10\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2314093558512465,\n            'f': \"0.2314093558512465\",\n        },\n{\n            'v': 0.2278759803661366,\n            'f': \"0.2278759803661366\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378469602887191,\n            'f': \"0.8378469602887191\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 11,\n            'f': \"11\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2304964062069699,\n            'f': \"0.2304964062069699\",\n        },\n{\n            'v': 0.2280344991647091,\n            'f': \"0.2280344991647091\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377898005373192,\n            'f': \"0.8377898005373192\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 12,\n            'f': \"12\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2582674642540472,\n            'f': \"0.2582674642540472\",\n        },\n{\n            'v': 0.2318031070261067,\n            'f': \"0.2318031070261067\",\n        },\n{\n            'v': 0.677434135166094,\n            'f': \"0.677434135166094\",\n        },\n{\n            'v': 0.4879725085910653,\n            'f': \"0.4879725085910653\",\n        },\n{\n            'v': 0.5667718616335917,\n            'f': \"0.5667718616335917\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 13,\n            'f': \"13\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2306358962943873,\n            'f': \"0.2306358962943873\",\n        },\n{\n            'v': 0.2277908083601915,\n            'f': \"0.2277908083601915\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379650916960658,\n            'f': \"0.8379650916960658\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 14,\n            'f': \"14\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2404346744765667,\n            'f': \"0.2404346744765667\",\n        },\n{\n            'v': 0.2279883616261465,\n            'f': \"0.2279883616261465\",\n        },\n{\n            'v': 0.9120274914089348,\n            'f': \"0.9120274914089348\",\n        },\n{\n            'v': 0.6577319587628866,\n            'f': \"0.6577319587628866\",\n        },\n{\n            'v': 0.7636797948350783,\n            'f': \"0.7636797948350783\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 15,\n            'f': \"15\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2298978606928118,\n            'f': \"0.2298978606928118\",\n        },\n{\n            'v': 0.2280078556222194,\n            'f': \"0.2280078556222194\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379435108647284,\n            'f': \"0.8379435108647284\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 16,\n            'f': \"16\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2318737103254934,\n            'f': \"0.2318737103254934\",\n        },\n{\n            'v': 0.2278251648358872,\n            'f': \"0.2278251648358872\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378981891847632,\n            'f': \"0.8378981891847632\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 17,\n            'f': \"17\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.235134704972597,\n            'f': \"0.235134704972597\",\n        },\n{\n            'v': 0.2276050292237108,\n            'f': \"0.2276050292237108\",\n        },\n{\n            'v': 0.9120274914089348,\n            'f': \"0.9120274914089348\",\n        },\n{\n            'v': 0.656930126002291,\n            'f': \"0.656930126002291\",\n        },\n{\n            'v': 0.7633690624041659,\n            'f': \"0.7633690624041659\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 18,\n            'f': \"18\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2365808906663051,\n            'f': \"0.2365808906663051\",\n        },\n{\n            'v': 0.2280552437010499,\n            'f': \"0.2280552437010499\",\n        },\n{\n            'v': 0.9266895761741124,\n            'f': \"0.9266895761741124\",\n        },\n{\n            'v': 0.6675830469644902,\n            'f': \"0.6675830469644902\",\n        },\n{\n            'v': 0.775601528856061,\n            'f': \"0.775601528856061\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 19,\n            'f': \"19\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2301230856540686,\n            'f': \"0.2301230856540686\",\n        },\n{\n            'v': 0.2281344659754501,\n            'f': \"0.2281344659754501\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8380585970171258,\n            'f': \"0.8380585970171258\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 20,\n            'f': \"20\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2478139611453944,\n            'f': \"0.2478139611453944\",\n        },\n{\n            'v': 0.2278049097438038,\n            'f': \"0.2278049097438038\",\n        },\n{\n            'v': 0.7802280990089148,\n            'f': \"0.7802280990089148\",\n        },\n{\n            'v': 0.5706758304696449,\n            'f': \"0.5706758304696449\",\n        },\n{\n            'v': 0.6541961675540556,\n            'f': \"0.6541961675540556\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 21,\n            'f': \"21\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2344763796422353,\n            'f': \"0.2344763796422353\",\n        },\n{\n            'v': 0.2277304204264047,\n            'f': \"0.2277304204264047\",\n        },\n{\n            'v': 0.9266895761741124,\n            'f': \"0.9266895761741124\",\n        },\n{\n            'v': 0.6696449026345933,\n            'f': \"0.6696449026345933\",\n        },\n{\n            'v': 0.7769669688757592,\n            'f': \"0.7769669688757592\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 22,\n            'f': \"22\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.232706584652538,\n            'f': \"0.232706584652538\",\n        },\n{\n            'v': 0.227595777917154,\n            'f': \"0.227595777917154\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379593694587404,\n            'f': \"0.8379593694587404\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 23,\n            'f': \"23\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2311307953054006,\n            'f': \"0.2311307953054006\",\n        },\n{\n            'v': 0.2278286294326749,\n            'f': \"0.2278286294326749\",\n        },\n{\n            'v': 0.9706758304696448,\n            'f': \"0.9706758304696448\",\n        },\n{\n            'v': 0.7010309278350515,\n            'f': \"0.7010309278350515\",\n        },\n{\n            'v': 0.8134646001971478,\n            'f': \"0.8134646001971478\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 24,\n            'f': \"24\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2423909458337097,\n            'f': \"0.2423909458337097\",\n        },\n{\n            'v': 0.2281810255394768,\n            'f': \"0.2281810255394768\",\n        },\n{\n            'v': 0.8680412371134021,\n            'f': \"0.8680412371134021\",\n        },\n{\n            'v': 0.6262313860252005,\n            'f': \"0.6262313860252005\",\n        },\n{\n            'v': 0.7268249415921961,\n            'f': \"0.7268249415921961\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 25,\n            'f': \"25\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2518623998826039,\n            'f': \"0.2518623998826039\",\n        },\n{\n            'v': 0.2276107653617039,\n            'f': \"0.2276107653617039\",\n        },\n{\n            'v': 0.7360824742268042,\n            'f': \"0.7360824742268042\",\n        },\n{\n            'v': 0.5312714776632302,\n            'f': \"0.5312714776632302\",\n        },\n{\n            'v': 0.6165380489730845,\n            'f': \"0.6165380489730845\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 26,\n            'f': \"26\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.232329122092306,\n            'f': \"0.232329122092306\",\n        },\n{\n            'v': 0.2277810290507025,\n            'f': \"0.2277810290507025\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8381023417873185,\n            'f': \"0.8381023417873185\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 27,\n            'f': \"27\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2309198405700314,\n            'f': \"0.2309198405700314\",\n        },\n{\n            'v': 0.2276114615787755,\n            'f': \"0.2276114615787755\",\n        },\n{\n            'v': 0.9853379152348224,\n            'f': \"0.9853379152348224\",\n        },\n{\n            'v': 0.711340206185567,\n            'f': \"0.711340206185567\",\n        },\n{\n            'v': 0.8256305016925937,\n            'f': \"0.8256305016925937\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 28,\n            'f': \"28\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2465969259045241,\n            'f': \"0.2465969259045241\",\n        },\n{\n            'v': 0.2301524313557189,\n            'f': \"0.2301524313557189\",\n        },\n{\n            'v': 0.9266895761741124,\n            'f': \"0.9266895761741124\",\n        },\n{\n            'v': 0.6688430698739977,\n            'f': \"0.6688430698739977\",\n        },\n{\n            'v': 0.7764289639170969,\n            'f': \"0.7764289639170969\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 29,\n            'f': \"29\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2480862285980244,\n            'f': \"0.2480862285980244\",\n        },\n{\n            'v': 0.2289700862161072,\n            'f': \"0.2289700862161072\",\n        },\n{\n            'v': 0.8093928980526919,\n            'f': \"0.8093928980526919\",\n        },\n{\n            'v': 0.5835051546391753,\n            'f': \"0.5835051546391753\",\n        },\n{\n            'v': 0.6777182478215532,\n            'f': \"0.6777182478215532\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 30,\n            'f': \"30\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2403691546067612,\n            'f': \"0.2403691546067612\",\n        },\n{\n            'v': 0.2288103014435555,\n            'f': \"0.2288103014435555\",\n        },\n{\n            'v': 0.9413516609392898,\n            'f': \"0.9413516609392898\",\n        },\n{\n            'v': 0.6799541809851088,\n            'f': \"0.6799541809851088\",\n        },\n{\n            'v': 0.7890973790044737,\n            'f': \"0.7890973790044737\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 31,\n            'f': \"31\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.233591164599033,\n            'f': \"0.233591164599033\",\n        },\n{\n            'v': 0.2288613215549705,\n            'f': \"0.2288613215549705\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379850303243254,\n            'f': \"0.8379850303243254\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 32,\n            'f': \"32\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2440406676010577,\n            'f': \"0.2440406676010577\",\n        },\n{\n            'v': 0.2296466238003006,\n            'f': \"0.2296466238003006\",\n        },\n{\n            'v': 0.9853379152348224,\n            'f': \"0.9853379152348224\",\n        },\n{\n            'v': 0.7116838487972509,\n            'f': \"0.7116838487972509\",\n        },\n{\n            'v': 0.8257649556000859,\n            'f': \"0.8257649556000859\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 33,\n            'f': \"33\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2460531033601138,\n            'f': \"0.2460531033601138\",\n        },\n{\n            'v': 0.2287216798853628,\n            'f': \"0.2287216798853628\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5956471935853379,\n            'f': \"0.5956471935853379\",\n        },\n{\n            'v': 0.6910118096782196,\n            'f': \"0.6910118096782196\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 34,\n            'f': \"34\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2334126355790054,\n            'f': \"0.2334126355790054\",\n        },\n{\n            'v': 0.2294187471321768,\n            'f': \"0.2294187471321768\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379098005498145,\n            'f': \"0.8379098005498145\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 35,\n            'f': \"35\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.239308625647429,\n            'f': \"0.239308625647429\",\n        },\n{\n            'v': 0.2278418298234644,\n            'f': \"0.2278418298234644\",\n        },\n{\n            'v': 0.8973654066437572,\n            'f': \"0.8973654066437572\",\n        },\n{\n            'v': 0.6483390607101948,\n            'f': \"0.6483390607101948\",\n        },\n{\n            'v': 0.7523462888410456,\n            'f': \"0.7523462888410456\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 36,\n            'f': \"36\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2482980477646044,\n            'f': \"0.2482980477646044\",\n        },\n{\n            'v': 0.2309678902945567,\n            'f': \"0.2309678902945567\",\n        },\n{\n            'v': 0.9120274914089348,\n            'f': \"0.9120274914089348\",\n        },\n{\n            'v': 0.6575028636884307,\n            'f': \"0.6575028636884307\",\n        },\n{\n            'v': 0.7637263087855324,\n            'f': \"0.7637263087855324\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 37,\n            'f': \"37\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.238547049010469,\n            'f': \"0.238547049010469\",\n        },\n{\n            'v': 0.2289582008860775,\n            'f': \"0.2289582008860775\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.837937023496059,\n            'f': \"0.837937023496059\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 38,\n            'f': \"38\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2464800974620849,\n            'f': \"0.2464800974620849\",\n        },\n{\n            'v': 0.2290936878047038,\n            'f': \"0.2290936878047038\",\n        },\n{\n            'v': 0.8973654066437572,\n            'f': \"0.8973654066437572\",\n        },\n{\n            'v': 0.6467353951890035,\n            'f': \"0.6467353951890035\",\n        },\n{\n            'v': 0.7511256735881737,\n            'f': \"0.7511256735881737\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 39,\n            'f': \"39\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2416094226286856,\n            'f': \"0.2416094226286856\",\n        },\n{\n            'v': 0.230320805843753,\n            'f': \"0.230320805843753\",\n        },\n{\n            'v': 0.9560137457044674,\n            'f': \"0.9560137457044674\",\n        },\n{\n            'v': 0.6904925544100802,\n            'f': \"0.6904925544100802\",\n        },\n{\n            'v': 0.8012523489553404,\n            'f': \"0.8012523489553404\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 40,\n            'f': \"40\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2375738425148311,\n            'f': \"0.2375738425148311\",\n        },\n{\n            'v': 0.2279223526159103,\n            'f': \"0.2279223526159103\",\n        },\n{\n            'v': 0.9706758304696448,\n            'f': \"0.9706758304696448\",\n        },\n{\n            'v': 0.7012600229095074,\n            'f': \"0.7012600229095074\",\n        },\n{\n            'v': 0.8137792079514921,\n            'f': \"0.8137792079514921\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 41,\n            'f': \"41\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.231801272186217,\n            'f': \"0.231801272186217\",\n        },\n{\n            'v': 0.2284740050428921,\n            'f': \"0.2284740050428921\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378703062107288,\n            'f': \"0.8378703062107288\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 42,\n            'f': \"42\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2379867125178803,\n            'f': \"0.2379867125178803\",\n        },\n{\n            'v': 0.2279532879181334,\n            'f': \"0.2279532879181334\",\n        },\n{\n            'v': 0.9266895761741124,\n            'f': \"0.9266895761741124\",\n        },\n{\n            'v': 0.66815578465063,\n            'f': \"0.66815578465063\",\n        },\n{\n            'v': 0.7758350982211567,\n            'f': \"0.7758350982211567\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 43,\n            'f': \"43\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2330004306798128,\n            'f': \"0.2330004306798128\",\n        },\n{\n            'v': 0.2276014363437993,\n            'f': \"0.2276014363437993\",\n        },\n{\n            'v': 0.9853379152348224,\n            'f': \"0.9853379152348224\",\n        },\n{\n            'v': 0.7112256586483391,\n            'f': \"0.7112256586483391\",\n        },\n{\n            'v': 0.8255556394557227,\n            'f': \"0.8255556394557227\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 44,\n            'f': \"44\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2339486833905026,\n            'f': \"0.2339486833905026\",\n        },\n{\n            'v': 0.2276405961755215,\n            'f': \"0.2276405961755215\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8380657319957353,\n            'f': \"0.8380657319957353\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 45,\n            'f': \"45\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2432572240272338,\n            'f': \"0.2432572240272338\",\n        },\n{\n            'v': 0.2276432225487076,\n            'f': \"0.2276432225487076\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5942726231386025,\n            'f': \"0.5942726231386025\",\n        },\n{\n            'v': 0.6899781748662641,\n            'f': \"0.6899781748662641\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 46,\n            'f': \"46\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2344848114122366,\n            'f': \"0.2344848114122366\",\n        },\n{\n            'v': 0.227700880128903,\n            'f': \"0.227700880128903\",\n        },\n{\n            'v': 0.9706758304696448,\n            'f': \"0.9706758304696448\",\n        },\n{\n            'v': 0.7005727376861397,\n            'f': \"0.7005727376861397\",\n        },\n{\n            'v': 0.8132708898924303,\n            'f': \"0.8132708898924303\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 47,\n            'f': \"47\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2330947979892106,\n            'f': \"0.2330947979892106\",\n        },\n{\n            'v': 0.22759252305293,\n            'f': \"0.22759252305293\",\n        },\n{\n            'v': 0.9706758304696448,\n            'f': \"0.9706758304696448\",\n        },\n{\n            'v': 0.7009163802978235,\n            'f': \"0.7009163802978235\",\n        },\n{\n            'v': 0.8135473343847958,\n            'f': \"0.8135473343847958\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 48,\n            'f': \"48\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2336235400498391,\n            'f': \"0.2336235400498391\",\n        },\n{\n            'v': 0.2276226307313466,\n            'f': \"0.2276226307313466\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377483941926969,\n            'f': \"0.8377483941926969\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 49,\n            'f': \"49\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2353885561330212,\n            'f': \"0.2353885561330212\",\n        },\n{\n            'v': 0.2277346340139297,\n            'f': \"0.2277346340139297\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377726111607469,\n            'f': \"0.8377726111607469\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 50,\n            'f': \"50\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2361053212709033,\n            'f': \"0.2361053212709033\",\n        },\n{\n            'v': 0.2275916725089869,\n            'f': \"0.2275916725089869\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378901998288699,\n            'f': \"0.8378901998288699\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 51,\n            'f': \"51\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2353698967692505,\n            'f': \"0.2353698967692505\",\n        },\n{\n            'v': 0.227735572172604,\n            'f': \"0.227735572172604\",\n        },\n{\n            'v': 0.9413516609392898,\n            'f': \"0.9413516609392898\",\n        },\n{\n            'v': 0.6781214203894617,\n            'f': \"0.6781214203894617\",\n        },\n{\n            'v': 0.7877237131052639,\n            'f': \"0.7877237131052639\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 52,\n            'f': \"52\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128}\",\n{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': 0.0225941218248006,\n            'f': \"0.0225941218248006\",\n        },\n{\n            'v': 0.0324072357766407,\n            'f': \"0.0324072357766407\",\n        },\n{\n            'v': 0.9930491495835344,\n            'f': \"0.9930491495835344\",\n        },\n{\n            'v': 0.9888522839808216,\n            'f': \"0.9888522839808216\",\n        },\n{\n            'v': 0.9908897871389476,\n            'f': \"0.9908897871389476\",\n        },\n{\n            'v': 0.9876317632765976,\n            'f': \"0.9876317632765976\",\n        },\n{\n            'v': 0.9820149289352244,\n            'f': \"0.9820149289352244\",\n        },\n{\n            'v': 0.9848132951156788,\n            'f': \"0.9848132951156788\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 53,\n            'f': \"53\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128}\",\n{\n            'v': 48,\n            'f': \"48\",\n        },\n{\n            'v': 0.0508416385021816,\n            'f': \"0.0508416385021816\",\n        },\n{\n            'v': 0.0645379028369471,\n            'f': \"0.0645379028369471\",\n        },\n{\n            'v': 0.9771973487937176,\n            'f': \"0.9771973487937176\",\n        },\n{\n            'v': 0.9632434459063972,\n            'f': \"0.9632434459063972\",\n        },\n{\n            'v': 0.969978386968445,\n            'f': \"0.969978386968445\",\n        },\n{\n            'v': 0.977255988477046,\n            'f': \"0.977255988477046\",\n        },\n{\n            'v': 0.9513458385652596,\n            'f': \"0.9513458385652596\",\n        },\n{\n            'v': 0.964113683443594,\n            'f': \"0.964113683443594\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 54,\n            'f': \"54\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128}\",\n{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': 0.0234362519921186,\n            'f': \"0.0234362519921186\",\n        },\n{\n            'v': 0.0337653790194144,\n            'f': \"0.0337653790194144\",\n        },\n{\n            'v': 0.9927152671716448,\n            'f': \"0.9927152671716448\",\n        },\n{\n            'v': 0.9894789600382607,\n            'f': \"0.9894789600382607\",\n        },\n{\n            'v': 0.991033007737149,\n            'f': \"0.991033007737149\",\n        },\n{\n            'v': 0.9895085101265404,\n            'f': \"0.9895085101265404\",\n        },\n{\n            'v': 0.9792537562341926,\n            'f': \"0.9792537562341926\",\n        },\n{\n            'v': 0.9843540435633988,\n            'f': \"0.9843540435633988\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 55,\n            'f': \"55\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128}\",\n{\n            'v': 48,\n            'f': \"48\",\n        },\n{\n            'v': 0.041094861244452,\n            'f': \"0.041094861244452\",\n        },\n{\n            'v': 0.0539704506544722,\n            'f': \"0.0539704506544722\",\n        },\n{\n            'v': 0.9844684149339924,\n            'f': \"0.9844684149339924\",\n        },\n{\n            'v': 0.9735874425621156,\n            'f': \"0.9735874425621156\",\n        },\n{\n            'v': 0.9789105681946668,\n            'f': \"0.9789105681946668\",\n        },\n{\n            'v': 0.98004619919496,\n            'f': \"0.98004619919496\",\n        },\n{\n            'v': 0.9585436117727596,\n            'f': \"0.9585436117727596\",\n        },\n{\n            'v': 0.9691670516937744,\n            'f': \"0.9691670516937744\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 56,\n            'f': \"56\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 64, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 67,\n            'f': \"67\",\n        },\n{\n            'v': 0.0319727446926589,\n            'f': \"0.0319727446926589\",\n        },\n{\n            'v': 0.0416173297310203,\n            'f': \"0.0416173297310203\",\n        },\n{\n            'v': 0.9851698975388764,\n            'f': \"0.9851698975388764\",\n        },\n{\n            'v': 0.9807703180133152,\n            'f': \"0.9807703180133152\",\n        },\n{\n            'v': 0.9827931725537998,\n            'f': \"0.9827931725537998\",\n        },\n{\n            'v': 0.9881414845050732,\n            'f': \"0.9881414845050732\",\n        },\n{\n            'v': 0.973273745558192,\n            'f': \"0.973273745558192\",\n        },\n{\n            'v': 0.980647582184756,\n            'f': \"0.980647582184756\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 57,\n            'f': \"57\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 64, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 73,\n            'f': \"73\",\n        },\n{\n            'v': 0.0226347173915115,\n            'f': \"0.0226347173915115\",\n        },\n{\n            'v': 0.0309710595180693,\n            'f': \"0.0309710595180693\",\n        },\n{\n            'v': 0.99191408212952,\n            'f': \"0.99191408212952\",\n        },\n{\n            'v': 0.9899790651755932,\n            'f': \"0.9899790651755932\",\n        },\n{\n            'v': 0.9908586166919356,\n            'f': \"0.9908586166919356\",\n        },\n{\n            'v': 0.9881131788241242,\n            'f': \"0.9881131788241242\",\n        },\n{\n            'v': 0.9815515050446332,\n            'f': \"0.9815515050446332\",\n        },\n{\n            'v': 0.984816031418734,\n            'f': \"0.984816031418734\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 58,\n            'f': \"58\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 64, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 60,\n            'f': \"60\",\n        },\n{\n            'v': 0.0198410483813265,\n            'f': \"0.0198410483813265\",\n        },\n{\n            'v': 0.0267581169419886,\n            'f': \"0.0267581169419886\",\n        },\n{\n            'v': 0.9943093931841506,\n            'f': \"0.9943093931841506\",\n        },\n{\n            'v': 0.9902659079188296,\n            'f': \"0.9902659079188296\",\n        },\n{\n            'v': 0.992205407052724,\n            'f': \"0.992205407052724\",\n        },\n{\n            'v': 0.9895211908484994,\n            'f': \"0.9895211908484994\",\n        },\n{\n            'v': 0.9834538606897848,\n            'f': \"0.9834538606897848\",\n        },\n{\n            'v': 0.9864772467254864,\n            'f': \"0.9864772467254864\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 59,\n            'f': \"59\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 64, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 66,\n            'f': \"66\",\n        },\n{\n            'v': 0.0180832726955584,\n            'f': \"0.0180832726955584\",\n        },\n{\n            'v': 0.0285525257900818,\n            'f': \"0.0285525257900818\",\n        },\n{\n            'v': 0.995118539802475,\n            'f': \"0.995118539802475\",\n        },\n{\n            'v': 0.9922175876110144,\n            'f': \"0.9922175876110144\",\n        },\n{\n            'v': 0.9935892673341796,\n            'f': \"0.9935892673341796\",\n        },\n{\n            'v': 0.9914042780594218,\n            'f': \"0.9914042780594218\",\n        },\n{\n            'v': 0.9829892481320543,\n            'f': \"0.9829892481320543\",\n        },\n{\n            'v': 0.9871780801632491,\n            'f': \"0.9871780801632491\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 60,\n            'f': \"60\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 128, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 69,\n            'f': \"69\",\n        },\n{\n            'v': 0.0323414533050199,\n            'f': \"0.0323414533050199\",\n        },\n{\n            'v': 0.0426254152841994,\n            'f': \"0.0426254152841994\",\n        },\n{\n            'v': 0.9888446952696524,\n            'f': \"0.9888446952696524\",\n        },\n{\n            'v': 0.9790619541172588,\n            'f': \"0.9790619541172588\",\n        },\n{\n            'v': 0.9838243053056284,\n            'f': \"0.9838243053056284\",\n        },\n{\n            'v': 0.9828989097488332,\n            'f': \"0.9828989097488332\",\n        },\n{\n            'v': 0.9726699625488778,\n            'f': \"0.9726699625488778\",\n        },\n{\n            'v': 0.9777539277857742,\n            'f': \"0.9777539277857742\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 61,\n            'f': \"61\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 128, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 61,\n            'f': \"61\",\n        },\n{\n            'v': 0.0261192137505624,\n            'f': \"0.0261192137505624\",\n        },\n{\n            'v': 0.0345351610217512,\n            'f': \"0.0345351610217512\",\n        },\n{\n            'v': 0.989936652112098,\n            'f': \"0.989936652112098\",\n        },\n{\n            'v': 0.9851828442809492,\n            'f': \"0.9851828442809492\",\n        },\n{\n            'v': 0.987474421795834,\n            'f': \"0.987474421795834\",\n        },\n{\n            'v': 0.9843533447632592,\n            'f': \"0.9843533447632592\",\n        },\n{\n            'v': 0.9805485876075036,\n            'f': \"0.9805485876075036\",\n        },\n{\n            'v': 0.9824346291016118,\n            'f': \"0.9824346291016118\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 62,\n            'f': \"62\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 128, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 61,\n            'f': \"61\",\n        },\n{\n            'v': 0.0208413961439479,\n            'f': \"0.0208413961439479\",\n        },\n{\n            'v': 0.0313250025574284,\n            'f': \"0.0313250025574284\",\n        },\n{\n            'v': 0.99409908633698,\n            'f': \"0.99409908633698\",\n        },\n{\n            'v': 0.989945060870546,\n            'f': \"0.989945060870546\",\n        },\n{\n            'v': 0.991970442142383,\n            'f': \"0.991970442142383\",\n        },\n{\n            'v': 0.9852731489922104,\n            'f': \"0.9852731489922104\",\n        },\n{\n            'v': 0.9856963600166032,\n            'f': \"0.9856963600166032\",\n        },\n{\n            'v': 0.9854805300827998,\n            'f': \"0.9854805300827998\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 63,\n            'f': \"63\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 61,\n            'f': \"61\",\n        },\n{\n            'v': 0.0205371012194306,\n            'f': \"0.0205371012194306\",\n        },\n{\n            'v': 0.0296960796379961,\n            'f': \"0.0296960796379961\",\n        },\n{\n            'v': 0.9933200993385128,\n            'f': \"0.9933200993385128\",\n        },\n{\n            'v': 0.9920391567361662,\n            'f': \"0.9920391567361662\",\n        },\n{\n            'v': 0.992638715347216,\n            'f': \"0.992638715347216\",\n        },\n{\n            'v': 0.9895207389615812,\n            'f': \"0.9895207389615812\",\n        },\n{\n            'v': 0.982485070140583,\n            'f': \"0.982485070140583\",\n        },\n{\n            'v': 0.9859881853664432,\n            'f': \"0.9859881853664432\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 64,\n            'f': \"64\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 64, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 65,\n            'f': \"65\",\n        },\n{\n            'v': 0.0445923503740068,\n            'f': \"0.0445923503740068\",\n        },\n{\n            'v': 0.0533447997522927,\n            'f': \"0.0533447997522927\",\n        },\n{\n            'v': 0.97788728511881,\n            'f': \"0.97788728511881\",\n        },\n{\n            'v': 0.9658741606771408,\n            'f': \"0.9658741606771408\",\n        },\n{\n            'v': 0.971530860097586,\n            'f': \"0.971530860097586\",\n        },\n{\n            'v': 0.9814854611373144,\n            'f': \"0.9814854611373144\",\n        },\n{\n            'v': 0.9594866875182908,\n            'f': \"0.9594866875182908\",\n        },\n{\n            'v': 0.970331733304305,\n            'f': \"0.970331733304305\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 65,\n            'f': \"65\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 64, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 66,\n            'f': \"66\",\n        },\n{\n            'v': 0.027518761248028,\n            'f': \"0.027518761248028\",\n        },\n{\n            'v': 0.0352827699298096,\n            'f': \"0.0352827699298096\",\n        },\n{\n            'v': 0.988422087740818,\n            'f': \"0.988422087740818\",\n        },\n{\n            'v': 0.9826435988122282,\n            'f': \"0.9826435988122282\",\n        },\n{\n            'v': 0.9853523332525296,\n            'f': \"0.9853523332525296\",\n        },\n{\n            'v': 0.9929148725488274,\n            'f': \"0.9929148725488274\",\n        },\n{\n            'v': 0.9738415929578762,\n            'f': \"0.9738415929578762\",\n        },\n{\n            'v': 0.9832630622680296,\n            'f': \"0.9832630622680296\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 66,\n            'f': \"66\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 64, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 53,\n            'f': \"53\",\n        },\n{\n            'v': 0.0229076002793811,\n            'f': \"0.0229076002793811\",\n        },\n{\n            'v': 0.031314392236807,\n            'f': \"0.031314392236807\",\n        },\n{\n            'v': 0.9895789435317424,\n            'f': \"0.9895789435317424\",\n        },\n{\n            'v': 0.9880341159804206,\n            'f': \"0.9880341159804206\",\n        },\n{\n            'v': 0.9886406784960506,\n            'f': \"0.9886406784960506\",\n        },\n{\n            'v': 0.9895461004054994,\n            'f': \"0.9895461004054994\",\n        },\n{\n            'v': 0.9811295384899156,\n            'f': \"0.9811295384899156\",\n        },\n{\n            'v': 0.9853134952850108,\n            'f': \"0.9853134952850108\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 67,\n            'f': \"67\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 64, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 46,\n            'f': \"46\",\n        },\n{\n            'v': 0.0271256226512847,\n            'f': \"0.0271256226512847\",\n        },\n{\n            'v': 0.0313349803775241,\n            'f': \"0.0313349803775241\",\n        },\n{\n            'v': 0.9875084849737936,\n            'f': \"0.9875084849737936\",\n        },\n{\n            'v': 0.9854593871144708,\n            'f': \"0.9854593871144708\",\n        },\n{\n            'v': 0.9862413861309312,\n            'f': \"0.9862413861309312\",\n        },\n{\n            'v': 0.9870810556452144,\n            'f': \"0.9870810556452144\",\n        },\n{\n            'v': 0.9838384281479295,\n            'f': \"0.9838384281479295\",\n        },\n{\n            'v': 0.9854540575230278,\n            'f': \"0.9854540575230278\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 68,\n            'f': \"68\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 128, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 60,\n            'f': \"60\",\n        },\n{\n            'v': 0.0377128832908464,\n            'f': \"0.0377128832908464\",\n        },\n{\n            'v': 0.0473070870252819,\n            'f': \"0.0473070870252819\",\n        },\n{\n            'v': 0.9826115192813438,\n            'f': \"0.9826115192813438\",\n        },\n{\n            'v': 0.9747130608081436,\n            'f': \"0.9747130608081436\",\n        },\n{\n            'v': 0.9784949405193664,\n            'f': \"0.9784949405193664\",\n        },\n{\n            'v': 0.9785976526545952,\n            'f': \"0.9785976526545952\",\n        },\n{\n            'v': 0.9762395192799388,\n            'f': \"0.9762395192799388\",\n        },\n{\n            'v': 0.9774115294274768,\n            'f': \"0.9774115294274768\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 69,\n            'f': \"69\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 128, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 68,\n            'f': \"68\",\n        },\n{\n            'v': 0.0262932670630198,\n            'f': \"0.0262932670630198\",\n        },\n{\n            'v': 0.0346950855400554,\n            'f': \"0.0346950855400554\",\n        },\n{\n            'v': 0.9901640051397432,\n            'f': \"0.9901640051397432\",\n        },\n{\n            'v': 0.9860449941566796,\n            'f': \"0.9860449941566796\",\n        },\n{\n            'v': 0.9880476287314058,\n            'f': \"0.9880476287314058\",\n        },\n{\n            'v': 0.9905030430264912,\n            'f': \"0.9905030430264912\",\n        },\n{\n            'v': 0.9774360827901885,\n            'f': \"0.9774360827901885\",\n        },\n{\n            'v': 0.9839198717123196,\n            'f': \"0.9839198717123196\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 70,\n            'f': \"70\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 128, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 53,\n            'f': \"53\",\n        },\n{\n            'v': 0.0227575901998174,\n            'f': \"0.0227575901998174\",\n        },\n{\n            'v': 0.0297750283199077,\n            'f': \"0.0297750283199077\",\n        },\n{\n            'v': 0.9905578247901032,\n            'f': \"0.9905578247901032\",\n        },\n{\n            'v': 0.9884047787262032,\n            'f': \"0.9884047787262032\",\n        },\n{\n            'v': 0.9893382888889144,\n            'f': \"0.9893382888889144\",\n        },\n{\n            'v': 0.9895211908484994,\n            'f': \"0.9895211908484994\",\n        },\n{\n            'v': 0.9778993212986437,\n            'f': \"0.9778993212986437\",\n        },\n{\n            'v': 0.9836716844776064,\n            'f': \"0.9836716844776064\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 71,\n            'f': \"71\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 59,\n            'f': \"59\",\n        },\n{\n            'v': 0.0207043360560313,\n            'f': \"0.0207043360560313\",\n        },\n{\n            'v': 0.0293698068848999,\n            'f': \"0.0293698068848999\",\n        },\n{\n            'v': 0.9928747692250744,\n            'f': \"0.9928747692250744\",\n        },\n{\n            'v': 0.9906364706857228,\n            'f': \"0.9906364706857228\",\n        },\n{\n            'v': 0.99169744370989,\n            'f': \"0.99169744370989\",\n        },\n{\n            'v': 0.9923705052784236,\n            'f': \"0.9923705052784236\",\n        },\n{\n            'v': 0.9802400997176342,\n            'f': \"0.9802400997176342\",\n        },\n{\n            'v': 0.9862676832958304,\n            'f': \"0.9862676832958304\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 72,\n            'f': \"72\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 64, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 70,\n            'f': \"70\",\n        },\n{\n            'v': 0.0338038982556789,\n            'f': \"0.0338038982556789\",\n        },\n{\n            'v': 0.0430851721297629,\n            'f': \"0.0430851721297629\",\n        },\n{\n            'v': 0.9863781589911244,\n            'f': \"0.9863781589911244\",\n        },\n{\n            'v': 0.9756989742667432,\n            'f': \"0.9756989742667432\",\n        },\n{\n            'v': 0.9808254609738144,\n            'f': \"0.9808254609738144\",\n        },\n{\n            'v': 0.9866933898516264,\n            'f': \"0.9866933898516264\",\n        },\n{\n            'v': 0.9682074433183168,\n            'f': \"0.9682074433183168\",\n        },\n{\n            'v': 0.9773535743876964,\n            'f': \"0.9773535743876964\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 73,\n            'f': \"73\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 64, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 57,\n            'f': \"57\",\n        },\n{\n            'v': 0.0255590647587047,\n            'f': \"0.0255590647587047\",\n        },\n{\n            'v': 0.0342229620518348,\n            'f': \"0.0342229620518348\",\n        },\n{\n            'v': 0.9904027161095676,\n            'f': \"0.9904027161095676\",\n        },\n{\n            'v': 0.9890296462801396,\n            'f': \"0.9890296462801396\",\n        },\n{\n            'v': 0.9895801934777269,\n            'f': \"0.9895801934777269\",\n        },\n{\n            'v': 0.9885857616605588,\n            'f': \"0.9885857616605588\",\n        },\n{\n            'v': 0.9792238695853906,\n            'f': \"0.9792238695853906\",\n        },\n{\n            'v': 0.9838778750886936,\n            'f': \"0.9838778750886936\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 74,\n            'f': \"74\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 64, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 71,\n            'f': \"71\",\n        },\n{\n            'v': 0.0183644991956932,\n            'f': \"0.0183644991956932\",\n        },\n{\n            'v': 0.0291317694068364,\n            'f': \"0.0291317694068364\",\n        },\n{\n            'v': 0.9944530854022698,\n            'f': \"0.9944530854022698\",\n        },\n{\n            'v': 0.9918271568553382,\n            'f': \"0.9918271568553382\",\n        },\n{\n            'v': 0.993052771523388,\n            'f': \"0.993052771523388\",\n        },\n{\n            'v': 0.9890798579300444,\n            'f': \"0.9890798579300444\",\n        },\n{\n            'v': 0.9806377111950018,\n            'f': \"0.9806377111950018\",\n        },\n{\n            'v': 0.984837378452415,\n            'f': \"0.984837378452415\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 75,\n            'f': \"75\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 64, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 49,\n            'f': \"49\",\n        },\n{\n            'v': 0.0218018670817079,\n            'f': \"0.0218018670817079\",\n        },\n{\n            'v': 0.0295907345910867,\n            'f': \"0.0295907345910867\",\n        },\n{\n            'v': 0.9921396932857312,\n            'f': \"0.9921396932857312\",\n        },\n{\n            'v': 0.991508951927166,\n            'f': \"0.991508951927166\",\n        },\n{\n            'v': 0.9917089988920692,\n            'f': \"0.9917089988920692\",\n        },\n{\n            'v': 0.9928616573108784,\n            'f': \"0.9928616573108784\",\n        },\n{\n            'v': 0.981167706451374,\n            'f': \"0.981167706451374\",\n        },\n{\n            'v': 0.9869792455311012,\n            'f': \"0.9869792455311012\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 76,\n            'f': \"76\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 128, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 66,\n            'f': \"66\",\n        },\n{\n            'v': 0.0323711157903365,\n            'f': \"0.0323711157903365\",\n        },\n{\n            'v': 0.0431540064360063,\n            'f': \"0.0431540064360063\",\n        },\n{\n            'v': 0.9878087183735752,\n            'f': \"0.9878087183735752\",\n        },\n{\n            'v': 0.9794630759684496,\n            'f': \"0.9794630759684496\",\n        },\n{\n            'v': 0.9835246686474806,\n            'f': \"0.9835246686474806\",\n        },\n{\n            'v': 0.9900182313550158,\n            'f': \"0.9900182313550158\",\n        },\n{\n            'v': 0.9696977925333922,\n            'f': \"0.9696977925333922\",\n        },\n{\n            'v': 0.9797430418452012,\n            'f': \"0.9797430418452012\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 77,\n            'f': \"77\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 128, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 53,\n            'f': \"53\",\n        },\n{\n            'v': 0.0249428394797843,\n            'f': \"0.0249428394797843\",\n        },\n{\n            'v': 0.0317356843411717,\n            'f': \"0.0317356843411717\",\n        },\n{\n            'v': 0.9896837237695192,\n            'f': \"0.9896837237695192\",\n        },\n{\n            'v': 0.988168739286128,\n            'f': \"0.988168739286128\",\n        },\n{\n            'v': 0.9888557483422324,\n            'f': \"0.9888557483422324\",\n        },\n{\n            'v': 0.9862017859322532,\n            'f': \"0.9862017859322532\",\n        },\n{\n            'v': 0.9805826512492936,\n            'f': \"0.9805826512492936\",\n        },\n{\n            'v': 0.9833759284616423,\n            'f': \"0.9833759284616423\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 78,\n            'f': \"78\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 128, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 70,\n            'f': \"70\",\n        },\n{\n            'v': 0.0202968601841492,\n            'f': \"0.0202968601841492\",\n        },\n{\n            'v': 0.0317459448659952,\n            'f': \"0.0317459448659952\",\n        },\n{\n            'v': 0.992809083879522,\n            'f': \"0.992809083879522\",\n        },\n{\n            'v': 0.99029767168157,\n            'f': \"0.99029767168157\",\n        },\n{\n            'v': 0.991493983071962,\n            'f': \"0.991493983071962\",\n        },\n{\n            'v': 0.984760031639786,\n            'f': \"0.984760031639786\",\n        },\n{\n            'v': 0.9810234786239234,\n            'f': \"0.9810234786239234\",\n        },\n{\n            'v': 0.9828878527093105,\n            'f': \"0.9828878527093105\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 79,\n            'f': \"79\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 65,\n            'f': \"65\",\n        },\n{\n            'v': 0.0204064372901347,\n            'f': \"0.0204064372901347\",\n        },\n{\n            'v': 0.0297257438430056,\n            'f': \"0.0297257438430056\",\n        },\n{\n            'v': 0.9945475061721204,\n            'f': \"0.9945475061721204\",\n        },\n{\n            'v': 0.9921020707503848,\n            'f': \"0.9921020707503848\",\n        },\n{\n            'v': 0.9932908275025076,\n            'f': \"0.9932908275025076\",\n        },\n{\n            'v': 0.9923734495154544,\n            'f': \"0.9923734495154544\",\n        },\n{\n            'v': 0.9816319855252548,\n            'f': \"0.9816319855252548\",\n        },\n{\n            'v': 0.9869672414952848,\n            'f': \"0.9869672414952848\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 80,\n            'f': \"80\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 64, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 72,\n            'f': \"72\",\n        },\n{\n            'v': 0.0317392521869617,\n            'f': \"0.0317392521869617\",\n        },\n{\n            'v': 0.0426631337691008,\n            'f': \"0.0426631337691008\",\n        },\n{\n            'v': 0.9864051453520862,\n            'f': \"0.9864051453520862\",\n        },\n{\n            'v': 0.981364368254378,\n            'f': \"0.981364368254378\",\n        },\n{\n            'v': 0.983715144596932,\n            'f': \"0.983715144596932\",\n        },\n{\n            'v': 0.9852634125072816,\n            'f': \"0.9852634125072816\",\n        },\n{\n            'v': 0.9741075961123318,\n            'f': \"0.9741075961123318\",\n        },\n{\n            'v': 0.9796422776519916,\n            'f': \"0.9796422776519916\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 81,\n            'f': \"81\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 64, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 63,\n            'f': \"63\",\n        },\n{\n            'v': 0.0243876284246523,\n            'f': \"0.0243876284246523\",\n        },\n{\n            'v': 0.0324053233278166,\n            'f': \"0.0324053233278166\",\n        },\n{\n            'v': 0.990040025478494,\n            'f': \"0.990040025478494\",\n        },\n{\n            'v': 0.9885772612206092,\n            'f': \"0.9885772612206092\",\n        },\n{\n            'v': 0.989186335135652,\n            'f': \"0.989186335135652\",\n        },\n{\n            'v': 0.9871748053991528,\n            'f': \"0.9871748053991528\",\n        },\n{\n            'v': 0.9806048055310496,\n            'f': \"0.9806048055310496\",\n        },\n{\n            'v': 0.9838764502083478,\n            'f': \"0.9838764502083478\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 82,\n            'f': \"82\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 64, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 72,\n            'f': \"72\",\n        },\n{\n            'v': 0.0188887925801493,\n            'f': \"0.0188887925801493\",\n        },\n{\n            'v': 0.0307255138287839,\n            'f': \"0.0307255138287839\",\n        },\n{\n            'v': 0.9936511908767084,\n            'f': \"0.9936511908767084\",\n        },\n{\n            'v': 0.9914648586807896,\n            'f': \"0.9914648586807896\",\n        },\n{\n            'v': 0.9924933786870498,\n            'f': \"0.9924933786870498\",\n        },\n{\n            'v': 0.9881102345870936,\n            'f': \"0.9881102345870936\",\n        },\n{\n            'v': 0.9806164112021588,\n            'f': \"0.9806164112021588\",\n        },\n{\n            'v': 0.9843457438411622,\n            'f': \"0.9843457438411622\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 83,\n            'f': \"83\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 64, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 55,\n            'f': \"55\",\n        },\n{\n            'v': 0.0205564205124035,\n            'f': \"0.0205564205124035\",\n        },\n{\n            'v': 0.0306676404173021,\n            'f': \"0.0306676404173021\",\n        },\n{\n            'v': 0.9932526568852024,\n            'f': \"0.9932526568852024\",\n        },\n{\n            'v': 0.9903923154790452,\n            'f': \"0.9903923154790452\",\n        },\n{\n            'v': 0.9917254732961516,\n            'f': \"0.9917254732961516\",\n        },\n{\n            'v': 0.9957299928237416,\n            'f': \"0.9957299928237416\",\n        },\n{\n            'v': 0.9775469534821436,\n            'f': \"0.9775469534821436\",\n        },\n{\n            'v': 0.9865511771519464,\n            'f': \"0.9865511771519464\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 84,\n            'f': \"84\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 128, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 68,\n            'f': \"68\",\n        },\n{\n            'v': 0.0333570541090674,\n            'f': \"0.0333570541090674\",\n        },\n{\n            'v': 0.0426418203950133,\n            'f': \"0.0426418203950133\",\n        },\n{\n            'v': 0.987131179785267,\n            'f': \"0.987131179785267\",\n        },\n{\n            'v': 0.9778778624206932,\n            'f': \"0.9778778624206932\",\n        },\n{\n            'v': 0.9823751364535344,\n            'f': \"0.9823751364535344\",\n        },\n{\n            'v': 0.986709014810616,\n            'f': \"0.986709014810616\",\n        },\n{\n            'v': 0.9709424323135254,\n            'f': \"0.9709424323135254\",\n        },\n{\n            'v': 0.9787546813620862,\n            'f': \"0.9787546813620862\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 85,\n            'f': \"85\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 128, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 57,\n            'f': \"57\",\n        },\n{\n            'v': 0.0252618197633331,\n            'f': \"0.0252618197633331\",\n        },\n{\n            'v': 0.0332341967861062,\n            'f': \"0.0332341967861062\",\n        },\n{\n            'v': 0.9894319626337144,\n            'f': \"0.9894319626337144\",\n        },\n{\n            'v': 0.987178686690833,\n            'f': \"0.987178686690833\",\n        },\n{\n            'v': 0.988221300514838,\n            'f': \"0.988221300514838\",\n        },\n{\n            'v': 0.9895334196835404,\n            'f': \"0.9895334196835404\",\n        },\n{\n            'v': 0.976967621873028,\n            'f': \"0.976967621873028\",\n        },\n{\n            'v': 0.9832085063804747,\n            'f': \"0.9832085063804747\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 86,\n            'f': \"86\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 128, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': 0.0224977813629094,\n            'f': \"0.0224977813629094\",\n        },\n{\n            'v': 0.0321758743311531,\n            'f': \"0.0321758743311531\",\n        },\n{\n            'v': 0.9917122998372048,\n            'f': \"0.9917122998372048\",\n        },\n{\n            'v': 0.9884129908491348,\n            'f': \"0.9884129908491348\",\n        },\n{\n            'v': 0.9899886965892488,\n            'f': \"0.9899886965892488\",\n        },\n{\n            'v': 0.9857296549827368,\n            'f': \"0.9857296549827368\",\n        },\n{\n            'v': 0.986191428820292,\n            'f': \"0.986191428820292\",\n        },\n{\n            'v': 0.9859597109839188,\n            'f': \"0.9859597109839188\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 87,\n            'f': \"87\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 68,\n            'f': \"68\",\n        },\n{\n            'v': 0.0190639302056636,\n            'f': \"0.0190639302056636\",\n        },\n{\n            'v': 0.0295586853528145,\n            'f': \"0.0295586853528145\",\n        },\n{\n            'v': 0.994666113903744,\n            'f': \"0.994666113903744\",\n        },\n{\n            'v': 0.9919930373465932,\n            'f': \"0.9919930373465932\",\n        },\n{\n            'v': 0.9932878000148904,\n            'f': \"0.9932878000148904\",\n        },\n{\n            'v': 0.9947701059657192,\n            'f': \"0.9947701059657192\",\n        },\n{\n            'v': 0.9798069556338956,\n            'f': \"0.9798069556338956\",\n        },\n{\n            'v': 0.9872270600185536,\n            'f': \"0.9872270600185536\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 88,\n            'f': \"88\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 64, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 72,\n            'f': \"72\",\n        },\n{\n            'v': 0.043072446857292,\n            'f': \"0.043072446857292\",\n        },\n{\n            'v': 0.0521123243130974,\n            'f': \"0.0521123243130974\",\n        },\n{\n            'v': 0.9761655142145956,\n            'f': \"0.9761655142145956\",\n        },\n{\n            'v': 0.9652731484445286,\n            'f': \"0.9652731484445286\",\n        },\n{\n            'v': 0.9704128224631444,\n            'f': \"0.9704128224631444\",\n        },\n{\n            'v': 0.9843098660102388,\n            'f': \"0.9843098660102388\",\n        },\n{\n            'v': 0.9569302978807798,\n            'f': \"0.9569302978807798\",\n        },\n{\n            'v': 0.9704189261368996,\n            'f': \"0.9704189261368996\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 89,\n            'f': \"89\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 64, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 59,\n            'f': \"59\",\n        },\n{\n            'v': 0.030040605852694,\n            'f': \"0.030040605852694\",\n        },\n{\n            'v': 0.0379349651589631,\n            'f': \"0.0379349651589631\",\n        },\n{\n            'v': 0.9867155911250656,\n            'f': \"0.9867155911250656\",\n        },\n{\n            'v': 0.981954771194292,\n            'f': \"0.981954771194292\",\n        },\n{\n            'v': 0.984163294462204,\n            'f': \"0.984163294462204\",\n        },\n{\n            'v': 0.990493758428481,\n            'f': \"0.990493758428481\",\n        },\n{\n            'v': 0.9756182817263758,\n            'f': \"0.9756182817263758\",\n        },\n{\n            'v': 0.9829893603788756,\n            'f': \"0.9829893603788756\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 90,\n            'f': \"90\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 64, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 69,\n            'f': \"69\",\n        },\n{\n            'v': 0.0194735377864807,\n            'f': \"0.0194735377864807\",\n        },\n{\n            'v': 0.0308191578745637,\n            'f': \"0.0308191578745637\",\n        },\n{\n            'v': 0.9939745491161102,\n            'f': \"0.9939745491161102\",\n        },\n{\n            'v': 0.99041226444162,\n            'f': \"0.99041226444162\",\n        },\n{\n            'v': 0.9920828913755968,\n            'f': \"0.9920828913755968\",\n        },\n{\n            'v': 0.9900216274789646,\n            'f': \"0.9900216274789646\",\n        },\n{\n            'v': 0.982493323413522,\n            'f': \"0.982493323413522\",\n        },\n{\n            'v': 0.9862372847437672,\n            'f': \"0.9862372847437672\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 91,\n            'f': \"91\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 64, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 71,\n            'f': \"71\",\n        },\n{\n            'v': 0.0197950694167011,\n            'f': \"0.0197950694167011\",\n        },\n{\n            'v': 0.0290308029048426,\n            'f': \"0.0290308029048426\",\n        },\n{\n            'v': 0.9931955240622338,\n            'f': \"0.9931955240622338\",\n        },\n{\n            'v': 0.9908098924086872,\n            'f': \"0.9908098924086872\",\n        },\n{\n            'v': 0.991912025067736,\n            'f': \"0.991912025067736\",\n        },\n{\n            'v': 0.9919232838859072,\n            'f': \"0.9919232838859072\",\n        },\n{\n            'v': 0.9825278301679968,\n            'f': \"0.9825278301679968\",\n        },\n{\n            'v': 0.9871981981102582,\n            'f': \"0.9871981981102582\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 92,\n            'f': \"92\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 128, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 68,\n            'f': \"68\",\n        },\n{\n            'v': 0.0437212067870647,\n            'f': \"0.0437212067870647\",\n        },\n{\n            'v': 0.051468710040616,\n            'f': \"0.051468710040616\",\n        },\n{\n            'v': 0.9795785473219948,\n            'f': \"0.9795785473219948\",\n        },\n{\n            'v': 0.966299185959206,\n            'f': \"0.966299185959206\",\n        },\n{\n            'v': 0.9727315315877392,\n            'f': \"0.9727315315877392\",\n        },\n{\n            'v': 0.9828866809137922,\n            'f': \"0.9828866809137922\",\n        },\n{\n            'v': 0.9577620900696096,\n            'f': \"0.9577620900696096\",\n        },\n{\n            'v': 0.9701544912050144,\n            'f': \"0.9701544912050144\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 93,\n            'f': \"93\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 128, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 64,\n            'f': \"64\",\n        },\n{\n            'v': 0.0264778440124765,\n            'f': \"0.0264778440124765\",\n        },\n{\n            'v': 0.0352039636564009,\n            'f': \"0.0352039636564009\",\n        },\n{\n            'v': 0.9875886881691768,\n            'f': \"0.9875886881691768\",\n        },\n{\n            'v': 0.9839115216691044,\n            'f': \"0.9839115216691044\",\n        },\n{\n            'v': 0.985648570217583,\n            'f': \"0.985648570217583\",\n        },\n{\n            'v': 0.9919545338038868,\n            'f': \"0.9919545338038868\",\n        },\n{\n            'v': 0.9729272514464116,\n            'f': \"0.9729272514464116\",\n        },\n{\n            'v': 0.982331143481537,\n            'f': \"0.982331143481537\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 94,\n            'f': \"94\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 128, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 67,\n            'f': \"67\",\n        },\n{\n            'v': 0.0194589778874816,\n            'f': \"0.0194589778874816\",\n        },\n{\n            'v': 0.0301188637352071,\n            'f': \"0.0301188637352071\",\n        },\n{\n            'v': 0.9945429184547366,\n            'f': \"0.9945429184547366\",\n        },\n{\n            'v': 0.9914561523154032,\n            'f': \"0.9914561523154032\",\n        },\n{\n            'v': 0.9929578206187109,\n            'f': \"0.9929578206187109\",\n        },\n{\n            'v': 0.9824390076343577,\n            'f': \"0.9824390076343577\",\n        },\n{\n            'v': 0.9861377405571846,\n            'f': \"0.9861377405571846\",\n        },\n{\n            'v': 0.9842780795098808,\n            'f': \"0.9842780795098808\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 95,\n            'f': \"95\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 62,\n            'f': \"62\",\n        },\n{\n            'v': 0.0196479943498781,\n            'f': \"0.0196479943498781\",\n        },\n{\n            'v': 0.0299029666200741,\n            'f': \"0.0299029666200741\",\n        },\n{\n            'v': 0.9942470512578548,\n            'f': \"0.9942470512578548\",\n        },\n{\n            'v': 0.9917314704256792,\n            'f': \"0.9917314704256792\",\n        },\n{\n            'v': 0.9929521535097452,\n            'f': \"0.9929521535097452\",\n        },\n{\n            'v': 0.9957021390297108,\n            'f': \"0.9957021390297108\",\n        },\n{\n            'v': 0.9793811045228158,\n            'f': \"0.9793811045228158\",\n        },\n{\n            'v': 0.9874734184320076,\n            'f': \"0.9874734184320076\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"model_name\"], [\"string\", \"hyperparameter\"], [\"number\", \"best_epoch\"], [\"number\", \"best_train_cost\"], [\"number\", \"best_val_cost\"], [\"number\", \"best_train_recall\"], [\"number\", \"best_train_precision\"], [\"number\", \"best_train_f1\"], [\"number\", \"best_val_recall\"], [\"number\", \"best_val_precision\"], [\"number\", \"best_val_f1\"], [\"number\", \"best_test_recall\"], [\"number\", \"best_test_precision\"], [\"number\", \"best_test_f1\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: \"0\",\n      });\n    "
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F1 score of the zero baseline model appears to be reasonably acceptable, indicating that we need to avoid falling into the trap of training a model that performs no better than chance (i.e., an F1 score of 83.84%).\n",
        "\n",
        "If we do not take any corrective measures, the model is likely to optimize the loss function by simply predicting that all heartbeats signals have Myocardial Infarction (the majority class), which would lead to poor performance on the test set.\n",
        "\n",
        "In the next section, we will explain how we addressed this issue and improved the model's performance by utilizing techniques such as weighted binary cross entropy and transfer learning."
      ],
      "metadata": {
        "id": "Dl_AN58mmRkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost function for imbalanced dataset\n",
        "\n",
        "To prevent the model from learning to optimize for the majority class, we chose to use weighted binary cross entropy instead of plain binary cross entropy. This weighting scheme assigns a higher weight to the minority class, which allows the model to better account for the imbalance in class distribution and prevent it from biasing towards the majority class."
      ],
      "metadata": {
        "id": "5nlIBYrtmw2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weighted_for_bce(y_train):\n",
        "    pos_counts = y_train.sum(axis=0)\n",
        "    total_counts = len(y_train)\n",
        "    neg_counts = total_counts - pos_counts\n",
        "\n",
        "    pos_weights = neg_counts / pos_counts\n",
        "    return torch.tensor(pos_weights)"
      ],
      "metadata": {
        "id": "psj9UbQJmMh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weighted_for_bce(y_train)\n",
        "\n",
        "# So, the loss will be -(0.3851(ylog(y_hat) + ((1-y)log((1-y_hat)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_iY7NokE8Ts",
        "outputId": "b4329c53-0890-44b4-b107-a718894d8b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3851], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, our loss function will be $-(0.3851(y\\log(\\hat{y}) + ((1-y)\\log((1-\\hat{y}))))$, the goal of the model is trying to minimize this loss function "
      ],
      "metadata": {
        "id": "mNtU_2qJOsUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model selection"
      ],
      "metadata": {
        "id": "BldJJG_Km2i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, threshold = 0.5, get_history = False):\n",
        "    best_val_f1 = 0.0\n",
        "    best_val_recall = 0.0\n",
        "    best_val_precision = 0.0\n",
        "    best_train_f1 = 0.0\n",
        "    best_train_recall = 0.0\n",
        "    best_train_precision = 0.0\n",
        "    best_model = None\n",
        "    train_loss_lst = []\n",
        "    train_f1_lst = []\n",
        "    val_loss_lst = []\n",
        "    val_f1_lst = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        train_loss, train_acc, train_recall, train_precision, train_f1 = train_epoch(model, optimizer, criterion, train_loader, scheduler, threshold)\n",
        "        model.eval()\n",
        "        val_loss, val_acc, val_recall, val_precision, val_f1 = eval_epoch(model, criterion, val_loader, threshold)\n",
        "        print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f} Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        if get_history:\n",
        "            train_loss_lst.append(train_loss)\n",
        "            train_f1_lst.append(train_f1)\n",
        "            val_loss_lst.append(val_loss)\n",
        "            val_f1_lst.append(val_f1)\n",
        "\n",
        "        # Save the best model based on validation F1 score\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_epoch = epoch+1\n",
        "            best_train_cost = train_loss\n",
        "            best_train_f1 = train_f1\n",
        "            best_train_recall = train_recall\n",
        "            best_train_precision = train_precision\n",
        "            best_val_cost = val_loss\n",
        "            best_val_f1 = val_f1\n",
        "            best_val_recall = val_recall\n",
        "            best_val_precision = val_precision\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "\n",
        "    # Load the best model parameters and return the trained model\n",
        "\n",
        "    model.load_state_dict(best_model)\n",
        "    model_stat = {'best_epoch' : best_epoch,\n",
        "                  'best_train_cost' : best_train_cost,\n",
        "                'best_train_f1': best_train_f1,\n",
        "                'best_train_recall': best_train_recall,\n",
        "                'best_train_precision': best_train_precision,\n",
        "                'best_val_cost' : best_val_cost,\n",
        "                'best_val_f1': best_val_f1,\n",
        "                'best_val_recall': best_val_recall,\n",
        "                'best_val_precision': best_val_precision}\n",
        "\n",
        "    if get_history:\n",
        "        return model, model_stat, train_loss_lst, train_f1_lst, val_loss_lst, val_f1_lst\n",
        "    else:\n",
        "        return model, model_stat\n",
        "\n",
        "\n",
        "def train_epoch(model, optimizer, criterion, train_loader, scheduler, threshold):\n",
        "    n_obs = len(train_loader.dataset)\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_recall_total = 0.0\n",
        "    train_precision_total = 0.0\n",
        "    train_f1_total = 0.0\n",
        "\n",
        "    preds_all = []\n",
        "    targets_all = []\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target) # criterion : expect BCEWithLogitLoss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "        output = F.sigmoid(output)\n",
        "        preds = torch.where(output > threshold, 1.0, 0.0)\n",
        "\n",
        "        preds_all.append(preds)\n",
        "        targets_all.append(target)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Stack all into tensor\n",
        "    preds = torch.cat(preds_all, dim=0)\n",
        "    target = torch.cat(targets_all, dim=0)\n",
        "\n",
        "    train_loss /= n_obs\n",
        "    train_acc = torch.sum(preds == target).double() / n_obs\n",
        "    train_recall = recall_score(target.cpu().numpy(), preds.cpu().numpy(), average='binary')\n",
        "    train_precision = precision_score(target.cpu().numpy(), preds.cpu().numpy(), average='binary')\n",
        "    train_f1 = f1_score(target.cpu().numpy(), preds.cpu().numpy(), average='binary')\n",
        "\n",
        "    return train_loss, train_acc, train_recall, train_precision, train_f1\n",
        "\n",
        "\n",
        "def eval_epoch(model, criterion, val_loader,threshold):\n",
        "\n",
        "    n_obs = len(val_loader.dataset)\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_recall_total = 0.0\n",
        "    val_precision_total = 0.0\n",
        "    val_f1_total = 0.0\n",
        "\n",
        "    preds_all = []\n",
        "    targets_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss += loss.item() * data.size(0)\n",
        "\n",
        "            output = F.sigmoid(output)\n",
        "            preds = torch.where(output > threshold, 1.0, 0.0)\n",
        "\n",
        "            preds_all.append(preds)\n",
        "            targets_all.append(target)\n",
        "        \n",
        "        preds = torch.cat(preds_all, dim=0)\n",
        "        target = torch.cat(targets_all, dim=0)\n",
        "\n",
        "        val_loss /= n_obs\n",
        "        val_acc = torch.sum(preds == target).double() / n_obs\n",
        "        val_recall = recall_score(target.cpu().numpy(), preds.cpu().numpy(), average='binary')\n",
        "        val_precision = precision_score(target.cpu().numpy(), preds.cpu().numpy(), average='binary')\n",
        "        val_f1 = f1_score(target.cpu().numpy(), preds.cpu().numpy(), average='binary')\n",
        "\n",
        "\n",
        "    return val_loss, val_acc, val_recall,val_precision,val_f1"
      ],
      "metadata": {
        "id": "d7VZDrXhkjU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function will be useful for plot history\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def plot_history(history, experiment_name='unknown experiment'):\n",
        "    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Train Loss\", \"Train F1\", \"Val Loss\", \"Val F1\"))\n",
        "\n",
        "    # Define a list of RGB color codes for the lines\n",
        "    colors = [(np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255)) for _ in range(len(history))]\n",
        "\n",
        "    # Loop over each model in history and add a trace to the corresponding subplot for each metric\n",
        "    for i, (model_name, model_data) in enumerate(history.items()):\n",
        "        train_loss_lst = model_data['train_loss_lst']\n",
        "        train_f1_lst = model_data['train_f1_lst']\n",
        "        val_loss_lst = model_data['val_loss_lst']\n",
        "        val_f1_lst = model_data['val_f1_lst']\n",
        "\n",
        "        # Select a unique color for each line\n",
        "        line_color = 'rgb({}, {}, {})'.format(*colors[i])\n",
        "\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(train_loss_lst))), y=train_loss_lst, mode='lines', name=model_name,\n",
        "                                 line_color=line_color, legendgroup=model_name), row=1, col=1)\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(train_f1_lst))), y=train_f1_lst, mode='lines', name=model_name,\n",
        "                                 line_color=line_color, showlegend=False, legendgroup=model_name), row=1, col=2)\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(val_loss_lst))), y=val_loss_lst, mode='lines', name=model_name,\n",
        "                                 line_color=line_color, showlegend=False, legendgroup=model_name), row=2, col=1)\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(val_f1_lst))), y=val_f1_lst, mode='lines', name=model_name,\n",
        "                                 line_color=line_color, showlegend=False, legendgroup=model_name), row=2, col=2)\n",
        "\n",
        "    # Update the layout of the figure with a title and axis labels\n",
        "    fig.update_layout(title=f\"Metrics for Different Models for {experiment_name}\", height=800, width=1600, showlegend=True)\n",
        "    fig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Epoch\", row=2, col=1)\n",
        "    fig.update_xaxes(title_text=\"Epoch\", row=2, col=2)\n",
        "    fig.update_yaxes(title_text=\"Train Loss\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Train F1\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Val Loss\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Val F1\", row=2, col=2)\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "5NnZ6Uw5we9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we don't know what should be the num_epochs we trained for each of sequence model, so the experiment_0, will look at the benchmark, and will be set for the next experiment\n",
        "\n",
        "Note that CosineAnnealingLR is used as the default lr_scheduler which will set learning rate (\\eta) per epoch like the following equation\n",
        "\n",
        "$$\\eta_{t+1} = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 + \\cos(\\frac{T_{max}}{T_{cur}}\\pi)), \\ \\ T_{cur} \\neq (2k+1)T_{max};$$\n",
        "\n",
        "$$\\eta_{t+1} = \\eta_{t} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 - \\cos(\\frac{T_{max}}{1}\\pi)), \\ \\ T_{cur} = (2k+1)T_{max}.$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\\eta_{t} = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 + \\cos(\\frac{T_{max}}{T_{cur}}\\pi))$$\n",
        "\n",
        "The idea behind using this learning rate scheduler, because we found that it is unnecessary to let the learning rate always shrinking\n",
        "\n",
        "Here's some visual guide:\n",
        "\n",
        "![](https://discuss.pytorch.org/uploads/default/original/3X/4/d/4de6d50a138c7a17184d147099687881d51eb9d1.png)\n",
        "\n",
        "the use of a cosine curve in the learning rate schedule ensures that the learning rate is decreased smoothly and gradually, avoiding abrupt changes that could lead to instability or poor convergence during training. Overall, the CosineAnnealingLR learning rate schedule has proven to be a effective method for training deep neural networks and is widely used in the deep learning community. You can use ither learning rate scheduler as well, but I (Run) decided not to use in the first 4 experiments"
      ],
      "metadata": {
        "id": "kGIord5T3SS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us start the new experiment"
      ],
      "metadata": {
        "id": "Uc-5ue1HyzVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Define the hyperparameters to search over\n",
        "def experiment_0(benchmark):\n",
        "\n",
        "    \"\"\"\n",
        "    Test which epoch the sequence model gru,lstm is surpass the zero baseline\n",
        "    \"\"\"\n",
        "    global X_train,y_train,X_val,y_val\n",
        "    name = ['gru']\n",
        "    weight_decay = [1e-2]\n",
        "    lr = [7e-4,5e-4,1e-4] # Initial guess lr\n",
        "    batch_size = [128]\n",
        "\n",
        "    # For lstm and gru only\n",
        "    hidden_sizes = [32]\n",
        "    num_layers_list = [1]\n",
        "    \n",
        "    # Create a Cartesian product of all hyperparameter combinations\n",
        "    hyperparams = list(itertools.product(name, weight_decay, lr, batch_size, hidden_sizes, num_layers_list))\n",
        "    print(f'grid search : perform {len(hyperparams)} searchs')\n",
        "\n",
        "    # Setup\n",
        "    train_set = MI_Dataset(X_train,y_train, truncate_to_max=True)\n",
        "    val_set = MI_Dataset(X_val,y_val, truncate_to_max=True)\n",
        "\n",
        "    # Loop over each combination of hyperparameters and train/evaluate the model\n",
        "    best_val_f1 = 0.0\n",
        "    num_epochs = 50\n",
        "    for i, (name, wd, lr, bs, hidden_size, num_layers) in enumerate(hyperparams):\n",
        "\n",
        "        print(f'\\n 🔎 search {i} : {name} --- hidden_size : {hidden_size}, num_layers : {num_layers}, lr : {lr}, weight_decay : {wd}, batch_size : {bs}')\n",
        "\n",
        "        # Setup Device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Create the model, optimizer, and loss function\n",
        "        model = get_architecture(name = name, device = device, hidden_size=hidden_size, num_layers=num_layers)\n",
        "        pos_weight = get_weighted_for_bce(y_train)\n",
        "        criterion = nn.BCEWithLogitsLoss(weight=pos_weight.to(device))\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=wd)\n",
        "\n",
        "        # Create the data loaders\n",
        "        train_loader, val_loader, test_loader = get_loader(train_set, val_set, test_set, train_batch_size=bs)\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n",
        "        model, model_stat = train(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs)\n",
        "\n",
        "        \n",
        "        # save the best model configuration based on validation F1 score\n",
        "        if model_stat['best_val_f1'] > best_val_f1:\n",
        "            best_val_f1 = model_stat['best_val_f1']\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "        \n",
        "        row = {'model_name' : name,\n",
        "            'hyperparameter' : {'weight_decay' : wd, 'learning_rate' : lr,\n",
        "                                'batch_size' : bs, 'hidden_size' : hidden_size,\n",
        "                                'num_layers' : num_layers},\n",
        "            'best_epoch' : model_stat['best_epoch'],\n",
        "            'best_train_cost' : model_stat['best_train_cost'],\n",
        "            'best_val_cost' : model_stat['best_val_cost'],\n",
        "            'best_train_recall' : model_stat['best_train_recall'],\n",
        "            'best_train_precision' : model_stat['best_train_precision'],\n",
        "            'best_train_f1' : model_stat['best_train_f1'],\n",
        "            'best_val_recall' : model_stat['best_val_recall'],\n",
        "            'best_val_precision' : model_stat['best_val_precision'],\n",
        "            'best_val_f1' : model_stat['best_val_f1'],\n",
        "            'best_test_recall' : None,\n",
        "            'best_test_precision' : None,\n",
        "            'best_test_f1' : None,\n",
        "            }\n",
        "\n",
        "        benchmark =benchmark.append(row, ignore_index=True)\n",
        "\n",
        "    return benchmark, best_model"
      ],
      "metadata": {
        "id": "q69tlDwo2xrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## For new user, you can uncomment to append the result of the experiment_0 to your benchmark\n",
        "benchmark, best_model = experiment_0(benchmark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0dbQbFW3MF7",
        "outputId": "4d5f3a00-ec13-41f3-f0c4-5425bf5bc922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid search : perform 3 searchs\n",
            "\n",
            " 🔎 search 0 : gru --- hidden_size : 32, num_layers : 1, lr : 0.0007, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/50, Train Loss: 0.2376, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 2/50, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 3/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 4/50, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 5/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 6/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 7/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 8/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 9/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 10/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 11/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 12/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 13/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 14/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 15/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 16/50, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 17/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 18/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 19/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 20/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 21/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 22/50, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 23/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 24/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 25/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 26/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 27/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 28/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 29/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 30/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 31/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 32/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 33/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 34/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 35/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 36/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 37/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 38/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 39/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 40/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 41/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 42/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 43/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 44/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 45/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 46/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 47/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 48/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 49/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 50/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "\n",
            " 🔎 search 1 : gru --- hidden_size : 32, num_layers : 1, lr : 0.0005, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/50, Train Loss: 0.2472, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2354, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 2/50, Train Loss: 0.2312, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2288, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 3/50, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2284, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 4/50, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 5/50, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 6/50, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 7/50, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 8/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 9/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 10/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 11/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 12/50, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 13/50, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 14/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 15/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 16/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 17/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 18/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 19/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 20/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 21/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 22/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 23/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 24/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 25/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 26/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 27/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 28/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 29/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 30/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 31/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 32/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 33/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 34/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 35/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 36/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 37/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 38/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 39/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 40/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 41/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 42/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 43/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 44/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 45/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 46/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 47/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 48/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 49/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 50/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "\n",
            " 🔎 search 2 : gru --- hidden_size : 32, num_layers : 1, lr : 0.0001, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/50, Train Loss: 0.2796, Train Acc: 0.2780, Train F1: 0.0000 Val Loss: 0.2734, Val Acc: 0.2780, Val F1: 0.0000\n",
            "Epoch: 2/50, Train Loss: 0.2682, Train Acc: 0.4504, Train F1: 0.5033 Val Loss: 0.2629, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 3/50, Train Loss: 0.2582, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2535, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 4/50, Train Loss: 0.2494, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2455, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 5/50, Train Loss: 0.2420, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2389, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 6/50, Train Loss: 0.2364, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2341, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 7/50, Train Loss: 0.2327, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2314, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 8/50, Train Loss: 0.2306, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2299, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 9/50, Train Loss: 0.2295, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2292, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 10/50, Train Loss: 0.2289, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2288, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 11/50, Train Loss: 0.2287, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2286, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 12/50, Train Loss: 0.2285, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 13/50, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 14/50, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 15/50, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 16/50, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 17/50, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 18/50, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 19/50, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 20/50, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 21/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 22/50, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 23/50, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 24/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 25/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 26/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 27/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 28/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 29/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 30/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 31/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 32/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 33/50, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 34/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 35/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 36/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 37/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 38/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 39/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 40/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 41/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 42/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 43/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 44/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 45/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 46/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 47/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 48/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 49/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n",
            "Epoch: 50/50, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8386 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8386\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(       model_name                                     hyperparameter  \\\n",
              " 0   zero_baseline                                            seed=42   \n",
              " 1             gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              " 2             gru  {'weight_decay': 0.01, 'learning_rate': 0.0005...   \n",
              " 3             gru  {'weight_decay': 0.01, 'learning_rate': 0.0001...   \n",
              " 4             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              " ..            ...                                                ...   \n",
              " 94    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              " 95    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              " 96            gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              " 97            gru  {'weight_decay': 0.01, 'learning_rate': 0.0005...   \n",
              " 98            gru  {'weight_decay': 0.01, 'learning_rate': 0.0001...   \n",
              " \n",
              "     best_epoch  best_train_cost  best_val_cost  best_train_recall  \\\n",
              " 0            1              NaN            NaN           1.000000   \n",
              " 1            1         0.237551       0.228324           1.000000   \n",
              " 2            1         0.247199       0.235428           1.000000   \n",
              " 3            2         0.268154       0.262874           0.384359   \n",
              " 4            1         0.237496       0.228500           1.000000   \n",
              " ..         ...              ...            ...                ...   \n",
              " 94          67         0.019459       0.030119           0.994543   \n",
              " 95          62         0.019648       0.029903           0.994247   \n",
              " 96           1         0.237551       0.228324           1.000000   \n",
              " 97           1         0.247199       0.235428           1.000000   \n",
              " 98           2         0.268154       0.262874           0.385689   \n",
              " \n",
              "     best_train_precision  best_train_f1  best_val_recall  best_val_precision  \\\n",
              " 0               0.721993       0.838555         1.000000            0.721993   \n",
              " 1               0.721993       0.837994         1.000000            0.721993   \n",
              " 2               0.721993       0.837963         1.000000            0.721993   \n",
              " 3               0.293013       0.322972         1.000000            0.721993   \n",
              " 4               0.721993       0.837926         1.000000            0.721993   \n",
              " ..                   ...            ...              ...                 ...   \n",
              " 94              0.991456       0.992958         0.982439            0.986138   \n",
              " 95              0.991731       0.992952         0.995702            0.979381   \n",
              " 96              0.721993       0.838555         1.000000            0.721993   \n",
              " 97              0.721993       0.838555         1.000000            0.721993   \n",
              " 98              0.724158       0.503313         1.000000            0.721993   \n",
              " \n",
              "     best_val_f1 best_test_recall best_test_precision best_test_f1  \n",
              " 0      0.838555              1.0            0.721841     0.838452  \n",
              " 1      0.838520              NaN                 NaN          NaN  \n",
              " 2      0.838520              NaN                 NaN          NaN  \n",
              " 3      0.838520              NaN                 NaN          NaN  \n",
              " 4      0.838520              NaN                 NaN          NaN  \n",
              " ..          ...              ...                 ...          ...  \n",
              " 94     0.984278              NaN                 NaN          NaN  \n",
              " 95     0.987473              NaN                 NaN          NaN  \n",
              " 96     0.838555              NaN                 NaN          NaN  \n",
              " 97     0.838555              NaN                 NaN          NaN  \n",
              " 98     0.838555              NaN                 NaN          NaN  \n",
              " \n",
              " [99 rows x 14 columns],\n",
              " OrderedDict([('gru.weight_ih_l0', tensor([[ 8.9683e-02],\n",
              "                       [ 1.0100e-01],\n",
              "                       [-5.1025e-03],\n",
              "                       [ 1.1639e-01],\n",
              "                       [-3.5336e-03],\n",
              "                       [ 1.9466e-03],\n",
              "                       [-4.2546e-02],\n",
              "                       [ 5.9344e-02],\n",
              "                       [ 1.0994e-01],\n",
              "                       [-8.4357e-02],\n",
              "                       [ 1.0780e-01],\n",
              "                       [ 8.3572e-04],\n",
              "                       [ 8.5242e-02],\n",
              "                       [-8.5023e-04],\n",
              "                       [ 4.1771e-02],\n",
              "                       [ 8.4464e-04],\n",
              "                       [ 9.0776e-02],\n",
              "                       [-8.1146e-04],\n",
              "                       [-3.9246e-02],\n",
              "                       [ 7.4927e-03],\n",
              "                       [-3.8249e-02],\n",
              "                       [ 5.1109e-04],\n",
              "                       [-2.9442e-02],\n",
              "                       [ 7.2288e-02],\n",
              "                       [-9.3968e-02],\n",
              "                       [-3.8298e-02],\n",
              "                       [-1.1028e-02],\n",
              "                       [-6.1712e-02],\n",
              "                       [ 1.9859e-04],\n",
              "                       [-1.2843e-01],\n",
              "                       [ 1.1369e-01],\n",
              "                       [-1.0438e-01],\n",
              "                       [ 9.0972e-02],\n",
              "                       [-3.0161e-04],\n",
              "                       [-1.6969e-02],\n",
              "                       [ 6.4544e-02],\n",
              "                       [-6.5005e-04],\n",
              "                       [ 9.7185e-02],\n",
              "                       [-2.6774e-04],\n",
              "                       [-1.5617e-02],\n",
              "                       [ 9.2362e-03],\n",
              "                       [-9.5567e-03],\n",
              "                       [ 3.1791e-02],\n",
              "                       [ 1.1191e-01],\n",
              "                       [ 5.7783e-02],\n",
              "                       [-3.4416e-02],\n",
              "                       [ 5.7648e-02],\n",
              "                       [ 3.1829e-04],\n",
              "                       [ 4.6011e-02],\n",
              "                       [-6.3109e-02],\n",
              "                       [-1.2881e-01],\n",
              "                       [-2.6324e-02],\n",
              "                       [-9.0108e-02],\n",
              "                       [ 9.9363e-02],\n",
              "                       [ 1.1791e-02],\n",
              "                       [ 3.0727e-02],\n",
              "                       [ 1.5744e-02],\n",
              "                       [-3.5015e-05],\n",
              "                       [ 9.2800e-02],\n",
              "                       [-8.0373e-02],\n",
              "                       [-8.2942e-07],\n",
              "                       [-7.5572e-02],\n",
              "                       [ 1.4614e-02],\n",
              "                       [-1.9882e-02],\n",
              "                       [ 1.4502e-02],\n",
              "                       [-3.4143e-03],\n",
              "                       [ 1.0096e-01],\n",
              "                       [-6.0390e-02],\n",
              "                       [-6.0760e-02],\n",
              "                       [-6.0939e-02],\n",
              "                       [ 1.1314e-01],\n",
              "                       [ 1.8341e-02],\n",
              "                       [ 1.2394e-01],\n",
              "                       [-1.0012e-01],\n",
              "                       [-1.2908e-01],\n",
              "                       [-9.2738e-02],\n",
              "                       [-7.3962e-02],\n",
              "                       [ 2.9238e-02],\n",
              "                       [ 2.2429e-02],\n",
              "                       [ 1.0107e-01],\n",
              "                       [-4.7503e-02],\n",
              "                       [-7.5517e-02],\n",
              "                       [ 4.9730e-02],\n",
              "                       [-2.8871e-02],\n",
              "                       [ 6.2639e-02],\n",
              "                       [-5.5745e-03],\n",
              "                       [ 5.6862e-02],\n",
              "                       [-9.1950e-02],\n",
              "                       [-4.5534e-02],\n",
              "                       [ 1.4541e-02],\n",
              "                       [ 2.5339e-03],\n",
              "                       [-7.4367e-03],\n",
              "                       [ 6.0838e-02],\n",
              "                       [ 7.5144e-02],\n",
              "                       [-8.2906e-02],\n",
              "                       [-5.0271e-02]], device='cuda:0')),\n",
              "              ('gru.weight_hh_l0',\n",
              "               tensor([[ 1.1639e-01, -1.8054e-02, -2.0555e-02,  ..., -5.9412e-02,\n",
              "                        -1.7507e-02, -9.4937e-02],\n",
              "                       [ 1.0347e-01, -3.1046e-04,  1.0693e-01,  ...,  1.8776e-02,\n",
              "                         1.9992e-02, -5.3797e-02],\n",
              "                       [ 1.1517e-01,  3.6676e-03, -1.3059e-03,  ...,  5.9460e-04,\n",
              "                         1.4055e-04, -1.4378e-02],\n",
              "                       ...,\n",
              "                       [-5.6807e-02, -2.2370e-02, -4.1647e-02,  ...,  7.5973e-02,\n",
              "                        -1.4161e-01, -2.2076e-02],\n",
              "                       [ 3.9987e-02,  9.3195e-02, -4.8807e-02,  ..., -5.2826e-02,\n",
              "                        -4.0831e-02, -4.6427e-02],\n",
              "                       [ 3.4979e-02, -7.4893e-02,  1.6359e-02,  ...,  1.2525e-01,\n",
              "                        -1.2308e-01, -3.1721e-02]], device='cuda:0')),\n",
              "              ('gru.bias_ih_l0',\n",
              "               tensor([ 9.0130e-02, -4.1493e-02,  1.0825e-01,  8.0016e-02,  1.1837e-01,\n",
              "                       -1.2412e-01,  2.8652e-02, -1.4898e-02,  5.9860e-03, -8.9918e-02,\n",
              "                        3.7831e-02,  9.1299e-04, -3.9853e-02, -1.1278e-01,  5.3925e-02,\n",
              "                       -1.9994e-02,  8.3877e-02,  5.0369e-03,  1.5474e-02,  1.0624e-02,\n",
              "                        1.4614e-02, -1.9207e-02, -1.0590e-01,  7.0527e-02, -6.6688e-02,\n",
              "                        6.1191e-02, -4.1854e-02,  6.7334e-02,  1.0595e-01, -6.4436e-02,\n",
              "                        1.0432e-01,  3.8624e-02, -8.9574e-02,  2.9791e-02,  5.5795e-02,\n",
              "                        1.0994e-01, -1.2219e-01,  1.1367e-01,  8.8577e-02,  1.2774e-01,\n",
              "                        5.2210e-02, -4.2430e-02,  8.8641e-02,  1.1557e-01, -2.5696e-02,\n",
              "                        4.0920e-04,  9.2795e-02,  9.6656e-02,  7.1814e-02,  8.9955e-02,\n",
              "                       -4.7025e-02,  1.7780e-05, -2.8402e-02,  4.7992e-02,  6.8433e-03,\n",
              "                       -9.1034e-02,  7.1240e-04, -9.8478e-02, -1.2031e-01,  1.1517e-01,\n",
              "                       -4.5212e-02,  3.5918e-02, -3.9112e-03, -2.0191e-03, -1.7609e-02,\n",
              "                        1.7377e-01, -5.1730e-02,  1.8037e-01,  6.7712e-02,  1.5692e-01,\n",
              "                       -1.9874e-01, -1.1627e-01, -1.1032e-01, -7.5844e-02, -7.2541e-02,\n",
              "                        1.2684e-02,  1.9145e-01,  1.1359e-01, -1.5879e-01, -7.6506e-02,\n",
              "                        1.0044e-01,  5.3493e-02,  8.6777e-02, -1.3106e-01,  1.4572e-02,\n",
              "                        1.1333e-02, -1.5083e-01,  1.5153e-01,  5.2170e-02, -1.4028e-01,\n",
              "                       -3.7664e-02, -7.6147e-02,  1.4652e-02, -1.0141e-01,  1.3476e-01,\n",
              "                       -2.0676e-01], device='cuda:0')),\n",
              "              ('gru.bias_hh_l0',\n",
              "               tensor([-4.9972e-02, -5.5647e-02,  1.3090e-02, -5.7904e-03, -2.8437e-03,\n",
              "                        1.0617e-01, -1.0665e-01,  2.2933e-02, -5.4746e-02, -5.4435e-04,\n",
              "                       -1.2305e-01, -8.4366e-02, -1.0402e-01,  3.5190e-02, -3.3827e-02,\n",
              "                        5.5672e-02, -5.3103e-02, -2.8701e-03,  4.6961e-03,  1.0322e-01,\n",
              "                       -3.6347e-03,  5.8008e-02,  1.1015e-02,  6.7229e-02,  6.6747e-02,\n",
              "                        3.6836e-02, -3.9617e-02, -7.8115e-03,  2.7368e-02,  5.3281e-02,\n",
              "                       -2.3372e-02,  7.5582e-02, -6.1770e-02,  5.5368e-02,  6.5535e-04,\n",
              "                       -7.6536e-02, -4.2384e-02, -8.9530e-02,  1.9760e-03, -6.0316e-02,\n",
              "                       -1.1582e-02, -7.4082e-05, -6.3240e-02, -1.2189e-04,  7.4782e-04,\n",
              "                        1.5811e-02,  8.2427e-02, -6.2137e-02,  5.2511e-02,  3.7536e-05,\n",
              "                       -5.6690e-02, -3.7072e-02,  7.4510e-03,  2.4997e-03, -9.2936e-05,\n",
              "                       -4.4335e-02,  4.5424e-02, -7.7579e-04, -1.2571e-01, -2.0531e-04,\n",
              "                       -9.5742e-03, -3.2710e-02, -5.3854e-02, -9.7759e-02,  1.4013e-01,\n",
              "                       -1.2330e-01,  1.2895e-01, -1.0108e-01, -7.7198e-02, -3.3836e-02,\n",
              "                       -1.3182e-01,  2.6387e-02,  1.2549e-01, -1.5241e-01, -1.1681e-01,\n",
              "                        5.9244e-02,  2.0756e-02,  7.2967e-02,  5.4919e-02, -3.0964e-02,\n",
              "                       -9.7518e-02,  9.8086e-02, -2.6565e-02, -1.5257e-01, -5.1786e-02,\n",
              "                       -5.6905e-02,  1.3495e-01,  7.0257e-02,  3.8121e-02, -1.4255e-02,\n",
              "                       -8.3626e-02,  8.8912e-02, -1.2925e-02, -8.1533e-02,  1.0920e-01,\n",
              "                       -1.2610e-01], device='cuda:0')),\n",
              "              ('fc.weight',\n",
              "               tensor([[ 0.0787,  0.5287, -0.0090,  0.2324, -0.4349,  0.1940, -0.1951, -0.1672,\n",
              "                         0.0961, -0.3450, -0.3804, -0.0408,  0.0147,  0.0864, -0.2975,  0.4195,\n",
              "                        -0.1383,  0.1586,  0.2218, -0.0470,  0.3304, -0.0913, -0.0987,  0.3906,\n",
              "                        -0.0460, -0.3172,  0.5396, -0.1751,  0.0195, -0.3224, -0.4060, -0.4216]],\n",
              "                      device='cuda:0')),\n",
              "              ('fc.bias', tensor([0.0736], device='cuda:0'))]))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "q0wvLXBEexPO",
        "outputId": "0657e74f-ef8b-49d1-af4f-2747dfb42de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      model_name                                     hyperparameter  \\\n",
              "0  zero_baseline                                            seed=42   \n",
              "1            gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "2            gru  {'weight_decay': 0.01, 'learning_rate': 0.0005...   \n",
              "3            gru  {'weight_decay': 0.01, 'learning_rate': 0.0001...   \n",
              "\n",
              "   best_epoch best_train_cost best_val_cost  best_train_recall  \\\n",
              "0           1            None          None           1.000000   \n",
              "1           1        0.237551      0.228324           1.000000   \n",
              "2           1        0.247199      0.235428           1.000000   \n",
              "3           2        0.268154      0.262874           0.384359   \n",
              "\n",
              "   best_train_precision  best_train_f1  best_val_recall  best_val_precision  \\\n",
              "0              0.721993       0.838555              1.0            0.721993   \n",
              "1              0.721993       0.837994              1.0            0.721993   \n",
              "2              0.721993       0.837963              1.0            0.721993   \n",
              "3              0.293013       0.322972              1.0            0.721993   \n",
              "\n",
              "   best_val_f1 best_test_recall best_test_precision best_test_f1  \n",
              "0     0.838555              1.0            0.721841     0.838452  \n",
              "1     0.838520             None                None         None  \n",
              "2     0.838520             None                None         None  \n",
              "3     0.838520             None                None         None  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c452691-0b23-467c-8028-50b183858845\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>hyperparameter</th>\n",
              "      <th>best_epoch</th>\n",
              "      <th>best_train_cost</th>\n",
              "      <th>best_val_cost</th>\n",
              "      <th>best_train_recall</th>\n",
              "      <th>best_train_precision</th>\n",
              "      <th>best_train_f1</th>\n",
              "      <th>best_val_recall</th>\n",
              "      <th>best_val_precision</th>\n",
              "      <th>best_val_f1</th>\n",
              "      <th>best_test_recall</th>\n",
              "      <th>best_test_precision</th>\n",
              "      <th>best_test_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_baseline</td>\n",
              "      <td>seed=42</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721841</td>\n",
              "      <td>0.838452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237551</td>\n",
              "      <td>0.228324</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837994</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0005...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.247199</td>\n",
              "      <td>0.235428</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837963</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0001...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.268154</td>\n",
              "      <td>0.262874</td>\n",
              "      <td>0.384359</td>\n",
              "      <td>0.293013</td>\n",
              "      <td>0.322972</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c452691-0b23-467c-8028-50b183858845')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6c452691-0b23-467c-8028-50b183858845 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6c452691-0b23-467c-8028-50b183858845');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/99dac6621f6ae8c4/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_baseline\",\n\"seed=42\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385551786070644,\n            'f': \"0.8385551786070644\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385551786070644,\n            'f': \"0.8385551786070644\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7218406593406593,\n            'f': \"0.7218406593406593\",\n        },\n{\n            'v': 0.8384523334662944,\n            'f': \"0.8384523334662944\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23755095152852188,\n            'f': \"0.23755095152852188\",\n        },\n{\n            'v': 0.2283238747890053,\n            'f': \"0.2283238747890053\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379937470948515,\n            'f': \"0.8379937470948515\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0005, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24719946554777686,\n            'f': \"0.24719946554777686\",\n        },\n{\n            'v': 0.235427502913983,\n            'f': \"0.235427502913983\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379629078268853,\n            'f': \"0.8379629078268853\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0.26815437865694225,\n            'f': \"0.26815437865694225\",\n        },\n{\n            'v': 0.2628738533180604,\n            'f': \"0.2628738533180604\",\n        },\n{\n            'v': 0.38435905446214724,\n            'f': \"0.38435905446214724\",\n        },\n{\n            'v': 0.2930126002290951,\n            'f': \"0.2930126002290951\",\n        },\n{\n            'v': 0.3229722700122437,\n            'f': \"0.3229722700122437\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"model_name\"], [\"string\", \"hyperparameter\"], [\"number\", \"best_epoch\"], [\"number\", \"best_train_cost\"], [\"number\", \"best_val_cost\"], [\"number\", \"best_train_recall\"], [\"number\", \"best_train_precision\"], [\"number\", \"best_train_f1\"], [\"number\", \"best_val_recall\"], [\"number\", \"best_val_precision\"], [\"number\", \"best_val_f1\"], [\"number\", \"best_test_recall\"], [\"number\", \"best_test_precision\"], [\"number\", \"best_test_f1\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: \"0\",\n      });\n    "
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#benchmark.to_csv(PATH + '/benchmark.csv', index=False)"
      ],
      "metadata": {
        "id": "RR4Xt5eimJq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the GRU-based models performed poorly on the task, with validation F1 scores that were similar to the zero baseline. This indicates that the models were unable to effectively learn from the sequence of heartbeats.\n",
        "\n",
        "This could be due to a number of factors, such as the use of zero masking and overly long sequences (>180). Despite this, we will perform one more experiment on these sequence-based models. If they continue to perform poorly, we will switch to a CNN-based model, which is typically easier to train.\n",
        "\n",
        "Additionally, we have found that the RNN-based models saturate quickly, even at early epochs, so we will reduce the number of epochs to 5 for the next experiment.\n",
        "\n",
        "In terms of selecting the threshold, we have used the default value of 0.5 as our initial threshold. However, the ideal threshold may vary depending on the specific model and hyperparameters. Therefore, in the next experiment, we will focus on finding the best architecture and hyperparameter settings, and will determine the optimal threshold afterwards."
      ],
      "metadata": {
        "id": "2nB8BN9t4vA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Define the hyperparameters to search over\n",
        "def experiment_1(benchmark, X_train = X_train ,y_train = y_train, X_val = X_val, y_val = y_val):\n",
        "\n",
        "    \"\"\"\n",
        "    Grid search on the sequence model namely GRU and LSTM only\n",
        "    (45 epochs)\n",
        "    \"\"\"\n",
        "    \n",
        "    name = ['gru', 'lstm']\n",
        "    weight_decay = [1e-2,1e-3]\n",
        "    lr = [1e-3, 9e-4, 7e-4]\n",
        "    batch_size = [128]\n",
        "\n",
        "    # For lstm and gru only\n",
        "    hidden_sizes = [32, 64]\n",
        "    num_layers_list = [1, 2]\n",
        "\n",
        "    # Create a Cartesian product of all hyperparameter combinations\n",
        "    hyperparams = list(itertools.product(name, weight_decay, lr, batch_size, hidden_sizes, num_layers_list))\n",
        "    print(f'grid search : perform {len(hyperparams)} searchs')\n",
        "\n",
        "    # Loop over each combination of hyperparameters and train/evaluate the model\n",
        "    best_val_f1 = 0.0\n",
        "    num_epochs = 5\n",
        "\n",
        "    train_set = MI_Dataset(X_train,y_train, truncate_to_max=True)\n",
        "    val_set = MI_Dataset(X_val,y_val, truncate_to_max=True)\n",
        "    for i, (name, wd, lr, bs, hidden_size, num_layers) in enumerate(hyperparams):\n",
        "\n",
        "        print(f'\\n 🔎 search {i} : {name} --- hidden_size : {hidden_size}, num_layers : {num_layers}, lr : {lr}, weight_decay : {wd}, batch_size : {bs}')\n",
        "\n",
        "        # Setup Device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Create the model, optimizer, and loss function\n",
        "        model = get_architecture(name = name, device = device, hidden_size=hidden_size, num_layers=num_layers)\n",
        "        pos_weight = get_weighted_for_bce(y_train)\n",
        "        criterion = nn.BCEWithLogitsLoss(weight=pos_weight.to(device))\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=wd)\n",
        "\n",
        "        # Create the data loaders\n",
        "        train_loader, val_loader, test_loader = get_loader(train_set, val_set, test_set, train_batch_size=bs)\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n",
        "        model, model_stat = train(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs)\n",
        "    \n",
        "        \n",
        "        # save the best model configuration based on validation F1 score\n",
        "        if model_stat['best_val_f1'] > best_val_f1:\n",
        "            best_val_f1 = model_stat['best_val_f1']\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "        \n",
        "        row = {'model_name' : name,\n",
        "            'hyperparameter' : {'weight_decay' : wd, 'learning_rate' : lr,\n",
        "                                'batch_size' : bs, 'hidden_size' : hidden_size,\n",
        "                                'num_layers' : num_layers},\n",
        "            'best_epoch' : model_stat['best_epoch'],\n",
        "            'best_train_cost' : model_stat['best_train_cost'],\n",
        "            'best_val_cost' : model_stat['best_val_cost'],\n",
        "            'best_train_recall' : model_stat['best_train_recall'],\n",
        "            'best_train_precision' : model_stat['best_train_precision'],\n",
        "            'best_train_f1' : model_stat['best_train_f1'],\n",
        "            'best_val_recall' : model_stat['best_val_recall'],\n",
        "            'best_val_precision' : model_stat['best_val_precision'],\n",
        "            'best_val_f1' : model_stat['best_val_f1'],\n",
        "            'best_test_recall' : None,\n",
        "            'best_test_precision' : None,\n",
        "            'best_test_f1' : None,\n",
        "            }\n",
        "\n",
        "        benchmark =benchmark.append(row, ignore_index=True)\n",
        "\n",
        "    return benchmark, best_model"
      ],
      "metadata": {
        "id": "jWphXtHfz_HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark, best_model = experiment_1(benchmark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb9FMRPlPs94",
        "outputId": "1a623db9-9611-4bb0-8e6c-85fd8a0f97bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid search : perform 48 searchs\n",
            "\n",
            " 🔎 search 0 : gru --- hidden_size : 32, num_layers : 1, lr : 0.001, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2375, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 1 : gru --- hidden_size : 32, num_layers : 2, lr : 0.001, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2377, Train Acc: 0.6924, Train F1: 0.7775 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8377 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 2 : gru --- hidden_size : 64, num_layers : 1, lr : 0.001, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2327, Train Acc: 0.7167, Train F1: 0.8262 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 3 : gru --- hidden_size : 64, num_layers : 2, lr : 0.001, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2309, Train Acc: 0.7153, Train F1: 0.8257 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 4 : gru --- hidden_size : 32, num_layers : 1, lr : 0.0009, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2460, Train Acc: 0.6441, Train F1: 0.6907 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 5 : gru --- hidden_size : 32, num_layers : 2, lr : 0.0009, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2387, Train Acc: 0.6666, Train F1: 0.7284 Val Loss: 0.2290, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2286, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8377 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 6 : gru --- hidden_size : 64, num_layers : 1, lr : 0.0009, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2314, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 7 : gru --- hidden_size : 64, num_layers : 2, lr : 0.0009, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2305, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 8 : gru --- hidden_size : 32, num_layers : 1, lr : 0.0007, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2583, Train Acc: 0.5765, Train F1: 0.5668 Val Loss: 0.2318, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2289, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 9 : gru --- hidden_size : 32, num_layers : 2, lr : 0.0007, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2306, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8377 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 10 : gru --- hidden_size : 64, num_layers : 1, lr : 0.0007, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2404, Train Acc: 0.6814, Train F1: 0.7637 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8377 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 11 : gru --- hidden_size : 64, num_layers : 2, lr : 0.0007, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2299, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 12 : gru --- hidden_size : 32, num_layers : 1, lr : 0.001, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2319, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 13 : gru --- hidden_size : 32, num_layers : 2, lr : 0.001, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2351, Train Acc: 0.6798, Train F1: 0.7634 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 14 : gru --- hidden_size : 64, num_layers : 1, lr : 0.001, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2366, Train Acc: 0.6865, Train F1: 0.7756 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 15 : gru --- hidden_size : 64, num_layers : 2, lr : 0.001, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2301, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2284, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 16 : gru --- hidden_size : 32, num_layers : 1, lr : 0.0009, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2478, Train Acc: 0.6246, Train F1: 0.6542 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 17 : gru --- hidden_size : 32, num_layers : 2, lr : 0.0009, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2345, Train Acc: 0.6906, Train F1: 0.7770 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8377 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 18 : gru --- hidden_size : 64, num_layers : 1, lr : 0.0009, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2327, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8377 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8377 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 19 : gru --- hidden_size : 64, num_layers : 2, lr : 0.0009, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2311, Train Acc: 0.7094, Train F1: 0.8135 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 20 : gru --- hidden_size : 32, num_layers : 1, lr : 0.0007, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2424, Train Acc: 0.6624, Train F1: 0.7268 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2279, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 21 : gru --- hidden_size : 32, num_layers : 2, lr : 0.0007, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2519, Train Acc: 0.6045, Train F1: 0.6165 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 22 : gru --- hidden_size : 64, num_layers : 1, lr : 0.0007, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2323, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 23 : gru --- hidden_size : 64, num_layers : 2, lr : 0.0007, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2309, Train Acc: 0.7153, Train F1: 0.8256 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 24 : lstm --- hidden_size : 32, num_layers : 1, lr : 0.001, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2466, Train Acc: 0.6890, Train F1: 0.7764 Val Loss: 0.2302, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2291, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2287, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2287, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2287, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2286, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2285, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2284, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 25 : lstm --- hidden_size : 32, num_layers : 2, lr : 0.001, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2481, Train Acc: 0.6356, Train F1: 0.6777 Val Loss: 0.2290, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2289, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2285, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2288, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2287, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 26 : lstm --- hidden_size : 64, num_layers : 1, lr : 0.001, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2404, Train Acc: 0.6966, Train F1: 0.7891 Val Loss: 0.2288, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2285, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2290, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2284, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 27 : lstm --- hidden_size : 64, num_layers : 2, lr : 0.001, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2336, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2289, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2285, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2286, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2286, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2287, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2284, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 28 : lstm --- hidden_size : 32, num_layers : 1, lr : 0.0009, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2440, Train Acc: 0.7160, Train F1: 0.8258 Val Loss: 0.2296, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2288, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2287, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2286, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2284, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 29 : lstm --- hidden_size : 32, num_layers : 2, lr : 0.0009, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2461, Train Acc: 0.6452, Train F1: 0.6910 Val Loss: 0.2287, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2289, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2294, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2289, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8382 Val Loss: 0.2286, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2286, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 30 : lstm --- hidden_size : 64, num_layers : 1, lr : 0.0009, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2334, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2294, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2285, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2281, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2282, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 31 : lstm --- hidden_size : 64, num_layers : 2, lr : 0.0009, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2393, Train Acc: 0.6773, Train F1: 0.7523 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2286, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2286, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 32 : lstm --- hidden_size : 32, num_layers : 1, lr : 0.0007, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2483, Train Acc: 0.6810, Train F1: 0.7637 Val Loss: 0.2310, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2292, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2283, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2284, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2284, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 33 : lstm --- hidden_size : 32, num_layers : 2, lr : 0.0007, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2385, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2290, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2288, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2288, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2289, Train Acc: 0.7220, Train F1: 0.8382 Val Loss: 0.2286, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2287, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2286, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2285, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 34 : lstm --- hidden_size : 64, num_layers : 1, lr : 0.0007, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2465, Train Acc: 0.6741, Train F1: 0.7511 Val Loss: 0.2291, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2288, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2287, Train Acc: 0.7220, Train F1: 0.8377 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2281, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2282, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 35 : lstm --- hidden_size : 64, num_layers : 2, lr : 0.0007, weight_decay : 0.01, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2416, Train Acc: 0.7030, Train F1: 0.8013 Val Loss: 0.2303, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2298, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2285, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2286, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2283, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2284, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2284, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 36 : lstm --- hidden_size : 32, num_layers : 1, lr : 0.001, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2376, Train Acc: 0.7099, Train F1: 0.8138 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 37 : lstm --- hidden_size : 32, num_layers : 2, lr : 0.001, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2318, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2285, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 38 : lstm --- hidden_size : 64, num_layers : 1, lr : 0.001, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2380, Train Acc: 0.6876, Train F1: 0.7758 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 39 : lstm --- hidden_size : 64, num_layers : 2, lr : 0.001, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2330, Train Acc: 0.7151, Train F1: 0.8256 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 40 : lstm --- hidden_size : 32, num_layers : 1, lr : 0.0009, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2339, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 41 : lstm --- hidden_size : 32, num_layers : 2, lr : 0.0009, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2433, Train Acc: 0.6425, Train F1: 0.6900 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2278, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 42 : lstm --- hidden_size : 64, num_layers : 1, lr : 0.0009, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2345, Train Acc: 0.7085, Train F1: 0.8133 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2280, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2279, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 43 : lstm --- hidden_size : 64, num_layers : 2, lr : 0.0009, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2331, Train Acc: 0.7092, Train F1: 0.8135 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2280, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 44 : lstm --- hidden_size : 32, num_layers : 1, lr : 0.0007, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2336, Train Acc: 0.7220, Train F1: 0.8377 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 45 : lstm --- hidden_size : 32, num_layers : 2, lr : 0.0007, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2354, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 46 : lstm --- hidden_size : 64, num_layers : 1, lr : 0.0007, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2361, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8376 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8381 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "\n",
            " 🔎 search 47 : lstm --- hidden_size : 64, num_layers : 2, lr : 0.0007, weight_decay : 0.001, batch_size : 128\n",
            "Epoch: 1/5, Train Loss: 0.2354, Train Acc: 0.6929, Train F1: 0.7877 Val Loss: 0.2277, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 2/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 3/5, Train Loss: 0.2277, Train Acc: 0.7220, Train F1: 0.8379 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 4/5, Train Loss: 0.2278, Train Acc: 0.7220, Train F1: 0.8380 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n",
            "Epoch: 5/5, Train Loss: 0.2276, Train Acc: 0.7220, Train F1: 0.8378 Val Loss: 0.2276, Val Acc: 0.7220, Val F1: 0.8385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3828
        },
        "id": "_4IUnD7vsYvs",
        "outputId": "c945de8e-116c-4275-aced-7a6d6269e04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       model_name                                     hyperparameter  \\\n",
              "0   zero_baseline                                            seed=42   \n",
              "1             gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "2             gru  {'weight_decay': 0.01, 'learning_rate': 0.0005...   \n",
              "3             gru  {'weight_decay': 0.01, 'learning_rate': 0.0001...   \n",
              "4             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "5             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "6             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "7             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "8             gru  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "9             gru  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "10            gru  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "11            gru  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "12            gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "13            gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "14            gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "15            gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "16            gru  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "17            gru  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "18            gru  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "19            gru  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "20            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "21            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "22            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "23            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "24            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "25            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "26            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "27            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "28           lstm  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "29           lstm  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "30           lstm  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "31           lstm  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "32           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "33           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "34           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "35           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "36           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "37           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "38           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "39           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "40           lstm  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "41           lstm  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "42           lstm  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "43           lstm  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "44           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "45           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "46           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "47           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "48           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "49           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "50           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "51           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "\n",
              "    best_epoch best_train_cost best_val_cost  best_train_recall  \\\n",
              "0            1            None          None           1.000000   \n",
              "1            1        0.237551      0.228324           1.000000   \n",
              "2            1        0.247199      0.235428           1.000000   \n",
              "3            2        0.268154      0.262874           0.384359   \n",
              "4            1        0.237496        0.2285           1.000000   \n",
              "5            1        0.237695      0.228308           0.926690   \n",
              "6            1        0.232703      0.227591           0.985338   \n",
              "7            1        0.230862       0.22799           0.985338   \n",
              "8            1        0.245969      0.228488           0.824055   \n",
              "9            1        0.238722      0.229009           0.868041   \n",
              "10           1        0.231409      0.227876           1.000000   \n",
              "11           1        0.230496      0.228034           1.000000   \n",
              "12           1        0.258267      0.231803           0.677434   \n",
              "13           1        0.230636      0.227791           1.000000   \n",
              "14           1        0.240435      0.227988           0.912027   \n",
              "15           1        0.229898      0.228008           1.000000   \n",
              "16           1        0.231874      0.227825           1.000000   \n",
              "17           1        0.235135      0.227605           0.912027   \n",
              "18           1        0.236581      0.228055           0.926690   \n",
              "19           1        0.230123      0.228134           1.000000   \n",
              "20           1        0.247814      0.227805           0.780228   \n",
              "21           1        0.234476       0.22773           0.926690   \n",
              "22           1        0.232707      0.227596           1.000000   \n",
              "23           1        0.231131      0.227829           0.970676   \n",
              "24           1        0.242391      0.228181           0.868041   \n",
              "25           1        0.251862      0.227611           0.736082   \n",
              "26           1        0.232329      0.227781           1.000000   \n",
              "27           1         0.23092      0.227611           0.985338   \n",
              "28           1        0.246597      0.230152           0.926690   \n",
              "29           1        0.248086       0.22897           0.809393   \n",
              "30           1        0.240369       0.22881           0.941352   \n",
              "31           1        0.233591      0.228861           1.000000   \n",
              "32           1        0.244041      0.229647           0.985338   \n",
              "33           1        0.246053      0.228722           0.824055   \n",
              "34           1        0.233413      0.229419           1.000000   \n",
              "35           1        0.239309      0.227842           0.897365   \n",
              "36           1        0.248298      0.230968           0.912027   \n",
              "37           1        0.238547      0.228958           1.000000   \n",
              "38           1         0.24648      0.229094           0.897365   \n",
              "39           1        0.241609      0.230321           0.956014   \n",
              "40           1        0.237574      0.227922           0.970676   \n",
              "41           1        0.231801      0.228474           1.000000   \n",
              "42           1        0.237987      0.227953           0.926690   \n",
              "43           1           0.233      0.227601           0.985338   \n",
              "44           1        0.233949      0.227641           1.000000   \n",
              "45           1        0.243257      0.227643           0.824055   \n",
              "46           1        0.234485      0.227701           0.970676   \n",
              "47           1        0.233095      0.227593           0.970676   \n",
              "48           1        0.233624      0.227623           1.000000   \n",
              "49           1        0.235389      0.227735           1.000000   \n",
              "50           1        0.236105      0.227592           1.000000   \n",
              "51           1         0.23537      0.227736           0.941352   \n",
              "\n",
              "    best_train_precision  best_train_f1  best_val_recall  best_val_precision  \\\n",
              "0               0.721993       0.838555              1.0            0.721993   \n",
              "1               0.721993       0.837994              1.0            0.721993   \n",
              "2               0.721993       0.837963              1.0            0.721993   \n",
              "3               0.293013       0.322972              1.0            0.721993   \n",
              "4               0.721993       0.837926              1.0            0.721993   \n",
              "5               0.670561       0.777513              1.0            0.721993   \n",
              "6               0.712027       0.826183              1.0            0.721993   \n",
              "7               0.711340       0.825672              1.0            0.721993   \n",
              "8               0.595074       0.690655              1.0            0.721993   \n",
              "9               0.628293       0.728386              1.0            0.721993   \n",
              "10              0.721993       0.837847              1.0            0.721993   \n",
              "11              0.721993       0.837790              1.0            0.721993   \n",
              "12              0.487973       0.566772              1.0            0.721993   \n",
              "13              0.721993       0.837965              1.0            0.721993   \n",
              "14              0.657732       0.763680              1.0            0.721993   \n",
              "15              0.721993       0.837944              1.0            0.721993   \n",
              "16              0.721993       0.837898              1.0            0.721993   \n",
              "17              0.656930       0.763369              1.0            0.721993   \n",
              "18              0.667583       0.775602              1.0            0.721993   \n",
              "19              0.721993       0.838059              1.0            0.721993   \n",
              "20              0.570676       0.654196              1.0            0.721993   \n",
              "21              0.669645       0.776967              1.0            0.721993   \n",
              "22              0.721993       0.837959              1.0            0.721993   \n",
              "23              0.701031       0.813465              1.0            0.721993   \n",
              "24              0.626231       0.726825              1.0            0.721993   \n",
              "25              0.531271       0.616538              1.0            0.721993   \n",
              "26              0.721993       0.838102              1.0            0.721993   \n",
              "27              0.711340       0.825631              1.0            0.721993   \n",
              "28              0.668843       0.776429              1.0            0.721993   \n",
              "29              0.583505       0.677718              1.0            0.721993   \n",
              "30              0.679954       0.789097              1.0            0.721993   \n",
              "31              0.721993       0.837985              1.0            0.721993   \n",
              "32              0.711684       0.825765              1.0            0.721993   \n",
              "33              0.595647       0.691012              1.0            0.721993   \n",
              "34              0.721993       0.837910              1.0            0.721993   \n",
              "35              0.648339       0.752346              1.0            0.721993   \n",
              "36              0.657503       0.763726              1.0            0.721993   \n",
              "37              0.721993       0.837937              1.0            0.721993   \n",
              "38              0.646735       0.751126              1.0            0.721993   \n",
              "39              0.690493       0.801252              1.0            0.721993   \n",
              "40              0.701260       0.813779              1.0            0.721993   \n",
              "41              0.721993       0.837870              1.0            0.721993   \n",
              "42              0.668156       0.775835              1.0            0.721993   \n",
              "43              0.711226       0.825556              1.0            0.721993   \n",
              "44              0.721993       0.838066              1.0            0.721993   \n",
              "45              0.594273       0.689978              1.0            0.721993   \n",
              "46              0.700573       0.813271              1.0            0.721993   \n",
              "47              0.700916       0.813547              1.0            0.721993   \n",
              "48              0.721993       0.837748              1.0            0.721993   \n",
              "49              0.721993       0.837773              1.0            0.721993   \n",
              "50              0.721993       0.837890              1.0            0.721993   \n",
              "51              0.678121       0.787724              1.0            0.721993   \n",
              "\n",
              "    best_val_f1 best_test_recall best_test_precision best_test_f1  \n",
              "0      0.838555              1.0            0.721841     0.838452  \n",
              "1      0.838520             None                None         None  \n",
              "2      0.838520             None                None         None  \n",
              "3      0.838520             None                None         None  \n",
              "4      0.838520             None                None         None  \n",
              "5      0.838520             None                None         None  \n",
              "6      0.838520             None                None         None  \n",
              "7      0.838520             None                None         None  \n",
              "8      0.838520             None                None         None  \n",
              "9      0.838520             None                None         None  \n",
              "10     0.838520             None                None         None  \n",
              "11     0.838520             None                None         None  \n",
              "12     0.838520             None                None         None  \n",
              "13     0.838520             None                None         None  \n",
              "14     0.838520             None                None         None  \n",
              "15     0.838520             None                None         None  \n",
              "16     0.838520             None                None         None  \n",
              "17     0.838520             None                None         None  \n",
              "18     0.838520             None                None         None  \n",
              "19     0.838520             None                None         None  \n",
              "20     0.838520             None                None         None  \n",
              "21     0.838520             None                None         None  \n",
              "22     0.838520             None                None         None  \n",
              "23     0.838520             None                None         None  \n",
              "24     0.838520             None                None         None  \n",
              "25     0.838520             None                None         None  \n",
              "26     0.838520             None                None         None  \n",
              "27     0.838520             None                None         None  \n",
              "28     0.838520             None                None         None  \n",
              "29     0.838520             None                None         None  \n",
              "30     0.838520             None                None         None  \n",
              "31     0.838520             None                None         None  \n",
              "32     0.838520             None                None         None  \n",
              "33     0.838520             None                None         None  \n",
              "34     0.838520             None                None         None  \n",
              "35     0.838520             None                None         None  \n",
              "36     0.838520             None                None         None  \n",
              "37     0.838520             None                None         None  \n",
              "38     0.838520             None                None         None  \n",
              "39     0.838520             None                None         None  \n",
              "40     0.838520             None                None         None  \n",
              "41     0.838520             None                None         None  \n",
              "42     0.838520             None                None         None  \n",
              "43     0.838520             None                None         None  \n",
              "44     0.838520             None                None         None  \n",
              "45     0.838520             None                None         None  \n",
              "46     0.838520             None                None         None  \n",
              "47     0.838520             None                None         None  \n",
              "48     0.838520             None                None         None  \n",
              "49     0.838520             None                None         None  \n",
              "50     0.838520             None                None         None  \n",
              "51     0.838520             None                None         None  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-98a1e360-b2d7-4008-a193-fbf5eaa0f2d2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>hyperparameter</th>\n",
              "      <th>best_epoch</th>\n",
              "      <th>best_train_cost</th>\n",
              "      <th>best_val_cost</th>\n",
              "      <th>best_train_recall</th>\n",
              "      <th>best_train_precision</th>\n",
              "      <th>best_train_f1</th>\n",
              "      <th>best_val_recall</th>\n",
              "      <th>best_val_precision</th>\n",
              "      <th>best_val_f1</th>\n",
              "      <th>best_test_recall</th>\n",
              "      <th>best_test_precision</th>\n",
              "      <th>best_test_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_baseline</td>\n",
              "      <td>seed=42</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721841</td>\n",
              "      <td>0.838452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237551</td>\n",
              "      <td>0.228324</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837994</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0005...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.247199</td>\n",
              "      <td>0.235428</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837963</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0001...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.268154</td>\n",
              "      <td>0.262874</td>\n",
              "      <td>0.384359</td>\n",
              "      <td>0.293013</td>\n",
              "      <td>0.322972</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237496</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837926</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237695</td>\n",
              "      <td>0.228308</td>\n",
              "      <td>0.926690</td>\n",
              "      <td>0.670561</td>\n",
              "      <td>0.777513</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.232703</td>\n",
              "      <td>0.227591</td>\n",
              "      <td>0.985338</td>\n",
              "      <td>0.712027</td>\n",
              "      <td>0.826183</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.230862</td>\n",
              "      <td>0.22799</td>\n",
              "      <td>0.985338</td>\n",
              "      <td>0.711340</td>\n",
              "      <td>0.825672</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.245969</td>\n",
              "      <td>0.228488</td>\n",
              "      <td>0.824055</td>\n",
              "      <td>0.595074</td>\n",
              "      <td>0.690655</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.238722</td>\n",
              "      <td>0.229009</td>\n",
              "      <td>0.868041</td>\n",
              "      <td>0.628293</td>\n",
              "      <td>0.728386</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.231409</td>\n",
              "      <td>0.227876</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837847</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.230496</td>\n",
              "      <td>0.228034</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837790</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.258267</td>\n",
              "      <td>0.231803</td>\n",
              "      <td>0.677434</td>\n",
              "      <td>0.487973</td>\n",
              "      <td>0.566772</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.230636</td>\n",
              "      <td>0.227791</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837965</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.240435</td>\n",
              "      <td>0.227988</td>\n",
              "      <td>0.912027</td>\n",
              "      <td>0.657732</td>\n",
              "      <td>0.763680</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.229898</td>\n",
              "      <td>0.228008</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837944</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.231874</td>\n",
              "      <td>0.227825</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837898</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.235135</td>\n",
              "      <td>0.227605</td>\n",
              "      <td>0.912027</td>\n",
              "      <td>0.656930</td>\n",
              "      <td>0.763369</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.236581</td>\n",
              "      <td>0.228055</td>\n",
              "      <td>0.926690</td>\n",
              "      <td>0.667583</td>\n",
              "      <td>0.775602</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.230123</td>\n",
              "      <td>0.228134</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838059</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.247814</td>\n",
              "      <td>0.227805</td>\n",
              "      <td>0.780228</td>\n",
              "      <td>0.570676</td>\n",
              "      <td>0.654196</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.234476</td>\n",
              "      <td>0.22773</td>\n",
              "      <td>0.926690</td>\n",
              "      <td>0.669645</td>\n",
              "      <td>0.776967</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.232707</td>\n",
              "      <td>0.227596</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837959</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.231131</td>\n",
              "      <td>0.227829</td>\n",
              "      <td>0.970676</td>\n",
              "      <td>0.701031</td>\n",
              "      <td>0.813465</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.242391</td>\n",
              "      <td>0.228181</td>\n",
              "      <td>0.868041</td>\n",
              "      <td>0.626231</td>\n",
              "      <td>0.726825</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.251862</td>\n",
              "      <td>0.227611</td>\n",
              "      <td>0.736082</td>\n",
              "      <td>0.531271</td>\n",
              "      <td>0.616538</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.232329</td>\n",
              "      <td>0.227781</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838102</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.23092</td>\n",
              "      <td>0.227611</td>\n",
              "      <td>0.985338</td>\n",
              "      <td>0.711340</td>\n",
              "      <td>0.825631</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.246597</td>\n",
              "      <td>0.230152</td>\n",
              "      <td>0.926690</td>\n",
              "      <td>0.668843</td>\n",
              "      <td>0.776429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.248086</td>\n",
              "      <td>0.22897</td>\n",
              "      <td>0.809393</td>\n",
              "      <td>0.583505</td>\n",
              "      <td>0.677718</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.240369</td>\n",
              "      <td>0.22881</td>\n",
              "      <td>0.941352</td>\n",
              "      <td>0.679954</td>\n",
              "      <td>0.789097</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233591</td>\n",
              "      <td>0.228861</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837985</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.244041</td>\n",
              "      <td>0.229647</td>\n",
              "      <td>0.985338</td>\n",
              "      <td>0.711684</td>\n",
              "      <td>0.825765</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.246053</td>\n",
              "      <td>0.228722</td>\n",
              "      <td>0.824055</td>\n",
              "      <td>0.595647</td>\n",
              "      <td>0.691012</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233413</td>\n",
              "      <td>0.229419</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837910</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.239309</td>\n",
              "      <td>0.227842</td>\n",
              "      <td>0.897365</td>\n",
              "      <td>0.648339</td>\n",
              "      <td>0.752346</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.248298</td>\n",
              "      <td>0.230968</td>\n",
              "      <td>0.912027</td>\n",
              "      <td>0.657503</td>\n",
              "      <td>0.763726</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.238547</td>\n",
              "      <td>0.228958</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837937</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24648</td>\n",
              "      <td>0.229094</td>\n",
              "      <td>0.897365</td>\n",
              "      <td>0.646735</td>\n",
              "      <td>0.751126</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.241609</td>\n",
              "      <td>0.230321</td>\n",
              "      <td>0.956014</td>\n",
              "      <td>0.690493</td>\n",
              "      <td>0.801252</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237574</td>\n",
              "      <td>0.227922</td>\n",
              "      <td>0.970676</td>\n",
              "      <td>0.701260</td>\n",
              "      <td>0.813779</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.231801</td>\n",
              "      <td>0.228474</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837870</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237987</td>\n",
              "      <td>0.227953</td>\n",
              "      <td>0.926690</td>\n",
              "      <td>0.668156</td>\n",
              "      <td>0.775835</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.227601</td>\n",
              "      <td>0.985338</td>\n",
              "      <td>0.711226</td>\n",
              "      <td>0.825556</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233949</td>\n",
              "      <td>0.227641</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838066</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.243257</td>\n",
              "      <td>0.227643</td>\n",
              "      <td>0.824055</td>\n",
              "      <td>0.594273</td>\n",
              "      <td>0.689978</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.234485</td>\n",
              "      <td>0.227701</td>\n",
              "      <td>0.970676</td>\n",
              "      <td>0.700573</td>\n",
              "      <td>0.813271</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233095</td>\n",
              "      <td>0.227593</td>\n",
              "      <td>0.970676</td>\n",
              "      <td>0.700916</td>\n",
              "      <td>0.813547</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233624</td>\n",
              "      <td>0.227623</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837748</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.235389</td>\n",
              "      <td>0.227735</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837773</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.236105</td>\n",
              "      <td>0.227592</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837890</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.23537</td>\n",
              "      <td>0.227736</td>\n",
              "      <td>0.941352</td>\n",
              "      <td>0.678121</td>\n",
              "      <td>0.787724</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98a1e360-b2d7-4008-a193-fbf5eaa0f2d2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-98a1e360-b2d7-4008-a193-fbf5eaa0f2d2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-98a1e360-b2d7-4008-a193-fbf5eaa0f2d2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/99dac6621f6ae8c4/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_baseline\",\n\"seed=42\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385551786070644,\n            'f': \"0.8385551786070644\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385551786070644,\n            'f': \"0.8385551786070644\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7218406593406593,\n            'f': \"0.7218406593406593\",\n        },\n{\n            'v': 0.8384523334662944,\n            'f': \"0.8384523334662944\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23755095152852188,\n            'f': \"0.23755095152852188\",\n        },\n{\n            'v': 0.2283238747890053,\n            'f': \"0.2283238747890053\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379937470948515,\n            'f': \"0.8379937470948515\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0005, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24719946554777686,\n            'f': \"0.24719946554777686\",\n        },\n{\n            'v': 0.235427502913983,\n            'f': \"0.235427502913983\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379629078268853,\n            'f': \"0.8379629078268853\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0.26815437865694225,\n            'f': \"0.26815437865694225\",\n        },\n{\n            'v': 0.2628738533180604,\n            'f': \"0.2628738533180604\",\n        },\n{\n            'v': 0.38435905446214724,\n            'f': \"0.38435905446214724\",\n        },\n{\n            'v': 0.2930126002290951,\n            'f': \"0.2930126002290951\",\n        },\n{\n            'v': 0.3229722700122437,\n            'f': \"0.3229722700122437\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23749594646426,\n            'f': \"0.23749594646426\",\n        },\n{\n            'v': 0.22849985904914816,\n            'f': \"0.22849985904914816\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379264789299304,\n            'f': \"0.8379264789299304\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23769533075008195,\n            'f': \"0.23769533075008195\",\n        },\n{\n            'v': 0.22830794468982932,\n            'f': \"0.22830794468982932\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6705612829324169,\n            'f': \"0.6705612829324169\",\n        },\n{\n            'v': 0.777512641968225,\n            'f': \"0.777512641968225\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 6,\n            'f': \"6\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23270269353092057,\n            'f': \"0.23270269353092057\",\n        },\n{\n            'v': 0.2275912654051666,\n            'f': \"0.2275912654051666\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.7120274914089347,\n            'f': \"0.7120274914089347\",\n        },\n{\n            'v': 0.8261833872750113,\n            'f': \"0.8261833872750113\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 7,\n            'f': \"7\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23086243700940176,\n            'f': \"0.23086243700940176\",\n        },\n{\n            'v': 0.22798991973457466,\n            'f': \"0.22798991973457466\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.711340206185567,\n            'f': \"0.711340206185567\",\n        },\n{\n            'v': 0.8256718950601964,\n            'f': \"0.8256718950601964\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 8,\n            'f': \"8\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24596911503550387,\n            'f': \"0.24596911503550387\",\n        },\n{\n            'v': 0.22848816736252447,\n            'f': \"0.22848816736252447\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5950744558991982,\n            'f': \"0.5950744558991982\",\n        },\n{\n            'v': 0.6906552425563023,\n            'f': \"0.6906552425563023\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 9,\n            'f': \"9\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23872165301684525,\n            'f': \"0.23872165301684525\",\n        },\n{\n            'v': 0.22900927653632214,\n            'f': \"0.22900927653632214\",\n        },\n{\n            'v': 0.8680412371134021,\n            'f': \"0.8680412371134021\",\n        },\n{\n            'v': 0.6282932416953035,\n            'f': \"0.6282932416953035\",\n        },\n{\n            'v': 0.7283860077496157,\n            'f': \"0.7283860077496157\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 10,\n            'f': \"10\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23140935585124658,\n            'f': \"0.23140935585124658\",\n        },\n{\n            'v': 0.2278759803661366,\n            'f': \"0.2278759803661366\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378469602887191,\n            'f': \"0.8378469602887191\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 11,\n            'f': \"11\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23049640620696996,\n            'f': \"0.23049640620696996\",\n        },\n{\n            'v': 0.2280344991647091,\n            'f': \"0.2280344991647091\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377898005373192,\n            'f': \"0.8377898005373192\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 12,\n            'f': \"12\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2582674642540472,\n            'f': \"0.2582674642540472\",\n        },\n{\n            'v': 0.2318031070261067,\n            'f': \"0.2318031070261067\",\n        },\n{\n            'v': 0.677434135166094,\n            'f': \"0.677434135166094\",\n        },\n{\n            'v': 0.4879725085910653,\n            'f': \"0.4879725085910653\",\n        },\n{\n            'v': 0.5667718616335917,\n            'f': \"0.5667718616335917\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 13,\n            'f': \"13\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23063589629438735,\n            'f': \"0.23063589629438735\",\n        },\n{\n            'v': 0.22779080836019155,\n            'f': \"0.22779080836019155\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379650916960658,\n            'f': \"0.8379650916960658\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 14,\n            'f': \"14\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24043467447656672,\n            'f': \"0.24043467447656672\",\n        },\n{\n            'v': 0.22798836162614658,\n            'f': \"0.22798836162614658\",\n        },\n{\n            'v': 0.9120274914089347,\n            'f': \"0.9120274914089347\",\n        },\n{\n            'v': 0.6577319587628866,\n            'f': \"0.6577319587628866\",\n        },\n{\n            'v': 0.7636797948350783,\n            'f': \"0.7636797948350783\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 15,\n            'f': \"15\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.22989786069281185,\n            'f': \"0.22989786069281185\",\n        },\n{\n            'v': 0.22800785562221948,\n            'f': \"0.22800785562221948\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379435108647284,\n            'f': \"0.8379435108647284\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 16,\n            'f': \"16\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23187371032549345,\n            'f': \"0.23187371032549345\",\n        },\n{\n            'v': 0.22782516483588727,\n            'f': \"0.22782516483588727\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378981891847632,\n            'f': \"0.8378981891847632\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 17,\n            'f': \"17\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23513470497259706,\n            'f': \"0.23513470497259706\",\n        },\n{\n            'v': 0.22760502922371081,\n            'f': \"0.22760502922371081\",\n        },\n{\n            'v': 0.9120274914089347,\n            'f': \"0.9120274914089347\",\n        },\n{\n            'v': 0.656930126002291,\n            'f': \"0.656930126002291\",\n        },\n{\n            'v': 0.7633690624041659,\n            'f': \"0.7633690624041659\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 18,\n            'f': \"18\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23658089066630514,\n            'f': \"0.23658089066630514\",\n        },\n{\n            'v': 0.22805524370104996,\n            'f': \"0.22805524370104996\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6675830469644902,\n            'f': \"0.6675830469644902\",\n        },\n{\n            'v': 0.775601528856061,\n            'f': \"0.775601528856061\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 19,\n            'f': \"19\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23012308565406864,\n            'f': \"0.23012308565406864\",\n        },\n{\n            'v': 0.2281344659754501,\n            'f': \"0.2281344659754501\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8380585970171258,\n            'f': \"0.8380585970171258\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 20,\n            'f': \"20\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24781396114539445,\n            'f': \"0.24781396114539445\",\n        },\n{\n            'v': 0.22780490974380388,\n            'f': \"0.22780490974380388\",\n        },\n{\n            'v': 0.7802280990089148,\n            'f': \"0.7802280990089148\",\n        },\n{\n            'v': 0.5706758304696449,\n            'f': \"0.5706758304696449\",\n        },\n{\n            'v': 0.6541961675540556,\n            'f': \"0.6541961675540556\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 21,\n            'f': \"21\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2344763796422353,\n            'f': \"0.2344763796422353\",\n        },\n{\n            'v': 0.22773042042640476,\n            'f': \"0.22773042042640476\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6696449026345933,\n            'f': \"0.6696449026345933\",\n        },\n{\n            'v': 0.7769669688757592,\n            'f': \"0.7769669688757592\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 22,\n            'f': \"22\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23270658465253802,\n            'f': \"0.23270658465253802\",\n        },\n{\n            'v': 0.22759577791715405,\n            'f': \"0.22759577791715405\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379593694587404,\n            'f': \"0.8379593694587404\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 23,\n            'f': \"23\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23113079530540065,\n            'f': \"0.23113079530540065\",\n        },\n{\n            'v': 0.22782862943267496,\n            'f': \"0.22782862943267496\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7010309278350515,\n            'f': \"0.7010309278350515\",\n        },\n{\n            'v': 0.8134646001971478,\n            'f': \"0.8134646001971478\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 24,\n            'f': \"24\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24239094583370976,\n            'f': \"0.24239094583370976\",\n        },\n{\n            'v': 0.22818102553947683,\n            'f': \"0.22818102553947683\",\n        },\n{\n            'v': 0.8680412371134021,\n            'f': \"0.8680412371134021\",\n        },\n{\n            'v': 0.6262313860252005,\n            'f': \"0.6262313860252005\",\n        },\n{\n            'v': 0.7268249415921961,\n            'f': \"0.7268249415921961\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 25,\n            'f': \"25\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2518623998826039,\n            'f': \"0.2518623998826039\",\n        },\n{\n            'v': 0.22761076536170397,\n            'f': \"0.22761076536170397\",\n        },\n{\n            'v': 0.7360824742268042,\n            'f': \"0.7360824742268042\",\n        },\n{\n            'v': 0.5312714776632302,\n            'f': \"0.5312714776632302\",\n        },\n{\n            'v': 0.6165380489730845,\n            'f': \"0.6165380489730845\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 26,\n            'f': \"26\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.232329122092306,\n            'f': \"0.232329122092306\",\n        },\n{\n            'v': 0.2277810290507025,\n            'f': \"0.2277810290507025\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8381023417873185,\n            'f': \"0.8381023417873185\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 27,\n            'f': \"27\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23091984057003143,\n            'f': \"0.23091984057003143\",\n        },\n{\n            'v': 0.22761146157877551,\n            'f': \"0.22761146157877551\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.711340206185567,\n            'f': \"0.711340206185567\",\n        },\n{\n            'v': 0.8256305016925937,\n            'f': \"0.8256305016925937\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 28,\n            'f': \"28\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24659692590452414,\n            'f': \"0.24659692590452414\",\n        },\n{\n            'v': 0.2301524313557189,\n            'f': \"0.2301524313557189\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6688430698739977,\n            'f': \"0.6688430698739977\",\n        },\n{\n            'v': 0.7764289639170969,\n            'f': \"0.7764289639170969\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 29,\n            'f': \"29\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24808622859802443,\n            'f': \"0.24808622859802443\",\n        },\n{\n            'v': 0.22897008621610726,\n            'f': \"0.22897008621610726\",\n        },\n{\n            'v': 0.8093928980526919,\n            'f': \"0.8093928980526919\",\n        },\n{\n            'v': 0.5835051546391753,\n            'f': \"0.5835051546391753\",\n        },\n{\n            'v': 0.6777182478215532,\n            'f': \"0.6777182478215532\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 30,\n            'f': \"30\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24036915460676125,\n            'f': \"0.24036915460676125\",\n        },\n{\n            'v': 0.22881030144355552,\n            'f': \"0.22881030144355552\",\n        },\n{\n            'v': 0.9413516609392898,\n            'f': \"0.9413516609392898\",\n        },\n{\n            'v': 0.6799541809851088,\n            'f': \"0.6799541809851088\",\n        },\n{\n            'v': 0.7890973790044737,\n            'f': \"0.7890973790044737\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 31,\n            'f': \"31\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.233591164599033,\n            'f': \"0.233591164599033\",\n        },\n{\n            'v': 0.2288613215549705,\n            'f': \"0.2288613215549705\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379850303243254,\n            'f': \"0.8379850303243254\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 32,\n            'f': \"32\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24404066760105775,\n            'f': \"0.24404066760105775\",\n        },\n{\n            'v': 0.22964662380030065,\n            'f': \"0.22964662380030065\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.7116838487972509,\n            'f': \"0.7116838487972509\",\n        },\n{\n            'v': 0.8257649556000859,\n            'f': \"0.8257649556000859\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 33,\n            'f': \"33\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24605310336011382,\n            'f': \"0.24605310336011382\",\n        },\n{\n            'v': 0.22872167988536285,\n            'f': \"0.22872167988536285\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5956471935853379,\n            'f': \"0.5956471935853379\",\n        },\n{\n            'v': 0.6910118096782196,\n            'f': \"0.6910118096782196\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 34,\n            'f': \"34\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2334126355790054,\n            'f': \"0.2334126355790054\",\n        },\n{\n            'v': 0.2294187471321768,\n            'f': \"0.2294187471321768\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379098005498145,\n            'f': \"0.8379098005498145\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 35,\n            'f': \"35\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23930862564742905,\n            'f': \"0.23930862564742905\",\n        },\n{\n            'v': 0.22784182982346446,\n            'f': \"0.22784182982346446\",\n        },\n{\n            'v': 0.8973654066437572,\n            'f': \"0.8973654066437572\",\n        },\n{\n            'v': 0.6483390607101948,\n            'f': \"0.6483390607101948\",\n        },\n{\n            'v': 0.7523462888410456,\n            'f': \"0.7523462888410456\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 36,\n            'f': \"36\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24829804776460446,\n            'f': \"0.24829804776460446\",\n        },\n{\n            'v': 0.23096789029455678,\n            'f': \"0.23096789029455678\",\n        },\n{\n            'v': 0.9120274914089347,\n            'f': \"0.9120274914089347\",\n        },\n{\n            'v': 0.6575028636884307,\n            'f': \"0.6575028636884307\",\n        },\n{\n            'v': 0.7637263087855324,\n            'f': \"0.7637263087855324\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 37,\n            'f': \"37\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23854704901046905,\n            'f': \"0.23854704901046905\",\n        },\n{\n            'v': 0.2289582008860775,\n            'f': \"0.2289582008860775\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.837937023496059,\n            'f': \"0.837937023496059\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 38,\n            'f': \"38\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24648009746208496,\n            'f': \"0.24648009746208496\",\n        },\n{\n            'v': 0.22909368780470385,\n            'f': \"0.22909368780470385\",\n        },\n{\n            'v': 0.8973654066437572,\n            'f': \"0.8973654066437572\",\n        },\n{\n            'v': 0.6467353951890035,\n            'f': \"0.6467353951890035\",\n        },\n{\n            'v': 0.7511256735881737,\n            'f': \"0.7511256735881737\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 39,\n            'f': \"39\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24160942262868565,\n            'f': \"0.24160942262868565\",\n        },\n{\n            'v': 0.23032080584375308,\n            'f': \"0.23032080584375308\",\n        },\n{\n            'v': 0.9560137457044674,\n            'f': \"0.9560137457044674\",\n        },\n{\n            'v': 0.6904925544100802,\n            'f': \"0.6904925544100802\",\n        },\n{\n            'v': 0.8012523489553404,\n            'f': \"0.8012523489553404\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 40,\n            'f': \"40\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23757384251483118,\n            'f': \"0.23757384251483118\",\n        },\n{\n            'v': 0.2279223526159103,\n            'f': \"0.2279223526159103\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7012600229095074,\n            'f': \"0.7012600229095074\",\n        },\n{\n            'v': 0.8137792079514921,\n            'f': \"0.8137792079514921\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 41,\n            'f': \"41\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23180127218621702,\n            'f': \"0.23180127218621702\",\n        },\n{\n            'v': 0.22847400504289214,\n            'f': \"0.22847400504289214\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378703062107288,\n            'f': \"0.8378703062107288\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 42,\n            'f': \"42\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23798671251788037,\n            'f': \"0.23798671251788037\",\n        },\n{\n            'v': 0.22795328791813343,\n            'f': \"0.22795328791813343\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.66815578465063,\n            'f': \"0.66815578465063\",\n        },\n{\n            'v': 0.7758350982211567,\n            'f': \"0.7758350982211567\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 43,\n            'f': \"43\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23300043067981288,\n            'f': \"0.23300043067981288\",\n        },\n{\n            'v': 0.22760143634379934,\n            'f': \"0.22760143634379934\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.7112256586483391,\n            'f': \"0.7112256586483391\",\n        },\n{\n            'v': 0.8255556394557227,\n            'f': \"0.8255556394557227\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 44,\n            'f': \"44\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23394868339050268,\n            'f': \"0.23394868339050268\",\n        },\n{\n            'v': 0.22764059617552151,\n            'f': \"0.22764059617552151\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8380657319957353,\n            'f': \"0.8380657319957353\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 45,\n            'f': \"45\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24325722402723385,\n            'f': \"0.24325722402723385\",\n        },\n{\n            'v': 0.22764322254870764,\n            'f': \"0.22764322254870764\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5942726231386025,\n            'f': \"0.5942726231386025\",\n        },\n{\n            'v': 0.6899781748662641,\n            'f': \"0.6899781748662641\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 46,\n            'f': \"46\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23448481141223668,\n            'f': \"0.23448481141223668\",\n        },\n{\n            'v': 0.22770088012890308,\n            'f': \"0.22770088012890308\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7005727376861397,\n            'f': \"0.7005727376861397\",\n        },\n{\n            'v': 0.8132708898924303,\n            'f': \"0.8132708898924303\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 47,\n            'f': \"47\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2330947979892106,\n            'f': \"0.2330947979892106\",\n        },\n{\n            'v': 0.22759252305293,\n            'f': \"0.22759252305293\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7009163802978235,\n            'f': \"0.7009163802978235\",\n        },\n{\n            'v': 0.8135473343847958,\n            'f': \"0.8135473343847958\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 48,\n            'f': \"48\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23362354004983912,\n            'f': \"0.23362354004983912\",\n        },\n{\n            'v': 0.22762263073134667,\n            'f': \"0.22762263073134667\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377483941926969,\n            'f': \"0.8377483941926969\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 49,\n            'f': \"49\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2353885561330212,\n            'f': \"0.2353885561330212\",\n        },\n{\n            'v': 0.22773463401392974,\n            'f': \"0.22773463401392974\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377726111607469,\n            'f': \"0.8377726111607469\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 50,\n            'f': \"50\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23610532127090336,\n            'f': \"0.23610532127090336\",\n        },\n{\n            'v': 0.22759167250898696,\n            'f': \"0.22759167250898696\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378901998288699,\n            'f': \"0.8378901998288699\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 51,\n            'f': \"51\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23536989676925052,\n            'f': \"0.23536989676925052\",\n        },\n{\n            'v': 0.22773557217260407,\n            'f': \"0.22773557217260407\",\n        },\n{\n            'v': 0.9413516609392898,\n            'f': \"0.9413516609392898\",\n        },\n{\n            'v': 0.6781214203894617,\n            'f': \"0.6781214203894617\",\n        },\n{\n            'v': 0.7877237131052639,\n            'f': \"0.7877237131052639\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"model_name\"], [\"string\", \"hyperparameter\"], [\"number\", \"best_epoch\"], [\"number\", \"best_train_cost\"], [\"number\", \"best_val_cost\"], [\"number\", \"best_train_recall\"], [\"number\", \"best_train_precision\"], [\"number\", \"best_train_f1\"], [\"number\", \"best_val_recall\"], [\"number\", \"best_val_precision\"], [\"number\", \"best_val_f1\"], [\"number\", \"best_test_recall\"], [\"number\", \"best_test_precision\"], [\"number\", \"best_test_f1\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: \"0\",\n      });\n    "
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#benchmark.to_csv(PATH + '/benchmark.csv',index=False)"
      ],
      "metadata": {
        "id": "b-i_bMPSsYGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Define the hyperparameters to search over\n",
        "def experiment_2(benchmark, X_train = X_train ,y_train = y_train, X_val = X_val, y_val = y_val):\n",
        "    name = ['deep_rescnn']\n",
        "    weight_decay = [1e-2]\n",
        "    lr = [1e-3,7e-4]\n",
        "    batch_size = [128]\n",
        "    pretrained = [True,False]\n",
        "    # Create a Cartesian product of all hyperparameter combinations\n",
        "    hyperparams = list(itertools.product(name, weight_decay, lr, batch_size, pretrained))\n",
        "    print(f'grid search : perform {len(hyperparams)} searchs')\n",
        "\n",
        "    # Loop over each combination of hyperparameters and train/evaluate the model\n",
        "    best_val_f1 = 0.0\n",
        "    num_epochs = 50\n",
        "\n",
        "    train_set = MI_Dataset(X_train,y_train, truncate_to_max=False)\n",
        "    val_set = MI_Dataset(X_val,y_val, truncate_to_max=False)\n",
        "\n",
        "    history = {}\n",
        "    for i, (name, wd, lr, bs, pt) in enumerate(hyperparams):\n",
        "\n",
        "        print(f'\\n 🔎 search {i} : {name} --- lr : {lr}, weight_decay : {wd}, batch_size : {bs}, pretrain : {pt}')\n",
        "        search_id = f'{name}(pretrain={pt},weight_decay={wd},learning_rate={lr},batch_size={bs})' \n",
        "        \n",
        "        # Setup Device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Create the model, optimizer, and loss function\n",
        "        model = get_architecture(name = name, device = device, pretrained = pt)\n",
        "        pos_weight = get_weighted_for_bce(y_train)\n",
        "        criterion = nn.BCEWithLogitsLoss(weight=pos_weight.to(device))\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=wd)\n",
        "\n",
        "        # Create the data loaders\n",
        "        train_loader, val_loader, test_loader = get_loader(train_set, val_set, test_set, train_batch_size=bs)\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n",
        "        model, model_stat, train_loss_lst, train_f1_lst, val_loss_lst, val_f1_lst = train(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, get_history=True)\n",
        "        \n",
        "        history.update({search_id : {'train_loss_lst' : train_loss_lst, \n",
        "                                    'train_f1_lst' : train_f1_lst,\n",
        "                                    'val_loss_lst' : val_loss_lst,\n",
        "                                    'val_f1_lst' : val_f1_lst}})\n",
        "        \n",
        "        # save the best model configuration based on validation F1 score\n",
        "        if model_stat['best_val_f1'] > best_val_f1:\n",
        "            best_val_f1 = model_stat['best_val_f1']\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "        \n",
        "        row = {'model_name' : name,\n",
        "            'hyperparameter' : {'weight_decay' : wd, 'learning_rate' : lr,\n",
        "                                'batch_size' : bs,},\n",
        "            'best_epoch' : model_stat['best_epoch'],\n",
        "            'best_train_cost' : model_stat['best_train_cost'],\n",
        "            'best_val_cost' : model_stat['best_val_cost'],\n",
        "            'best_train_recall' : model_stat['best_train_recall'],\n",
        "            'best_train_precision' : model_stat['best_train_precision'],\n",
        "            'best_train_f1' : model_stat['best_train_f1'],\n",
        "            'best_val_recall' : model_stat['best_val_recall'],\n",
        "            'best_val_precision' : model_stat['best_val_precision'],\n",
        "            'best_val_f1' : model_stat['best_val_f1'],\n",
        "            'best_test_recall' : None,\n",
        "            'best_test_precision' : None,\n",
        "            'best_test_f1' : None,\n",
        "            }\n",
        "\n",
        "        benchmark =benchmark.append(row, ignore_index=True)\n",
        "    \n",
        "    return benchmark, best_model, history"
      ],
      "metadata": {
        "id": "AlyedXgFsmal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark, best_model, history = experiment_2(benchmark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vb7kzmQsvm3",
        "outputId": "2a331fdd-193a-4aaf-af4c-d5a0e957be77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid search : perform 4 searchs\n",
            "\n",
            " 🔎 search 0 : deep_rescnn --- lr : 0.001, weight_decay : 0.01, batch_size : 128, pretrain : True\n",
            "Epoch: 1/50, Train Loss: 0.1148, Train Acc: 0.8704, Train F1: 0.9110 Val Loss: 0.0838, Val Acc: 0.9282, Val F1: 0.9513\n",
            "Epoch: 2/50, Train Loss: 0.0710, Train Acc: 0.9305, Train F1: 0.9519 Val Loss: 0.0746, Val Acc: 0.9254, Val F1: 0.9501\n",
            "Epoch: 3/50, Train Loss: 0.0663, Train Acc: 0.9304, Train F1: 0.9520 Val Loss: 0.0701, Val Acc: 0.9364, Val F1: 0.9552\n",
            "Epoch: 4/50, Train Loss: 0.0564, Train Acc: 0.9439, Train F1: 0.9610 Val Loss: 0.0707, Val Acc: 0.9251, Val F1: 0.9463\n",
            "Epoch: 5/50, Train Loss: 0.0530, Train Acc: 0.9512, Train F1: 0.9662 Val Loss: 0.0534, Val Acc: 0.9567, Val F1: 0.9704\n",
            "Epoch: 6/50, Train Loss: 0.0467, Train Acc: 0.9598, Train F1: 0.9721 Val Loss: 0.0804, Val Acc: 0.9165, Val F1: 0.9451\n",
            "Epoch: 7/50, Train Loss: 0.0461, Train Acc: 0.9578, Train F1: 0.9705 Val Loss: 0.0475, Val Acc: 0.9612, Val F1: 0.9734\n",
            "Epoch: 8/50, Train Loss: 0.0412, Train Acc: 0.9646, Train F1: 0.9755 Val Loss: 0.0631, Val Acc: 0.9392, Val F1: 0.9593\n",
            "Epoch: 9/50, Train Loss: 0.0423, Train Acc: 0.9607, Train F1: 0.9726 Val Loss: 0.0490, Val Acc: 0.9550, Val F1: 0.9684\n",
            "Epoch: 10/50, Train Loss: 0.0401, Train Acc: 0.9639, Train F1: 0.9751 Val Loss: 0.0611, Val Acc: 0.9368, Val F1: 0.9578\n",
            "Epoch: 11/50, Train Loss: 0.0397, Train Acc: 0.9663, Train F1: 0.9766 Val Loss: 0.0442, Val Acc: 0.9656, Val F1: 0.9766\n",
            "Epoch: 12/50, Train Loss: 0.0339, Train Acc: 0.9730, Train F1: 0.9813 Val Loss: 0.0410, Val Acc: 0.9711, Val F1: 0.9800\n",
            "Epoch: 13/50, Train Loss: 0.0358, Train Acc: 0.9709, Train F1: 0.9797 Val Loss: 0.0440, Val Acc: 0.9639, Val F1: 0.9752\n",
            "Epoch: 14/50, Train Loss: 0.0337, Train Acc: 0.9737, Train F1: 0.9817 Val Loss: 0.0420, Val Acc: 0.9667, Val F1: 0.9772\n",
            "Epoch: 15/50, Train Loss: 0.0333, Train Acc: 0.9731, Train F1: 0.9813 Val Loss: 0.0429, Val Acc: 0.9643, Val F1: 0.9750\n",
            "Epoch: 16/50, Train Loss: 0.0320, Train Acc: 0.9757, Train F1: 0.9830 Val Loss: 0.0392, Val Acc: 0.9674, Val F1: 0.9773\n",
            "Epoch: 17/50, Train Loss: 0.0334, Train Acc: 0.9731, Train F1: 0.9812 Val Loss: 0.0434, Val Acc: 0.9667, Val F1: 0.9773\n",
            "Epoch: 18/50, Train Loss: 0.0326, Train Acc: 0.9724, Train F1: 0.9808 Val Loss: 0.0373, Val Acc: 0.9694, Val F1: 0.9789\n",
            "Epoch: 19/50, Train Loss: 0.0303, Train Acc: 0.9779, Train F1: 0.9846 Val Loss: 0.0361, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 20/50, Train Loss: 0.0293, Train Acc: 0.9778, Train F1: 0.9846 Val Loss: 0.0393, Val Acc: 0.9704, Val F1: 0.9797\n",
            "Epoch: 21/50, Train Loss: 0.0288, Train Acc: 0.9796, Train F1: 0.9858 Val Loss: 0.0373, Val Acc: 0.9715, Val F1: 0.9804\n",
            "Epoch: 22/50, Train Loss: 0.0280, Train Acc: 0.9813, Train F1: 0.9869 Val Loss: 0.0386, Val Acc: 0.9698, Val F1: 0.9793\n",
            "Epoch: 23/50, Train Loss: 0.0272, Train Acc: 0.9819, Train F1: 0.9875 Val Loss: 0.0385, Val Acc: 0.9663, Val F1: 0.9766\n",
            "Epoch: 24/50, Train Loss: 0.0287, Train Acc: 0.9789, Train F1: 0.9854 Val Loss: 0.0365, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 25/50, Train Loss: 0.0267, Train Acc: 0.9813, Train F1: 0.9870 Val Loss: 0.0497, Val Acc: 0.9543, Val F1: 0.9692\n",
            "Epoch: 26/50, Train Loss: 0.0284, Train Acc: 0.9788, Train F1: 0.9852 Val Loss: 0.0432, Val Acc: 0.9663, Val F1: 0.9771\n",
            "Epoch: 27/50, Train Loss: 0.0268, Train Acc: 0.9822, Train F1: 0.9877 Val Loss: 0.0345, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 28/50, Train Loss: 0.0260, Train Acc: 0.9835, Train F1: 0.9886 Val Loss: 0.0380, Val Acc: 0.9701, Val F1: 0.9792\n",
            "Epoch: 29/50, Train Loss: 0.0258, Train Acc: 0.9832, Train F1: 0.9882 Val Loss: 0.0337, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 30/50, Train Loss: 0.0263, Train Acc: 0.9805, Train F1: 0.9865 Val Loss: 0.0348, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 31/50, Train Loss: 0.0250, Train Acc: 0.9833, Train F1: 0.9884 Val Loss: 0.0355, Val Acc: 0.9718, Val F1: 0.9807\n",
            "Epoch: 32/50, Train Loss: 0.0249, Train Acc: 0.9843, Train F1: 0.9891 Val Loss: 0.0371, Val Acc: 0.9735, Val F1: 0.9816\n",
            "Epoch: 33/50, Train Loss: 0.0250, Train Acc: 0.9827, Train F1: 0.9880 Val Loss: 0.0335, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 34/50, Train Loss: 0.0240, Train Acc: 0.9850, Train F1: 0.9896 Val Loss: 0.0341, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 35/50, Train Loss: 0.0233, Train Acc: 0.9858, Train F1: 0.9902 Val Loss: 0.0342, Val Acc: 0.9749, Val F1: 0.9828\n",
            "Epoch: 36/50, Train Loss: 0.0242, Train Acc: 0.9840, Train F1: 0.9888 Val Loss: 0.0376, Val Acc: 0.9674, Val F1: 0.9777\n",
            "Epoch: 37/50, Train Loss: 0.0249, Train Acc: 0.9825, Train F1: 0.9878 Val Loss: 0.0328, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 38/50, Train Loss: 0.0238, Train Acc: 0.9841, Train F1: 0.9890 Val Loss: 0.0351, Val Acc: 0.9749, Val F1: 0.9828\n",
            "Epoch: 39/50, Train Loss: 0.0226, Train Acc: 0.9861, Train F1: 0.9903 Val Loss: 0.0333, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 40/50, Train Loss: 0.0226, Train Acc: 0.9869, Train F1: 0.9909 Val Loss: 0.0324, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 41/50, Train Loss: 0.0227, Train Acc: 0.9867, Train F1: 0.9908 Val Loss: 0.0329, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 42/50, Train Loss: 0.0227, Train Acc: 0.9864, Train F1: 0.9906 Val Loss: 0.0324, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 43/50, Train Loss: 0.0221, Train Acc: 0.9875, Train F1: 0.9914 Val Loss: 0.0324, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 44/50, Train Loss: 0.0217, Train Acc: 0.9885, Train F1: 0.9921 Val Loss: 0.0329, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 45/50, Train Loss: 0.0215, Train Acc: 0.9877, Train F1: 0.9915 Val Loss: 0.0322, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 46/50, Train Loss: 0.0213, Train Acc: 0.9882, Train F1: 0.9918 Val Loss: 0.0322, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 47/50, Train Loss: 0.0213, Train Acc: 0.9885, Train F1: 0.9920 Val Loss: 0.0322, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 48/50, Train Loss: 0.0212, Train Acc: 0.9885, Train F1: 0.9920 Val Loss: 0.0322, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 49/50, Train Loss: 0.0212, Train Acc: 0.9883, Train F1: 0.9920 Val Loss: 0.0322, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 50/50, Train Loss: 0.0212, Train Acc: 0.9885, Train F1: 0.9921 Val Loss: 0.0322, Val Acc: 0.9777, Val F1: 0.9846\n",
            "\n",
            " 🔎 search 1 : deep_rescnn --- lr : 0.001, weight_decay : 0.01, batch_size : 128, pretrain : False\n",
            "Epoch: 1/50, Train Loss: 0.2273, Train Acc: 0.7698, Train F1: 0.8421 Val Loss: 0.1454, Val Acc: 0.8258, Val F1: 0.8890\n",
            "Epoch: 2/50, Train Loss: 0.1266, Train Acc: 0.8560, Train F1: 0.9017 Val Loss: 0.1257, Val Acc: 0.8526, Val F1: 0.9053\n",
            "Epoch: 3/50, Train Loss: 0.1168, Train Acc: 0.8690, Train F1: 0.9109 Val Loss: 0.1232, Val Acc: 0.8677, Val F1: 0.9036\n",
            "Epoch: 4/50, Train Loss: 0.1097, Train Acc: 0.8819, Train F1: 0.9193 Val Loss: 0.1158, Val Acc: 0.8787, Val F1: 0.9128\n",
            "Epoch: 5/50, Train Loss: 0.1008, Train Acc: 0.8968, Train F1: 0.9291 Val Loss: 0.1139, Val Acc: 0.8636, Val F1: 0.9128\n",
            "Epoch: 6/50, Train Loss: 0.0940, Train Acc: 0.9064, Train F1: 0.9355 Val Loss: 0.1135, Val Acc: 0.8615, Val F1: 0.9119\n",
            "Epoch: 7/50, Train Loss: 0.1017, Train Acc: 0.8901, Train F1: 0.9247 Val Loss: 0.0972, Val Acc: 0.9100, Val F1: 0.9367\n",
            "Epoch: 8/50, Train Loss: 0.0871, Train Acc: 0.9144, Train F1: 0.9409 Val Loss: 0.1011, Val Acc: 0.9045, Val F1: 0.9318\n",
            "Epoch: 9/50, Train Loss: 0.0828, Train Acc: 0.9239, Train F1: 0.9474 Val Loss: 0.0884, Val Acc: 0.9196, Val F1: 0.9457\n",
            "Epoch: 10/50, Train Loss: 0.0830, Train Acc: 0.9174, Train F1: 0.9429 Val Loss: 0.0855, Val Acc: 0.9192, Val F1: 0.9456\n",
            "Epoch: 11/50, Train Loss: 0.0770, Train Acc: 0.9309, Train F1: 0.9522 Val Loss: 0.0852, Val Acc: 0.9203, Val F1: 0.9467\n",
            "Epoch: 12/50, Train Loss: 0.0764, Train Acc: 0.9284, Train F1: 0.9505 Val Loss: 0.1044, Val Acc: 0.8856, Val F1: 0.9168\n",
            "Epoch: 13/50, Train Loss: 0.0759, Train Acc: 0.9289, Train F1: 0.9506 Val Loss: 0.0952, Val Acc: 0.9021, Val F1: 0.9361\n",
            "Epoch: 14/50, Train Loss: 0.0722, Train Acc: 0.9337, Train F1: 0.9543 Val Loss: 0.0842, Val Acc: 0.9199, Val F1: 0.9467\n",
            "Epoch: 15/50, Train Loss: 0.0740, Train Acc: 0.9317, Train F1: 0.9528 Val Loss: 0.0800, Val Acc: 0.9244, Val F1: 0.9479\n",
            "Epoch: 16/50, Train Loss: 0.0701, Train Acc: 0.9378, Train F1: 0.9570 Val Loss: 0.0915, Val Acc: 0.9076, Val F1: 0.9392\n",
            "Epoch: 17/50, Train Loss: 0.0804, Train Acc: 0.9171, Train F1: 0.9430 Val Loss: 0.0798, Val Acc: 0.9227, Val F1: 0.9474\n",
            "Epoch: 18/50, Train Loss: 0.0679, Train Acc: 0.9396, Train F1: 0.9583 Val Loss: 0.0751, Val Acc: 0.9357, Val F1: 0.9560\n",
            "Epoch: 19/50, Train Loss: 0.0689, Train Acc: 0.9383, Train F1: 0.9574 Val Loss: 0.0930, Val Acc: 0.8948, Val F1: 0.9309\n",
            "Epoch: 20/50, Train Loss: 0.0696, Train Acc: 0.9354, Train F1: 0.9555 Val Loss: 0.0762, Val Acc: 0.9326, Val F1: 0.9541\n",
            "Epoch: 21/50, Train Loss: 0.0664, Train Acc: 0.9389, Train F1: 0.9581 Val Loss: 0.0749, Val Acc: 0.9344, Val F1: 0.9545\n",
            "Epoch: 22/50, Train Loss: 0.0681, Train Acc: 0.9368, Train F1: 0.9562 Val Loss: 0.0773, Val Acc: 0.9316, Val F1: 0.9540\n",
            "Epoch: 23/50, Train Loss: 0.0658, Train Acc: 0.9393, Train F1: 0.9581 Val Loss: 0.0732, Val Acc: 0.9357, Val F1: 0.9556\n",
            "Epoch: 24/50, Train Loss: 0.0624, Train Acc: 0.9455, Train F1: 0.9621 Val Loss: 0.0756, Val Acc: 0.9354, Val F1: 0.9557\n",
            "Epoch: 25/50, Train Loss: 0.0626, Train Acc: 0.9409, Train F1: 0.9592 Val Loss: 0.0709, Val Acc: 0.9395, Val F1: 0.9583\n",
            "Epoch: 26/50, Train Loss: 0.0710, Train Acc: 0.9289, Train F1: 0.9510 Val Loss: 0.0753, Val Acc: 0.9316, Val F1: 0.9524\n",
            "Epoch: 27/50, Train Loss: 0.0640, Train Acc: 0.9425, Train F1: 0.9600 Val Loss: 0.0712, Val Acc: 0.9351, Val F1: 0.9548\n",
            "Epoch: 28/50, Train Loss: 0.0608, Train Acc: 0.9468, Train F1: 0.9632 Val Loss: 0.0785, Val Acc: 0.9192, Val F1: 0.9430\n",
            "Epoch: 29/50, Train Loss: 0.0682, Train Acc: 0.9341, Train F1: 0.9544 Val Loss: 0.0711, Val Acc: 0.9347, Val F1: 0.9551\n",
            "Epoch: 30/50, Train Loss: 0.0622, Train Acc: 0.9419, Train F1: 0.9597 Val Loss: 0.0829, Val Acc: 0.9148, Val F1: 0.9436\n",
            "Epoch: 31/50, Train Loss: 0.0597, Train Acc: 0.9486, Train F1: 0.9644 Val Loss: 0.0698, Val Acc: 0.9375, Val F1: 0.9573\n",
            "Epoch: 32/50, Train Loss: 0.0575, Train Acc: 0.9499, Train F1: 0.9654 Val Loss: 0.0737, Val Acc: 0.9320, Val F1: 0.9544\n",
            "Epoch: 33/50, Train Loss: 0.0566, Train Acc: 0.9506, Train F1: 0.9659 Val Loss: 0.0694, Val Acc: 0.9385, Val F1: 0.9582\n",
            "Epoch: 34/50, Train Loss: 0.0566, Train Acc: 0.9502, Train F1: 0.9657 Val Loss: 0.0710, Val Acc: 0.9361, Val F1: 0.9564\n",
            "Epoch: 35/50, Train Loss: 0.0567, Train Acc: 0.9491, Train F1: 0.9648 Val Loss: 0.0687, Val Acc: 0.9412, Val F1: 0.9595\n",
            "Epoch: 36/50, Train Loss: 0.0576, Train Acc: 0.9482, Train F1: 0.9641 Val Loss: 0.0693, Val Acc: 0.9405, Val F1: 0.9599\n",
            "Epoch: 37/50, Train Loss: 0.0554, Train Acc: 0.9535, Train F1: 0.9679 Val Loss: 0.0683, Val Acc: 0.9426, Val F1: 0.9612\n",
            "Epoch: 38/50, Train Loss: 0.0551, Train Acc: 0.9507, Train F1: 0.9661 Val Loss: 0.0675, Val Acc: 0.9423, Val F1: 0.9608\n",
            "Epoch: 39/50, Train Loss: 0.0535, Train Acc: 0.9534, Train F1: 0.9678 Val Loss: 0.0678, Val Acc: 0.9433, Val F1: 0.9615\n",
            "Epoch: 40/50, Train Loss: 0.0541, Train Acc: 0.9528, Train F1: 0.9674 Val Loss: 0.0660, Val Acc: 0.9402, Val F1: 0.9586\n",
            "Epoch: 41/50, Train Loss: 0.0533, Train Acc: 0.9525, Train F1: 0.9672 Val Loss: 0.0658, Val Acc: 0.9440, Val F1: 0.9615\n",
            "Epoch: 42/50, Train Loss: 0.0526, Train Acc: 0.9550, Train F1: 0.9687 Val Loss: 0.0655, Val Acc: 0.9464, Val F1: 0.9635\n",
            "Epoch: 43/50, Train Loss: 0.0523, Train Acc: 0.9551, Train F1: 0.9689 Val Loss: 0.0663, Val Acc: 0.9457, Val F1: 0.9631\n",
            "Epoch: 44/50, Train Loss: 0.0517, Train Acc: 0.9562, Train F1: 0.9698 Val Loss: 0.0653, Val Acc: 0.9467, Val F1: 0.9637\n",
            "Epoch: 45/50, Train Loss: 0.0516, Train Acc: 0.9567, Train F1: 0.9701 Val Loss: 0.0645, Val Acc: 0.9447, Val F1: 0.9620\n",
            "Epoch: 46/50, Train Loss: 0.0512, Train Acc: 0.9565, Train F1: 0.9699 Val Loss: 0.0658, Val Acc: 0.9450, Val F1: 0.9627\n",
            "Epoch: 47/50, Train Loss: 0.0510, Train Acc: 0.9573, Train F1: 0.9705 Val Loss: 0.0646, Val Acc: 0.9447, Val F1: 0.9619\n",
            "Epoch: 48/50, Train Loss: 0.0508, Train Acc: 0.9566, Train F1: 0.9700 Val Loss: 0.0645, Val Acc: 0.9474, Val F1: 0.9641\n",
            "Epoch: 49/50, Train Loss: 0.0508, Train Acc: 0.9572, Train F1: 0.9704 Val Loss: 0.0645, Val Acc: 0.9474, Val F1: 0.9641\n",
            "Epoch: 50/50, Train Loss: 0.0507, Train Acc: 0.9569, Train F1: 0.9703 Val Loss: 0.0645, Val Acc: 0.9474, Val F1: 0.9641\n",
            "\n",
            " 🔎 search 2 : deep_rescnn --- lr : 0.0007, weight_decay : 0.01, batch_size : 128, pretrain : True\n",
            "Epoch: 1/50, Train Loss: 0.1581, Train Acc: 0.8026, Train F1: 0.8529 Val Loss: 0.1090, Val Acc: 0.8749, Val F1: 0.9167\n",
            "Epoch: 2/50, Train Loss: 0.0909, Train Acc: 0.9099, Train F1: 0.9377 Val Loss: 0.0815, Val Acc: 0.9237, Val F1: 0.9474\n",
            "Epoch: 3/50, Train Loss: 0.0725, Train Acc: 0.9297, Train F1: 0.9511 Val Loss: 0.0722, Val Acc: 0.9316, Val F1: 0.9521\n",
            "Epoch: 4/50, Train Loss: 0.0630, Train Acc: 0.9400, Train F1: 0.9585 Val Loss: 0.0691, Val Acc: 0.9313, Val F1: 0.9514\n",
            "Epoch: 5/50, Train Loss: 0.0582, Train Acc: 0.9443, Train F1: 0.9613 Val Loss: 0.0637, Val Acc: 0.9443, Val F1: 0.9625\n",
            "Epoch: 6/50, Train Loss: 0.0497, Train Acc: 0.9550, Train F1: 0.9687 Val Loss: 0.0533, Val Acc: 0.9570, Val F1: 0.9705\n",
            "Epoch: 7/50, Train Loss: 0.0471, Train Acc: 0.9580, Train F1: 0.9709 Val Loss: 0.0532, Val Acc: 0.9550, Val F1: 0.9684\n",
            "Epoch: 8/50, Train Loss: 0.0440, Train Acc: 0.9615, Train F1: 0.9734 Val Loss: 0.0473, Val Acc: 0.9622, Val F1: 0.9740\n",
            "Epoch: 9/50, Train Loss: 0.0387, Train Acc: 0.9699, Train F1: 0.9791 Val Loss: 0.0454, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 10/50, Train Loss: 0.0400, Train Acc: 0.9679, Train F1: 0.9777 Val Loss: 0.0684, Val Acc: 0.9299, Val F1: 0.9534\n",
            "Epoch: 11/50, Train Loss: 0.0404, Train Acc: 0.9668, Train F1: 0.9770 Val Loss: 0.0468, Val Acc: 0.9622, Val F1: 0.9743\n",
            "Epoch: 12/50, Train Loss: 0.0361, Train Acc: 0.9714, Train F1: 0.9801 Val Loss: 0.0425, Val Acc: 0.9646, Val F1: 0.9757\n",
            "Epoch: 13/50, Train Loss: 0.0382, Train Acc: 0.9654, Train F1: 0.9760 Val Loss: 0.0573, Val Acc: 0.9361, Val F1: 0.9544\n",
            "Epoch: 14/50, Train Loss: 0.0370, Train Acc: 0.9691, Train F1: 0.9784 Val Loss: 0.0504, Val Acc: 0.9567, Val F1: 0.9707\n",
            "Epoch: 15/50, Train Loss: 0.0331, Train Acc: 0.9766, Train F1: 0.9839 Val Loss: 0.0456, Val Acc: 0.9636, Val F1: 0.9745\n",
            "Epoch: 16/50, Train Loss: 0.0328, Train Acc: 0.9756, Train F1: 0.9830 Val Loss: 0.0405, Val Acc: 0.9698, Val F1: 0.9793\n",
            "Epoch: 17/50, Train Loss: 0.0325, Train Acc: 0.9761, Train F1: 0.9834 Val Loss: 0.0398, Val Acc: 0.9718, Val F1: 0.9807\n",
            "Epoch: 18/50, Train Loss: 0.0339, Train Acc: 0.9742, Train F1: 0.9821 Val Loss: 0.0393, Val Acc: 0.9708, Val F1: 0.9798\n",
            "Epoch: 19/50, Train Loss: 0.0297, Train Acc: 0.9796, Train F1: 0.9857 Val Loss: 0.0394, Val Acc: 0.9691, Val F1: 0.9785\n",
            "Epoch: 20/50, Train Loss: 0.0311, Train Acc: 0.9761, Train F1: 0.9834 Val Loss: 0.0394, Val Acc: 0.9677, Val F1: 0.9775\n",
            "Epoch: 21/50, Train Loss: 0.0335, Train Acc: 0.9730, Train F1: 0.9812 Val Loss: 0.0417, Val Acc: 0.9711, Val F1: 0.9803\n",
            "Epoch: 22/50, Train Loss: 0.0298, Train Acc: 0.9790, Train F1: 0.9854 Val Loss: 0.0411, Val Acc: 0.9691, Val F1: 0.9789\n",
            "Epoch: 23/50, Train Loss: 0.0306, Train Acc: 0.9779, Train F1: 0.9849 Val Loss: 0.0389, Val Acc: 0.9715, Val F1: 0.9802\n",
            "Epoch: 24/50, Train Loss: 0.0287, Train Acc: 0.9804, Train F1: 0.9864 Val Loss: 0.0376, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 25/50, Train Loss: 0.0273, Train Acc: 0.9821, Train F1: 0.9876 Val Loss: 0.0357, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 26/50, Train Loss: 0.0265, Train Acc: 0.9829, Train F1: 0.9882 Val Loss: 0.0388, Val Acc: 0.9722, Val F1: 0.9810\n",
            "Epoch: 27/50, Train Loss: 0.0267, Train Acc: 0.9824, Train F1: 0.9877 Val Loss: 0.0361, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 28/50, Train Loss: 0.0256, Train Acc: 0.9829, Train F1: 0.9880 Val Loss: 0.0434, Val Acc: 0.9684, Val F1: 0.9784\n",
            "Epoch: 29/50, Train Loss: 0.0261, Train Acc: 0.9836, Train F1: 0.9886 Val Loss: 0.0393, Val Acc: 0.9711, Val F1: 0.9803\n",
            "Epoch: 30/50, Train Loss: 0.0250, Train Acc: 0.9849, Train F1: 0.9895 Val Loss: 0.0343, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 31/50, Train Loss: 0.0260, Train Acc: 0.9835, Train F1: 0.9886 Val Loss: 0.0363, Val Acc: 0.9739, Val F1: 0.9821\n",
            "Epoch: 32/50, Train Loss: 0.0248, Train Acc: 0.9855, Train F1: 0.9899 Val Loss: 0.0362, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 33/50, Train Loss: 0.0240, Train Acc: 0.9855, Train F1: 0.9899 Val Loss: 0.0356, Val Acc: 0.9739, Val F1: 0.9821\n",
            "Epoch: 34/50, Train Loss: 0.0242, Train Acc: 0.9858, Train F1: 0.9901 Val Loss: 0.0353, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 35/50, Train Loss: 0.0241, Train Acc: 0.9853, Train F1: 0.9899 Val Loss: 0.0345, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 36/50, Train Loss: 0.0233, Train Acc: 0.9867, Train F1: 0.9908 Val Loss: 0.0347, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 37/50, Train Loss: 0.0233, Train Acc: 0.9867, Train F1: 0.9908 Val Loss: 0.0346, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 38/50, Train Loss: 0.0231, Train Acc: 0.9867, Train F1: 0.9908 Val Loss: 0.0338, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 39/50, Train Loss: 0.0229, Train Acc: 0.9876, Train F1: 0.9914 Val Loss: 0.0341, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 40/50, Train Loss: 0.0234, Train Acc: 0.9871, Train F1: 0.9910 Val Loss: 0.0338, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 41/50, Train Loss: 0.0231, Train Acc: 0.9866, Train F1: 0.9907 Val Loss: 0.0356, Val Acc: 0.9732, Val F1: 0.9816\n",
            "Epoch: 42/50, Train Loss: 0.0228, Train Acc: 0.9877, Train F1: 0.9915 Val Loss: 0.0345, Val Acc: 0.9739, Val F1: 0.9821\n",
            "Epoch: 43/50, Train Loss: 0.0224, Train Acc: 0.9873, Train F1: 0.9912 Val Loss: 0.0341, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 44/50, Train Loss: 0.0224, Train Acc: 0.9888, Train F1: 0.9922 Val Loss: 0.0338, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 45/50, Train Loss: 0.0223, Train Acc: 0.9884, Train F1: 0.9920 Val Loss: 0.0335, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 46/50, Train Loss: 0.0222, Train Acc: 0.9880, Train F1: 0.9916 Val Loss: 0.0341, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 47/50, Train Loss: 0.0221, Train Acc: 0.9881, Train F1: 0.9918 Val Loss: 0.0344, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 48/50, Train Loss: 0.0221, Train Acc: 0.9881, Train F1: 0.9917 Val Loss: 0.0336, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 49/50, Train Loss: 0.0220, Train Acc: 0.9887, Train F1: 0.9922 Val Loss: 0.0335, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 50/50, Train Loss: 0.0219, Train Acc: 0.9885, Train F1: 0.9921 Val Loss: 0.0335, Val Acc: 0.9770, Val F1: 0.9841\n",
            "\n",
            " 🔎 search 3 : deep_rescnn --- lr : 0.0007, weight_decay : 0.01, batch_size : 128, pretrain : False\n",
            "Epoch: 1/50, Train Loss: 0.1905, Train Acc: 0.7754, Train F1: 0.8447 Val Loss: 0.1589, Val Acc: 0.8038, Val F1: 0.8781\n",
            "Epoch: 2/50, Train Loss: 0.1402, Train Acc: 0.8411, Train F1: 0.8931 Val Loss: 0.1285, Val Acc: 0.8605, Val F1: 0.9085\n",
            "Epoch: 3/50, Train Loss: 0.1217, Train Acc: 0.8656, Train F1: 0.9083 Val Loss: 0.1165, Val Acc: 0.8763, Val F1: 0.9194\n",
            "Epoch: 4/50, Train Loss: 0.1083, Train Acc: 0.8901, Train F1: 0.9247 Val Loss: 0.1128, Val Acc: 0.8715, Val F1: 0.9176\n",
            "Epoch: 5/50, Train Loss: 0.0955, Train Acc: 0.9116, Train F1: 0.9394 Val Loss: 0.1018, Val Acc: 0.8997, Val F1: 0.9344\n",
            "Epoch: 6/50, Train Loss: 0.0953, Train Acc: 0.9016, Train F1: 0.9318 Val Loss: 0.1020, Val Acc: 0.8997, Val F1: 0.9343\n",
            "Epoch: 7/50, Train Loss: 0.0862, Train Acc: 0.9234, Train F1: 0.9474 Val Loss: 0.0924, Val Acc: 0.9148, Val F1: 0.9423\n",
            "Epoch: 8/50, Train Loss: 0.0878, Train Acc: 0.9157, Train F1: 0.9421 Val Loss: 0.0863, Val Acc: 0.9278, Val F1: 0.9516\n",
            "Epoch: 9/50, Train Loss: 0.0849, Train Acc: 0.9207, Train F1: 0.9453 Val Loss: 0.0938, Val Acc: 0.9096, Val F1: 0.9398\n",
            "Epoch: 10/50, Train Loss: 0.0805, Train Acc: 0.9281, Train F1: 0.9506 Val Loss: 0.0850, Val Acc: 0.9289, Val F1: 0.9511\n",
            "Epoch: 11/50, Train Loss: 0.0752, Train Acc: 0.9320, Train F1: 0.9532 Val Loss: 0.0803, Val Acc: 0.9278, Val F1: 0.9503\n",
            "Epoch: 12/50, Train Loss: 0.0738, Train Acc: 0.9336, Train F1: 0.9541 Val Loss: 0.0771, Val Acc: 0.9364, Val F1: 0.9568\n",
            "Epoch: 13/50, Train Loss: 0.0696, Train Acc: 0.9405, Train F1: 0.9589 Val Loss: 0.0766, Val Acc: 0.9296, Val F1: 0.9511\n",
            "Epoch: 14/50, Train Loss: 0.0716, Train Acc: 0.9360, Train F1: 0.9558 Val Loss: 0.0843, Val Acc: 0.9089, Val F1: 0.9359\n",
            "Epoch: 15/50, Train Loss: 0.0684, Train Acc: 0.9409, Train F1: 0.9591 Val Loss: 0.0723, Val Acc: 0.9405, Val F1: 0.9592\n",
            "Epoch: 16/50, Train Loss: 0.0694, Train Acc: 0.9357, Train F1: 0.9554 Val Loss: 0.0752, Val Acc: 0.9333, Val F1: 0.9553\n",
            "Epoch: 17/50, Train Loss: 0.0638, Train Acc: 0.9441, Train F1: 0.9611 Val Loss: 0.0741, Val Acc: 0.9296, Val F1: 0.9509\n",
            "Epoch: 18/50, Train Loss: 0.0717, Train Acc: 0.9333, Train F1: 0.9541 Val Loss: 0.1084, Val Acc: 0.8718, Val F1: 0.9044\n",
            "Epoch: 19/50, Train Loss: 0.0683, Train Acc: 0.9391, Train F1: 0.9580 Val Loss: 0.0799, Val Acc: 0.9210, Val F1: 0.9441\n",
            "Epoch: 20/50, Train Loss: 0.0709, Train Acc: 0.9341, Train F1: 0.9546 Val Loss: 0.0808, Val Acc: 0.9227, Val F1: 0.9489\n",
            "Epoch: 21/50, Train Loss: 0.0625, Train Acc: 0.9463, Train F1: 0.9628 Val Loss: 0.0675, Val Acc: 0.9419, Val F1: 0.9600\n",
            "Epoch: 22/50, Train Loss: 0.0614, Train Acc: 0.9487, Train F1: 0.9644 Val Loss: 0.0710, Val Acc: 0.9364, Val F1: 0.9572\n",
            "Epoch: 23/50, Train Loss: 0.0571, Train Acc: 0.9521, Train F1: 0.9668 Val Loss: 0.0686, Val Acc: 0.9392, Val F1: 0.9586\n",
            "Epoch: 24/50, Train Loss: 0.0604, Train Acc: 0.9478, Train F1: 0.9637 Val Loss: 0.0824, Val Acc: 0.9186, Val F1: 0.9462\n",
            "Epoch: 25/50, Train Loss: 0.0573, Train Acc: 0.9505, Train F1: 0.9659 Val Loss: 0.0643, Val Acc: 0.9392, Val F1: 0.9579\n",
            "Epoch: 26/50, Train Loss: 0.0539, Train Acc: 0.9544, Train F1: 0.9685 Val Loss: 0.0658, Val Acc: 0.9450, Val F1: 0.9627\n",
            "Epoch: 27/50, Train Loss: 0.0536, Train Acc: 0.9564, Train F1: 0.9700 Val Loss: 0.0694, Val Acc: 0.9412, Val F1: 0.9607\n",
            "Epoch: 28/50, Train Loss: 0.0536, Train Acc: 0.9551, Train F1: 0.9690 Val Loss: 0.0664, Val Acc: 0.9416, Val F1: 0.9607\n",
            "Epoch: 29/50, Train Loss: 0.0516, Train Acc: 0.9578, Train F1: 0.9708 Val Loss: 0.0598, Val Acc: 0.9454, Val F1: 0.9623\n",
            "Epoch: 30/50, Train Loss: 0.0513, Train Acc: 0.9546, Train F1: 0.9686 Val Loss: 0.0611, Val Acc: 0.9509, Val F1: 0.9667\n",
            "Epoch: 31/50, Train Loss: 0.0482, Train Acc: 0.9608, Train F1: 0.9730 Val Loss: 0.0584, Val Acc: 0.9491, Val F1: 0.9652\n",
            "Epoch: 32/50, Train Loss: 0.0500, Train Acc: 0.9590, Train F1: 0.9716 Val Loss: 0.0610, Val Acc: 0.9485, Val F1: 0.9651\n",
            "Epoch: 33/50, Train Loss: 0.0481, Train Acc: 0.9595, Train F1: 0.9721 Val Loss: 0.0582, Val Acc: 0.9509, Val F1: 0.9666\n",
            "Epoch: 34/50, Train Loss: 0.0464, Train Acc: 0.9625, Train F1: 0.9740 Val Loss: 0.0573, Val Acc: 0.9498, Val F1: 0.9659\n",
            "Epoch: 35/50, Train Loss: 0.0464, Train Acc: 0.9638, Train F1: 0.9748 Val Loss: 0.0602, Val Acc: 0.9509, Val F1: 0.9667\n",
            "Epoch: 36/50, Train Loss: 0.0455, Train Acc: 0.9628, Train F1: 0.9743 Val Loss: 0.0563, Val Acc: 0.9519, Val F1: 0.9673\n",
            "Epoch: 37/50, Train Loss: 0.0449, Train Acc: 0.9652, Train F1: 0.9760 Val Loss: 0.0557, Val Acc: 0.9509, Val F1: 0.9663\n",
            "Epoch: 38/50, Train Loss: 0.0441, Train Acc: 0.9638, Train F1: 0.9748 Val Loss: 0.0558, Val Acc: 0.9536, Val F1: 0.9680\n",
            "Epoch: 39/50, Train Loss: 0.0433, Train Acc: 0.9669, Train F1: 0.9770 Val Loss: 0.0559, Val Acc: 0.9505, Val F1: 0.9657\n",
            "Epoch: 40/50, Train Loss: 0.0431, Train Acc: 0.9658, Train F1: 0.9763 Val Loss: 0.0554, Val Acc: 0.9533, Val F1: 0.9677\n",
            "Epoch: 41/50, Train Loss: 0.0427, Train Acc: 0.9666, Train F1: 0.9769 Val Loss: 0.0546, Val Acc: 0.9540, Val F1: 0.9683\n",
            "Epoch: 42/50, Train Loss: 0.0424, Train Acc: 0.9699, Train F1: 0.9791 Val Loss: 0.0547, Val Acc: 0.9522, Val F1: 0.9670\n",
            "Epoch: 43/50, Train Loss: 0.0421, Train Acc: 0.9677, Train F1: 0.9776 Val Loss: 0.0552, Val Acc: 0.9540, Val F1: 0.9687\n",
            "Epoch: 44/50, Train Loss: 0.0419, Train Acc: 0.9682, Train F1: 0.9780 Val Loss: 0.0539, Val Acc: 0.9533, Val F1: 0.9679\n",
            "Epoch: 45/50, Train Loss: 0.0420, Train Acc: 0.9685, Train F1: 0.9781 Val Loss: 0.0538, Val Acc: 0.9533, Val F1: 0.9678\n",
            "Epoch: 46/50, Train Loss: 0.0415, Train Acc: 0.9699, Train F1: 0.9792 Val Loss: 0.0540, Val Acc: 0.9536, Val F1: 0.9683\n",
            "Epoch: 47/50, Train Loss: 0.0412, Train Acc: 0.9694, Train F1: 0.9788 Val Loss: 0.0538, Val Acc: 0.9546, Val F1: 0.9689\n",
            "Epoch: 48/50, Train Loss: 0.0411, Train Acc: 0.9695, Train F1: 0.9789 Val Loss: 0.0540, Val Acc: 0.9550, Val F1: 0.9692\n",
            "Epoch: 49/50, Train Loss: 0.0410, Train Acc: 0.9695, Train F1: 0.9789 Val Loss: 0.0538, Val Acc: 0.9543, Val F1: 0.9687\n",
            "Epoch: 50/50, Train Loss: 0.0410, Train Acc: 0.9700, Train F1: 0.9793 Val Loss: 0.0538, Val Acc: 0.9543, Val F1: 0.9687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Report the learning of the model for all\n",
        "\n",
        "plot_history(history, 'experiment 2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "rsiOubHBuCRY",
        "outputId": "3fe12fff-2cb2-48b8-9273-a0b0968c9d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"b36a7896-85bc-48ac-9028-a56bf39b9733\" class=\"plotly-graph-div\" style=\"height:800px; width:1600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b36a7896-85bc-48ac-9028-a56bf39b9733\")) {                    Plotly.newPlot(                        \"b36a7896-85bc-48ac-9028-a56bf39b9733\",                        [{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"line\":{\"color\":\"rgb(251, 153, 118)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.11479073936484524,0.07099823134546973,0.06629408426232901,0.056431560551604726,0.053022553950957826,0.046670910692160346,0.046146746913455466,0.04116791216709906,0.04233532886512637,0.0400689132963942,0.039693621877593684,0.03393580937760222,0.03582752535134588,0.033740191695462794,0.033274124882355725,0.03200027496220047,0.03342206476924073,0.03263560922865469,0.030328480494800712,0.02932372311213377,0.028775793333182628,0.02804062478756372,0.027174410225241044,0.02872989700097945,0.026679061736830458,0.028424494334614973,0.026832876636093014,0.026001453363386762,0.02584394561734908,0.026300273369037612,0.0250347363085135,0.02493439571076379,0.024996787086470566,0.02395503335085112,0.02332715498601414,0.02418866018420096,0.024926153189776826,0.02380955293945294,0.02263440652094346,0.022594121824800625,0.022677281847968273,0.022709442704557554,0.022060503556317345,0.021650134821988874,0.021498582872294343,0.02129521929367711,0.021310148025006227,0.021240186800738584,0.02118785976702166,0.021153436610123137],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"line\":{\"color\":\"rgb(251, 153, 118)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.910952024475153,0.9518803743562049,0.9520401753418755,0.9610061255938683,0.9662051886561964,0.9720601815554925,0.9705481393812089,0.9754978297511937,0.9725812349875196,0.9750890580907431,0.9765663313612982,0.9812503410186276,0.9797412790112743,0.9816884576617156,0.9813003010575684,0.9830421458772619,0.9811916187095756,0.9808277408639494,0.984616990786054,0.984590495153802,0.9857935293127165,0.9868660560310318,0.9875101808245879,0.9853544928344584,0.987031722635576,0.9851743429257125,0.9877369289901972,0.9885536121716169,0.9881816453430313,0.9865208861062392,0.9884064130969473,0.9891473368128256,0.9879866339063875,0.9895590770984937,0.9901542446759568,0.9888173843256416,0.9877930957420341,0.9889803668757879,0.990325706935639,0.9908897871389477,0.9908046641696361,0.9906143922985136,0.991358224955736,0.9921012988030995,0.9914725840377512,0.9917706753399942,0.9920294062186465,0.9920383583121205,0.9919678018858787,0.9920945520441782],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"line\":{\"color\":\"rgb(251, 153, 118)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.08376294679146043,0.0746011592114914,0.07005765583506975,0.07065195712129685,0.053360637909767965,0.08042496346014062,0.04745715665038918,0.06305787769687135,0.049033847973518764,0.06114746781251684,0.04422392922732019,0.04102372315638663,0.04397075098148736,0.04201242862800552,0.042874127997667925,0.039235832593387754,0.04344431608147228,0.03728169832391427,0.03607806785102562,0.03928240272392522,0.037326544633333625,0.03859318069012714,0.03848588394442785,0.03646117073717396,0.049684927494767606,0.04320836971510727,0.03451017467785127,0.037967551891336736,0.03372160071359877,0.03484896673625687,0.035467641580453034,0.03708352784264538,0.03354122313539597,0.03412221068471568,0.03423815338881974,0.037596832907076966,0.03281311806790607,0.03514829652569548,0.033299182427093336,0.0324072357766407,0.032856955048964194,0.03238070181145291,0.032367073780380166,0.03287307094155308,0.03220016412769806,0.03221157305838726,0.03223858193535985,0.03215159721339691,0.03222902293006579,0.03217829100506003],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"line\":{\"color\":\"rgb(251, 153, 118)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.9512953849269433,0.950095082036006,0.9551563379350854,0.9463278214865262,0.9704117800421145,0.945129881561639,0.9733607541020326,0.9593043637912758,0.9683961661588766,0.9578491941006823,0.9765587056555526,0.9799725021565232,0.975196125752806,0.9772061110156667,0.9750323186287563,0.9772689562979048,0.97727942749973,0.9789036729891566,0.9822179605970733,0.9797426534310231,0.9803735184588285,0.9793149140037781,0.9765622905753949,0.9817819089156387,0.9692020886741394,0.9770781776818918,0.9821838748176547,0.9791757218871349,0.9828756850609568,0.983205501064155,0.9806872203255667,0.9815574968768703,0.9832208740559658,0.9833416996216238,0.9827685005432154,0.9777344021660723,0.9838285426958702,0.9827818245817501,0.9838862688996499,0.9848132951156789,0.9837994082847905,0.9843148359327287,0.9845833213635702,0.9825256209951757,0.9846023891362465,0.9845786144685787,0.9841079872371234,0.9848009910371187,0.9838779095065958,0.9845786144685787],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"line\":{\"color\":\"rgb(106, 132, 152)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.22727110171167847,0.12660686206093905,0.11678726349158249,0.10972797601971293,0.10083220048731818,0.09396824406935723,0.10171635875826165,0.0870730186041278,0.08278712951167479,0.08298451594948632,0.07701371262897604,0.07641744030232009,0.07587873040947035,0.07216597063742143,0.07395763240865552,0.07012404490172658,0.08040010022406999,0.06787435651030327,0.06891166369290969,0.06960750890454066,0.06639406276527549,0.06813373221564539,0.06583180592180937,0.062417038659835056,0.06263088442615627,0.07095848496358965,0.064028981870802,0.0607792402017021,0.0681880103797574,0.06219671320379258,0.05968116361472888,0.05749126360725018,0.056599414806475894,0.056574739588324284,0.05671553335409667,0.05758360155942377,0.05541216994871525,0.05510411755121041,0.0535294513543481,0.05407430614768298,0.053271163399208586,0.052567841380595615,0.052329855979786156,0.05168080068313925,0.05163646851754817,0.05121559288356995,0.05100947558828645,0.0508416385021816,0.05075645153277382,0.05070879739311279],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"line\":{\"color\":\"rgb(106, 132, 152)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.8421091617586326,0.9016716663955942,0.9109310417030846,0.9193469558937333,0.9290691173198439,0.9355090696786501,0.9247417786794881,0.9408814161202629,0.9473915985700303,0.9428759303591774,0.9522240977239118,0.9505415911102039,0.9506110412398411,0.9542545770655901,0.95278302946312,0.9569823578858354,0.9430494111365177,0.9583253396897095,0.957429911040089,0.9555439205077046,0.9580514740997327,0.9561764245803276,0.9581272509535016,0.9621377477146522,0.9592159510906457,0.9509748228502906,0.9599587362014276,0.9632108997379549,0.954396041085807,0.9596958496572815,0.9643923694408839,0.965422542868182,0.9658659389840827,0.9656585627398283,0.9648420147214231,0.9641278838322834,0.9678550321851283,0.9660569320548268,0.9677933125737385,0.9673661511266718,0.96723927932227,0.968706393947833,0.9688708512861149,0.9697981465931762,0.9700803642680074,0.9698613547645131,0.9705489148002516,0.969978386968445,0.9704357045878537,0.9703078540252162],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"line\":{\"color\":\"rgb(106, 132, 152)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.14542684325647517,0.12572940918792974,0.12321135251177955,0.11583187889397349,0.11389532042123197,0.11348314034877365,0.09718865028668924,0.10106103102263717,0.08840420316789568,0.08546124417654836,0.08518993287664099,0.10440628351931719,0.09522415690917739,0.08415045463752091,0.07999979108572006,0.09152488233809619,0.07981668950356159,0.07513222642678166,0.0930085916648206,0.0762188037748599,0.07492207411126173,0.07733866713710667,0.07320444539342959,0.0756211367874211,0.07085166492105759,0.07530561996387043,0.07123947645380735,0.07851682691434814,0.07109045326914575,0.08287205399106867,0.06983694373742003,0.07374319091592868,0.06941233926110252,0.07100308938944053,0.06866704897372583,0.06928715779097219,0.06830069613210933,0.0674643118356921,0.06781771834363642,0.06595423967670329,0.06581759877323695,0.06545299561470234,0.06634849213140527,0.06526806052915009,0.06452077946199994,0.06576505578875132,0.06460499040449608,0.0645379028369471,0.06451809777017312,0.06446795356027858],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"line\":{\"color\":\"rgb(106, 132, 152)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.001,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.888967949768214,0.9053150739307416,0.9035973039682629,0.9127602479896657,0.91282969762129,0.9118548007951188,0.9366706022811417,0.9317571833407438,0.9457274976310365,0.9456359280442048,0.9466738818693119,0.9167583122947509,0.9361434139842216,0.9466511106909979,0.9479103884658645,0.9392042251713852,0.9474002156450877,0.955978682994072,0.9308916869029101,0.9540644146833471,0.9544544615575029,0.9539975549628918,0.9555945206808162,0.9557338850874462,0.9583379623253226,0.9524324230662711,0.9548353077957913,0.9429584131113882,0.9551465266521872,0.943566152150972,0.9572726489523878,0.9544416479900563,0.9581798804281138,0.9564495755126411,0.959482135122019,0.9598684571561918,0.9611877161078016,0.9608421289970581,0.9614714847413534,0.9585628602791005,0.9615077578380984,0.9635279993201222,0.9630930874515586,0.9637377536848925,0.9619508339732872,0.9626910794611309,0.9619499801234491,0.9641136834435939,0.9641136834435939,0.9641136834435939],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"line\":{\"color\":\"rgb(140, 2, 123)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.15809356187729492,0.09093035884875748,0.07253834651473735,0.06303611213057585,0.058248585378079074,0.049701814612060066,0.04711636168133352,0.0440105266792258,0.03870915894280321,0.04004981606550097,0.040411376359126824,0.03612654412683752,0.03822029610372491,0.03704087864244858,0.03314139404913404,0.032765841588616916,0.03245194840873298,0.03394098432450838,0.029654299967494482,0.03114885735621021,0.03353770902595842,0.029814858519966115,0.030645098771904206,0.02868877684874599,0.027259007342936382,0.026549952240812275,0.026669400367983976,0.025613509184699016,0.026063436121260587,0.024989801668321963,0.02596839161254764,0.024794137097237993,0.02402931585105697,0.024171848522599484,0.02406656999708519,0.023316737836630075,0.02329730008646658,0.023099465963813685,0.02285592317077644,0.02343625199211866,0.023129676098915038,0.02281746172835839,0.02238933256371768,0.022382936104790724,0.02230870806081871,0.022192522929074567,0.0220670649638871,0.02212916304787273,0.021980982861841122,0.021949142734397454],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"line\":{\"color\":\"rgb(140, 2, 123)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.852938373588426,0.9376727857381629,0.9511012631275749,0.9584639197817965,0.961344665937561,0.9686764866673196,0.970911182898691,0.9733829311108451,0.9790824168704698,0.9776897956755005,0.9769589835200859,0.980129317499948,0.9759638593721264,0.9784490563477223,0.9838976658065428,0.9830423927387091,0.9833750945550235,0.9820958943633387,0.9857359572424891,0.9834475603983522,0.9811731238439108,0.9854140591381397,0.9848614010814153,0.9863868127343666,0.9875794348000755,0.988242233121916,0.9877497066545967,0.9880174040877023,0.9885975838666766,0.9894703993840019,0.9886436499120187,0.9898852246918938,0.9898887094783801,0.9901133623830178,0.989882871812077,0.9907845636159587,0.9908440644749482,0.9908053632782495,0.9914314592607222,0.991033007737149,0.9907121413123342,0.9914613284265582,0.9912114457982697,0.9922173419821164,0.9919500341787669,0.9915934140209329,0.9917713965979581,0.9917026835865552,0.9921956959573223,0.9920965919779912],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"line\":{\"color\":\"rgb(140, 2, 123)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.108959445572391,0.08148404743868051,0.0722181813925812,0.06912089071118135,0.0636874903262276,0.053343806054788764,0.05320788956538509,0.047258151258594804,0.04543509830263062,0.06838757940584032,0.04684975965274978,0.04247876433111548,0.057275122398977835,0.05043959831402883,0.045579678053196356,0.04047212368741478,0.03984981344123067,0.0393236100776089,0.039449846473141634,0.039432843277851744,0.04174167047866022,0.04108308917007496,0.038937230618139314,0.037596804610232716,0.03574722064626995,0.03876731177580725,0.0361374686366504,0.0433839630553198,0.03934631431891336,0.03430940693698798,0.03632892973695424,0.03624700669621684,0.03562924094318934,0.03526331173236837,0.03449029073989678,0.03474761628100962,0.03456807602004906,0.033757328444330145,0.03410447486692278,0.033765379019414433,0.035634716651386414,0.03448492214851773,0.03407466074394196,0.03384561046087455,0.03348237016715135,0.034108965390736296,0.034423037308597894,0.03356540910259555,0.03352583894358877,0.033539529626107296],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"line\":{\"color\":\"rgb(140, 2, 123)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.9166875442957533,0.9473694540170713,0.9520812365809699,0.9513918888807766,0.9624646004410948,0.9704895334583761,0.9684122647676282,0.973988205523899,0.9785110422625,0.9534482998821898,0.9742563348310144,0.9757052681175616,0.954354231357207,0.970745740149225,0.9745083401592266,0.9792898584273019,0.9806790695582215,0.9797656433399828,0.9785235510898257,0.9775361037297454,0.980295883255886,0.9788871793826971,0.9801798745864805,0.9824564409861851,0.9831594916571986,0.9809743691426221,0.9817100521368191,0.9784429545715649,0.9802752748348106,0.9817365935665422,0.9820753789681886,0.9829973590473531,0.9820570710750723,0.9838356282670067,0.9822162685606657,0.9836584404317498,0.9823072021263772,0.9831336812622957,0.9828928123244826,0.9843540435633988,0.9816354942485553,0.9820590699198505,0.9832134235872577,0.9835941983950124,0.9831336812622957,0.9825231507475674,0.9823157475446082,0.9840949887892039,0.9838579229631028,0.9840951525486404],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"line\":{\"color\":\"rgb(195, 37, 218)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.19052282745895516,0.14024449034176878,0.12171606952313842,0.10833098842140462,0.09548897279974794,0.0953207294726563,0.08622048715340723,0.08782126589636623,0.08489753609397294,0.08049098200336491,0.07523022451931942,0.07378501919570794,0.06963483190328258,0.07161465889399814,0.06836604146886391,0.0694199445884662,0.06380206661945356,0.07166911125831724,0.06832235272704941,0.070900912917425,0.06253659644751956,0.061372010213539496,0.05708712212920598,0.060400874512046066,0.05730206977153562,0.05390445994296435,0.05359917552917683,0.05355642866364596,0.051569330195585884,0.05128803810900566,0.04819964668868197,0.049965659233678524,0.04814329773085224,0.04639671189200564,0.046351689334109616,0.045469031364306,0.04487100650320621,0.044124117373293616,0.043267441333569204,0.04307313617852272,0.04270540346889698,0.04237812484602816,0.04210826358741911,0.04192435440414277,0.04200769073424812,0.04153586130200929,0.041185674484379646,0.041094861244452094,0.04103146021436785,0.04096369389849976],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"line\":{\"color\":\"rgb(195, 37, 218)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.8446593050863472,0.8930924269124201,0.9083002581107935,0.9247498042535272,0.939400932017152,0.9318412644276902,0.9473610796063822,0.9421328693262524,0.9452823974825951,0.9506042075700218,0.9532000039127098,0.9541316915867278,0.9589137158417058,0.955757550096565,0.9591101038851692,0.955387923775183,0.9611006911834712,0.9540586191531882,0.958038711663877,0.95459914547021,0.9627975758504062,0.9644487737465937,0.9668366615203856,0.9637433627258142,0.9658971394649086,0.9685185716455494,0.9699565308009503,0.9689971811401789,0.9708200128128436,0.9685840002943915,0.9729718516377934,0.9716130041762804,0.9721264111258112,0.9739637925967876,0.9747758502074333,0.9743332061605653,0.9759772583141347,0.9747997539645643,0.977019608049758,0.9763418874492242,0.9768501336119897,0.979054947614672,0.9776405902517856,0.9780008750299639,0.9781071697540512,0.9791940983833567,0.97879676318706,0.9789105681946668,0.9788805799642117,0.9792773290024218],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"line\":{\"color\":\"rgb(195, 37, 218)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.15894382946270028,0.12854284014283995,0.11648755593295769,0.11277182887202686,0.10177472360764991,0.10202671154258178,0.09241586592291638,0.08631049246108,0.09383622415082152,0.08504421455958455,0.08025227373920356,0.0770664964908177,0.07655069411722655,0.0843382067901572,0.07225582703077506,0.07518812115864246,0.07405701940207138,0.10843296630685682,0.07992369055338332,0.08077832652940783,0.06753340271842438,0.07104197656063689,0.06860813430904113,0.08235410608581661,0.06427021623169843,0.06577779036207297,0.06941140033432708,0.06642246037926461,0.059824305214627914,0.061128708817500016,0.05839515982317351,0.060998662036309124,0.05823603494163231,0.057349403220456084,0.060176362506917255,0.056288829829889475,0.05573135942374308,0.055831081246080265,0.05594147772774664,0.05539773231263423,0.054608300059419315,0.05468421774273066,0.0552465683943832,0.05385818851414005,0.05381364393019185,0.05397211876521815,0.053817855566740035,0.05397045065447227,0.05383598997523285,0.05379048669358709],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"line\":{\"color\":\"rgb(195, 37, 218)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=False,weight_decay=0.01,learning_rate=0.0007,batch_size=128)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.8780557647807657,0.9085373567155052,0.9193796675009884,0.9175804965791818,0.9344402424343127,0.934272157052635,0.9423304973505138,0.9515608159316332,0.9398021365975211,0.9511013197690643,0.9503235174337189,0.9567715487720881,0.9510503566508014,0.9359263395889232,0.9591890683918047,0.955327530773098,0.9509343974016239,0.9043560649194464,0.9440535758614325,0.9488704847762285,0.9599638713032688,0.9571763202856466,0.958617217745884,0.9461696694686385,0.9578758496275024,0.9626641970103161,0.9606931697944596,0.9606807191777489,0.9623216877326329,0.9666679313100309,0.965203671274888,0.9650874947625121,0.9665629101953428,0.9658706171291329,0.9667064314374568,0.9672588979521586,0.9663153245455174,0.9680159402824212,0.9657401665559513,0.9677368930644253,0.9682621627135283,0.9670435295170159,0.9686662817398707,0.9678507571852737,0.967829260086652,0.9682529856371761,0.9688736343840026,0.9691670516937745,0.9686848006752268,0.9686723489295175],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0],\"title\":{\"text\":\"Train Loss\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.625,1.0],\"title\":{\"text\":\"Train F1\"}},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Epoch\"}},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375],\"title\":{\"text\":\"Val Loss\"}},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,0.375],\"title\":{\"text\":\"Val F1\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Train Loss\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Train F1\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Val Loss\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Val F1\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Metrics for Different Models for experiment 2\"},\"height\":800,\"width\":1600,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b36a7896-85bc-48ac-9028-a56bf39b9733');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is evident that transferring the feature extraction layer from the DeepResCNN model with pre-trained parameters from the Arithmetic task, which has a relatively larger number of observations, can significantly improve the model's convergence to a lower loss function.\n",
        "\n",
        "Therefore, in the next experiment, we will continue to leverage transfer learning by utilizing pre-trained parameters from the Arithmetic task to fine-tune the feature extraction layer of the DeepResCNN model. This will allow us to take advantage of the larger dataset from the Arithmetic task and improve the model's performance on the current task."
      ],
      "metadata": {
        "id": "oj3HaMOYyWrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3630
        },
        "id": "KcMNoJ7h49Zb",
        "outputId": "7d94f154-f21e-4754-f3d2-aa8c77fb4711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       model_name                                     hyperparameter  \\\n",
              "0   zero_baseline                                            seed=42   \n",
              "1             gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "2             gru  {'weight_decay': 0.01, 'learning_rate': 0.0005...   \n",
              "3             gru  {'weight_decay': 0.01, 'learning_rate': 0.0001...   \n",
              "4             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "5             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "6             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "7             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "8             gru  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "9             gru  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "10            gru  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "11            gru  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "12            gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "13            gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "14            gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "15            gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "16            gru  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "17            gru  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "18            gru  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "19            gru  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "20            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "21            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "22            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "23            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "24            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "25            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "26            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "27            gru  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "28           lstm  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "29           lstm  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "30           lstm  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "31           lstm  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "32           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "33           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "34           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "35           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0009...   \n",
              "36           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "37           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "38           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "39           lstm  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "40           lstm  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "41           lstm  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "42           lstm  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "43           lstm  {'weight_decay': 0.001, 'learning_rate': 0.001...   \n",
              "44           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "45           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "46           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "47           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "48           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "49           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "50           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "51           lstm  {'weight_decay': 0.001, 'learning_rate': 0.000...   \n",
              "52    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "53    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "54    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "55    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "\n",
              "    best_epoch best_train_cost best_val_cost  best_train_recall  \\\n",
              "0            1            None          None           1.000000   \n",
              "1            1        0.237551      0.228324           1.000000   \n",
              "2            1        0.247199      0.235428           1.000000   \n",
              "3            2        0.268154      0.262874           0.384359   \n",
              "4            1        0.237496        0.2285           1.000000   \n",
              "5            1        0.237695      0.228308           0.926690   \n",
              "6            1        0.232703      0.227591           0.985338   \n",
              "7            1        0.230862       0.22799           0.985338   \n",
              "8            1        0.245969      0.228488           0.824055   \n",
              "9            1        0.238722      0.229009           0.868041   \n",
              "10           1        0.231409      0.227876           1.000000   \n",
              "11           1        0.230496      0.228034           1.000000   \n",
              "12           1        0.258267      0.231803           0.677434   \n",
              "13           1        0.230636      0.227791           1.000000   \n",
              "14           1        0.240435      0.227988           0.912027   \n",
              "15           1        0.229898      0.228008           1.000000   \n",
              "16           1        0.231874      0.227825           1.000000   \n",
              "17           1        0.235135      0.227605           0.912027   \n",
              "18           1        0.236581      0.228055           0.926690   \n",
              "19           1        0.230123      0.228134           1.000000   \n",
              "20           1        0.247814      0.227805           0.780228   \n",
              "21           1        0.234476       0.22773           0.926690   \n",
              "22           1        0.232707      0.227596           1.000000   \n",
              "23           1        0.231131      0.227829           0.970676   \n",
              "24           1        0.242391      0.228181           0.868041   \n",
              "25           1        0.251862      0.227611           0.736082   \n",
              "26           1        0.232329      0.227781           1.000000   \n",
              "27           1         0.23092      0.227611           0.985338   \n",
              "28           1        0.246597      0.230152           0.926690   \n",
              "29           1        0.248086       0.22897           0.809393   \n",
              "30           1        0.240369       0.22881           0.941352   \n",
              "31           1        0.233591      0.228861           1.000000   \n",
              "32           1        0.244041      0.229647           0.985338   \n",
              "33           1        0.246053      0.228722           0.824055   \n",
              "34           1        0.233413      0.229419           1.000000   \n",
              "35           1        0.239309      0.227842           0.897365   \n",
              "36           1        0.248298      0.230968           0.912027   \n",
              "37           1        0.238547      0.228958           1.000000   \n",
              "38           1         0.24648      0.229094           0.897365   \n",
              "39           1        0.241609      0.230321           0.956014   \n",
              "40           1        0.237574      0.227922           0.970676   \n",
              "41           1        0.231801      0.228474           1.000000   \n",
              "42           1        0.237987      0.227953           0.926690   \n",
              "43           1           0.233      0.227601           0.985338   \n",
              "44           1        0.233949      0.227641           1.000000   \n",
              "45           1        0.243257      0.227643           0.824055   \n",
              "46           1        0.234485      0.227701           0.970676   \n",
              "47           1        0.233095      0.227593           0.970676   \n",
              "48           1        0.233624      0.227623           1.000000   \n",
              "49           1        0.235389      0.227735           1.000000   \n",
              "50           1        0.236105      0.227592           1.000000   \n",
              "51           1         0.23537      0.227736           0.941352   \n",
              "52          40        0.022594      0.032407           0.993049   \n",
              "53          48        0.050842      0.064538           0.977197   \n",
              "54          40        0.023436      0.033765           0.992715   \n",
              "55          48        0.041095       0.05397           0.984468   \n",
              "\n",
              "    best_train_precision  best_train_f1  best_val_recall  best_val_precision  \\\n",
              "0               0.721993       0.838555         1.000000            0.721993   \n",
              "1               0.721993       0.837994         1.000000            0.721993   \n",
              "2               0.721993       0.837963         1.000000            0.721993   \n",
              "3               0.293013       0.322972         1.000000            0.721993   \n",
              "4               0.721993       0.837926         1.000000            0.721993   \n",
              "5               0.670561       0.777513         1.000000            0.721993   \n",
              "6               0.712027       0.826183         1.000000            0.721993   \n",
              "7               0.711340       0.825672         1.000000            0.721993   \n",
              "8               0.595074       0.690655         1.000000            0.721993   \n",
              "9               0.628293       0.728386         1.000000            0.721993   \n",
              "10              0.721993       0.837847         1.000000            0.721993   \n",
              "11              0.721993       0.837790         1.000000            0.721993   \n",
              "12              0.487973       0.566772         1.000000            0.721993   \n",
              "13              0.721993       0.837965         1.000000            0.721993   \n",
              "14              0.657732       0.763680         1.000000            0.721993   \n",
              "15              0.721993       0.837944         1.000000            0.721993   \n",
              "16              0.721993       0.837898         1.000000            0.721993   \n",
              "17              0.656930       0.763369         1.000000            0.721993   \n",
              "18              0.667583       0.775602         1.000000            0.721993   \n",
              "19              0.721993       0.838059         1.000000            0.721993   \n",
              "20              0.570676       0.654196         1.000000            0.721993   \n",
              "21              0.669645       0.776967         1.000000            0.721993   \n",
              "22              0.721993       0.837959         1.000000            0.721993   \n",
              "23              0.701031       0.813465         1.000000            0.721993   \n",
              "24              0.626231       0.726825         1.000000            0.721993   \n",
              "25              0.531271       0.616538         1.000000            0.721993   \n",
              "26              0.721993       0.838102         1.000000            0.721993   \n",
              "27              0.711340       0.825631         1.000000            0.721993   \n",
              "28              0.668843       0.776429         1.000000            0.721993   \n",
              "29              0.583505       0.677718         1.000000            0.721993   \n",
              "30              0.679954       0.789097         1.000000            0.721993   \n",
              "31              0.721993       0.837985         1.000000            0.721993   \n",
              "32              0.711684       0.825765         1.000000            0.721993   \n",
              "33              0.595647       0.691012         1.000000            0.721993   \n",
              "34              0.721993       0.837910         1.000000            0.721993   \n",
              "35              0.648339       0.752346         1.000000            0.721993   \n",
              "36              0.657503       0.763726         1.000000            0.721993   \n",
              "37              0.721993       0.837937         1.000000            0.721993   \n",
              "38              0.646735       0.751126         1.000000            0.721993   \n",
              "39              0.690493       0.801252         1.000000            0.721993   \n",
              "40              0.701260       0.813779         1.000000            0.721993   \n",
              "41              0.721993       0.837870         1.000000            0.721993   \n",
              "42              0.668156       0.775835         1.000000            0.721993   \n",
              "43              0.711226       0.825556         1.000000            0.721993   \n",
              "44              0.721993       0.838066         1.000000            0.721993   \n",
              "45              0.594273       0.689978         1.000000            0.721993   \n",
              "46              0.700573       0.813271         1.000000            0.721993   \n",
              "47              0.700916       0.813547         1.000000            0.721993   \n",
              "48              0.721993       0.837748         1.000000            0.721993   \n",
              "49              0.721993       0.837773         1.000000            0.721993   \n",
              "50              0.721993       0.837890         1.000000            0.721993   \n",
              "51              0.678121       0.787724         1.000000            0.721993   \n",
              "52              0.988852       0.990890         0.987632            0.982015   \n",
              "53              0.963243       0.969978         0.977256            0.951346   \n",
              "54              0.989479       0.991033         0.989509            0.979254   \n",
              "55              0.973587       0.978911         0.980046            0.958544   \n",
              "\n",
              "    best_val_f1 best_test_recall best_test_precision best_test_f1  \n",
              "0      0.838555              1.0            0.721841     0.838452  \n",
              "1      0.838520             None                None         None  \n",
              "2      0.838520             None                None         None  \n",
              "3      0.838520             None                None         None  \n",
              "4      0.838520             None                None         None  \n",
              "5      0.838520             None                None         None  \n",
              "6      0.838520             None                None         None  \n",
              "7      0.838520             None                None         None  \n",
              "8      0.838520             None                None         None  \n",
              "9      0.838520             None                None         None  \n",
              "10     0.838520             None                None         None  \n",
              "11     0.838520             None                None         None  \n",
              "12     0.838520             None                None         None  \n",
              "13     0.838520             None                None         None  \n",
              "14     0.838520             None                None         None  \n",
              "15     0.838520             None                None         None  \n",
              "16     0.838520             None                None         None  \n",
              "17     0.838520             None                None         None  \n",
              "18     0.838520             None                None         None  \n",
              "19     0.838520             None                None         None  \n",
              "20     0.838520             None                None         None  \n",
              "21     0.838520             None                None         None  \n",
              "22     0.838520             None                None         None  \n",
              "23     0.838520             None                None         None  \n",
              "24     0.838520             None                None         None  \n",
              "25     0.838520             None                None         None  \n",
              "26     0.838520             None                None         None  \n",
              "27     0.838520             None                None         None  \n",
              "28     0.838520             None                None         None  \n",
              "29     0.838520             None                None         None  \n",
              "30     0.838520             None                None         None  \n",
              "31     0.838520             None                None         None  \n",
              "32     0.838520             None                None         None  \n",
              "33     0.838520             None                None         None  \n",
              "34     0.838520             None                None         None  \n",
              "35     0.838520             None                None         None  \n",
              "36     0.838520             None                None         None  \n",
              "37     0.838520             None                None         None  \n",
              "38     0.838520             None                None         None  \n",
              "39     0.838520             None                None         None  \n",
              "40     0.838520             None                None         None  \n",
              "41     0.838520             None                None         None  \n",
              "42     0.838520             None                None         None  \n",
              "43     0.838520             None                None         None  \n",
              "44     0.838520             None                None         None  \n",
              "45     0.838520             None                None         None  \n",
              "46     0.838520             None                None         None  \n",
              "47     0.838520             None                None         None  \n",
              "48     0.838520             None                None         None  \n",
              "49     0.838520             None                None         None  \n",
              "50     0.838520             None                None         None  \n",
              "51     0.838520             None                None         None  \n",
              "52     0.984813             None                None         None  \n",
              "53     0.964114             None                None         None  \n",
              "54     0.984354             None                None         None  \n",
              "55     0.969167             None                None         None  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5a447e5-643b-4942-ba16-9ca5790031ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>hyperparameter</th>\n",
              "      <th>best_epoch</th>\n",
              "      <th>best_train_cost</th>\n",
              "      <th>best_val_cost</th>\n",
              "      <th>best_train_recall</th>\n",
              "      <th>best_train_precision</th>\n",
              "      <th>best_train_f1</th>\n",
              "      <th>best_val_recall</th>\n",
              "      <th>best_val_precision</th>\n",
              "      <th>best_val_f1</th>\n",
              "      <th>best_test_recall</th>\n",
              "      <th>best_test_precision</th>\n",
              "      <th>best_test_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_baseline</td>\n",
              "      <td>seed=42</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838555</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721841</td>\n",
              "      <td>0.838452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237551</td>\n",
              "      <td>0.228324</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837994</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0005...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.247199</td>\n",
              "      <td>0.235428</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837963</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0001...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.268154</td>\n",
              "      <td>0.262874</td>\n",
              "      <td>0.384359</td>\n",
              "      <td>0.293013</td>\n",
              "      <td>0.322972</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237496</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837926</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237695</td>\n",
              "      <td>0.228308</td>\n",
              "      <td>0.926690</td>\n",
              "      <td>0.670561</td>\n",
              "      <td>0.777513</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.232703</td>\n",
              "      <td>0.227591</td>\n",
              "      <td>0.985338</td>\n",
              "      <td>0.712027</td>\n",
              "      <td>0.826183</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.230862</td>\n",
              "      <td>0.22799</td>\n",
              "      <td>0.985338</td>\n",
              "      <td>0.711340</td>\n",
              "      <td>0.825672</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.245969</td>\n",
              "      <td>0.228488</td>\n",
              "      <td>0.824055</td>\n",
              "      <td>0.595074</td>\n",
              "      <td>0.690655</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.238722</td>\n",
              "      <td>0.229009</td>\n",
              "      <td>0.868041</td>\n",
              "      <td>0.628293</td>\n",
              "      <td>0.728386</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.231409</td>\n",
              "      <td>0.227876</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837847</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.230496</td>\n",
              "      <td>0.228034</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837790</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.258267</td>\n",
              "      <td>0.231803</td>\n",
              "      <td>0.677434</td>\n",
              "      <td>0.487973</td>\n",
              "      <td>0.566772</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.230636</td>\n",
              "      <td>0.227791</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837965</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.240435</td>\n",
              "      <td>0.227988</td>\n",
              "      <td>0.912027</td>\n",
              "      <td>0.657732</td>\n",
              "      <td>0.763680</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.229898</td>\n",
              "      <td>0.228008</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837944</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.231874</td>\n",
              "      <td>0.227825</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837898</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.235135</td>\n",
              "      <td>0.227605</td>\n",
              "      <td>0.912027</td>\n",
              "      <td>0.656930</td>\n",
              "      <td>0.763369</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.236581</td>\n",
              "      <td>0.228055</td>\n",
              "      <td>0.926690</td>\n",
              "      <td>0.667583</td>\n",
              "      <td>0.775602</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.230123</td>\n",
              "      <td>0.228134</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838059</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.247814</td>\n",
              "      <td>0.227805</td>\n",
              "      <td>0.780228</td>\n",
              "      <td>0.570676</td>\n",
              "      <td>0.654196</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.234476</td>\n",
              "      <td>0.22773</td>\n",
              "      <td>0.926690</td>\n",
              "      <td>0.669645</td>\n",
              "      <td>0.776967</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.232707</td>\n",
              "      <td>0.227596</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837959</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.231131</td>\n",
              "      <td>0.227829</td>\n",
              "      <td>0.970676</td>\n",
              "      <td>0.701031</td>\n",
              "      <td>0.813465</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.242391</td>\n",
              "      <td>0.228181</td>\n",
              "      <td>0.868041</td>\n",
              "      <td>0.626231</td>\n",
              "      <td>0.726825</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.251862</td>\n",
              "      <td>0.227611</td>\n",
              "      <td>0.736082</td>\n",
              "      <td>0.531271</td>\n",
              "      <td>0.616538</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.232329</td>\n",
              "      <td>0.227781</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838102</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.23092</td>\n",
              "      <td>0.227611</td>\n",
              "      <td>0.985338</td>\n",
              "      <td>0.711340</td>\n",
              "      <td>0.825631</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.246597</td>\n",
              "      <td>0.230152</td>\n",
              "      <td>0.926690</td>\n",
              "      <td>0.668843</td>\n",
              "      <td>0.776429</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.248086</td>\n",
              "      <td>0.22897</td>\n",
              "      <td>0.809393</td>\n",
              "      <td>0.583505</td>\n",
              "      <td>0.677718</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.240369</td>\n",
              "      <td>0.22881</td>\n",
              "      <td>0.941352</td>\n",
              "      <td>0.679954</td>\n",
              "      <td>0.789097</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233591</td>\n",
              "      <td>0.228861</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837985</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.244041</td>\n",
              "      <td>0.229647</td>\n",
              "      <td>0.985338</td>\n",
              "      <td>0.711684</td>\n",
              "      <td>0.825765</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.246053</td>\n",
              "      <td>0.228722</td>\n",
              "      <td>0.824055</td>\n",
              "      <td>0.595647</td>\n",
              "      <td>0.691012</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233413</td>\n",
              "      <td>0.229419</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837910</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0009...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.239309</td>\n",
              "      <td>0.227842</td>\n",
              "      <td>0.897365</td>\n",
              "      <td>0.648339</td>\n",
              "      <td>0.752346</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.248298</td>\n",
              "      <td>0.230968</td>\n",
              "      <td>0.912027</td>\n",
              "      <td>0.657503</td>\n",
              "      <td>0.763726</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.238547</td>\n",
              "      <td>0.228958</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837937</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24648</td>\n",
              "      <td>0.229094</td>\n",
              "      <td>0.897365</td>\n",
              "      <td>0.646735</td>\n",
              "      <td>0.751126</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.241609</td>\n",
              "      <td>0.230321</td>\n",
              "      <td>0.956014</td>\n",
              "      <td>0.690493</td>\n",
              "      <td>0.801252</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237574</td>\n",
              "      <td>0.227922</td>\n",
              "      <td>0.970676</td>\n",
              "      <td>0.701260</td>\n",
              "      <td>0.813779</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.231801</td>\n",
              "      <td>0.228474</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837870</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237987</td>\n",
              "      <td>0.227953</td>\n",
              "      <td>0.926690</td>\n",
              "      <td>0.668156</td>\n",
              "      <td>0.775835</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.001...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.227601</td>\n",
              "      <td>0.985338</td>\n",
              "      <td>0.711226</td>\n",
              "      <td>0.825556</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233949</td>\n",
              "      <td>0.227641</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838066</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.243257</td>\n",
              "      <td>0.227643</td>\n",
              "      <td>0.824055</td>\n",
              "      <td>0.594273</td>\n",
              "      <td>0.689978</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.234485</td>\n",
              "      <td>0.227701</td>\n",
              "      <td>0.970676</td>\n",
              "      <td>0.700573</td>\n",
              "      <td>0.813271</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233095</td>\n",
              "      <td>0.227593</td>\n",
              "      <td>0.970676</td>\n",
              "      <td>0.700916</td>\n",
              "      <td>0.813547</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233624</td>\n",
              "      <td>0.227623</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837748</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.235389</td>\n",
              "      <td>0.227735</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837773</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.236105</td>\n",
              "      <td>0.227592</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837890</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>lstm</td>\n",
              "      <td>{'weight_decay': 0.001, 'learning_rate': 0.000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.23537</td>\n",
              "      <td>0.227736</td>\n",
              "      <td>0.941352</td>\n",
              "      <td>0.678121</td>\n",
              "      <td>0.787724</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>40</td>\n",
              "      <td>0.022594</td>\n",
              "      <td>0.032407</td>\n",
              "      <td>0.993049</td>\n",
              "      <td>0.988852</td>\n",
              "      <td>0.990890</td>\n",
              "      <td>0.987632</td>\n",
              "      <td>0.982015</td>\n",
              "      <td>0.984813</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>48</td>\n",
              "      <td>0.050842</td>\n",
              "      <td>0.064538</td>\n",
              "      <td>0.977197</td>\n",
              "      <td>0.963243</td>\n",
              "      <td>0.969978</td>\n",
              "      <td>0.977256</td>\n",
              "      <td>0.951346</td>\n",
              "      <td>0.964114</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>40</td>\n",
              "      <td>0.023436</td>\n",
              "      <td>0.033765</td>\n",
              "      <td>0.992715</td>\n",
              "      <td>0.989479</td>\n",
              "      <td>0.991033</td>\n",
              "      <td>0.989509</td>\n",
              "      <td>0.979254</td>\n",
              "      <td>0.984354</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>48</td>\n",
              "      <td>0.041095</td>\n",
              "      <td>0.05397</td>\n",
              "      <td>0.984468</td>\n",
              "      <td>0.973587</td>\n",
              "      <td>0.978911</td>\n",
              "      <td>0.980046</td>\n",
              "      <td>0.958544</td>\n",
              "      <td>0.969167</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5a447e5-643b-4942-ba16-9ca5790031ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b5a447e5-643b-4942-ba16-9ca5790031ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b5a447e5-643b-4942-ba16-9ca5790031ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/99dac6621f6ae8c4/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_baseline\",\n\"seed=42\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385551786070644,\n            'f': \"0.8385551786070644\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385551786070644,\n            'f': \"0.8385551786070644\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7218406593406593,\n            'f': \"0.7218406593406593\",\n        },\n{\n            'v': 0.8384523334662944,\n            'f': \"0.8384523334662944\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23755095152852188,\n            'f': \"0.23755095152852188\",\n        },\n{\n            'v': 0.2283238747890053,\n            'f': \"0.2283238747890053\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379937470948515,\n            'f': \"0.8379937470948515\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0005, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24719946554777686,\n            'f': \"0.24719946554777686\",\n        },\n{\n            'v': 0.235427502913983,\n            'f': \"0.235427502913983\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379629078268853,\n            'f': \"0.8379629078268853\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0.26815437865694225,\n            'f': \"0.26815437865694225\",\n        },\n{\n            'v': 0.2628738533180604,\n            'f': \"0.2628738533180604\",\n        },\n{\n            'v': 0.38435905446214724,\n            'f': \"0.38435905446214724\",\n        },\n{\n            'v': 0.2930126002290951,\n            'f': \"0.2930126002290951\",\n        },\n{\n            'v': 0.3229722700122437,\n            'f': \"0.3229722700122437\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23749594646426,\n            'f': \"0.23749594646426\",\n        },\n{\n            'v': 0.22849985904914816,\n            'f': \"0.22849985904914816\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379264789299304,\n            'f': \"0.8379264789299304\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23769533075008195,\n            'f': \"0.23769533075008195\",\n        },\n{\n            'v': 0.22830794468982932,\n            'f': \"0.22830794468982932\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6705612829324169,\n            'f': \"0.6705612829324169\",\n        },\n{\n            'v': 0.777512641968225,\n            'f': \"0.777512641968225\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 6,\n            'f': \"6\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23270269353092057,\n            'f': \"0.23270269353092057\",\n        },\n{\n            'v': 0.2275912654051666,\n            'f': \"0.2275912654051666\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.7120274914089347,\n            'f': \"0.7120274914089347\",\n        },\n{\n            'v': 0.8261833872750113,\n            'f': \"0.8261833872750113\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 7,\n            'f': \"7\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23086243700940176,\n            'f': \"0.23086243700940176\",\n        },\n{\n            'v': 0.22798991973457466,\n            'f': \"0.22798991973457466\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.711340206185567,\n            'f': \"0.711340206185567\",\n        },\n{\n            'v': 0.8256718950601964,\n            'f': \"0.8256718950601964\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 8,\n            'f': \"8\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24596911503550387,\n            'f': \"0.24596911503550387\",\n        },\n{\n            'v': 0.22848816736252447,\n            'f': \"0.22848816736252447\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5950744558991982,\n            'f': \"0.5950744558991982\",\n        },\n{\n            'v': 0.6906552425563023,\n            'f': \"0.6906552425563023\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 9,\n            'f': \"9\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23872165301684525,\n            'f': \"0.23872165301684525\",\n        },\n{\n            'v': 0.22900927653632214,\n            'f': \"0.22900927653632214\",\n        },\n{\n            'v': 0.8680412371134021,\n            'f': \"0.8680412371134021\",\n        },\n{\n            'v': 0.6282932416953035,\n            'f': \"0.6282932416953035\",\n        },\n{\n            'v': 0.7283860077496157,\n            'f': \"0.7283860077496157\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 10,\n            'f': \"10\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23140935585124658,\n            'f': \"0.23140935585124658\",\n        },\n{\n            'v': 0.2278759803661366,\n            'f': \"0.2278759803661366\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378469602887191,\n            'f': \"0.8378469602887191\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 11,\n            'f': \"11\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23049640620696996,\n            'f': \"0.23049640620696996\",\n        },\n{\n            'v': 0.2280344991647091,\n            'f': \"0.2280344991647091\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377898005373192,\n            'f': \"0.8377898005373192\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 12,\n            'f': \"12\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2582674642540472,\n            'f': \"0.2582674642540472\",\n        },\n{\n            'v': 0.2318031070261067,\n            'f': \"0.2318031070261067\",\n        },\n{\n            'v': 0.677434135166094,\n            'f': \"0.677434135166094\",\n        },\n{\n            'v': 0.4879725085910653,\n            'f': \"0.4879725085910653\",\n        },\n{\n            'v': 0.5667718616335917,\n            'f': \"0.5667718616335917\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 13,\n            'f': \"13\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23063589629438735,\n            'f': \"0.23063589629438735\",\n        },\n{\n            'v': 0.22779080836019155,\n            'f': \"0.22779080836019155\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379650916960658,\n            'f': \"0.8379650916960658\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 14,\n            'f': \"14\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24043467447656672,\n            'f': \"0.24043467447656672\",\n        },\n{\n            'v': 0.22798836162614658,\n            'f': \"0.22798836162614658\",\n        },\n{\n            'v': 0.9120274914089347,\n            'f': \"0.9120274914089347\",\n        },\n{\n            'v': 0.6577319587628866,\n            'f': \"0.6577319587628866\",\n        },\n{\n            'v': 0.7636797948350783,\n            'f': \"0.7636797948350783\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 15,\n            'f': \"15\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.22989786069281185,\n            'f': \"0.22989786069281185\",\n        },\n{\n            'v': 0.22800785562221948,\n            'f': \"0.22800785562221948\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379435108647284,\n            'f': \"0.8379435108647284\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 16,\n            'f': \"16\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23187371032549345,\n            'f': \"0.23187371032549345\",\n        },\n{\n            'v': 0.22782516483588727,\n            'f': \"0.22782516483588727\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378981891847632,\n            'f': \"0.8378981891847632\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 17,\n            'f': \"17\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23513470497259706,\n            'f': \"0.23513470497259706\",\n        },\n{\n            'v': 0.22760502922371081,\n            'f': \"0.22760502922371081\",\n        },\n{\n            'v': 0.9120274914089347,\n            'f': \"0.9120274914089347\",\n        },\n{\n            'v': 0.656930126002291,\n            'f': \"0.656930126002291\",\n        },\n{\n            'v': 0.7633690624041659,\n            'f': \"0.7633690624041659\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 18,\n            'f': \"18\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23658089066630514,\n            'f': \"0.23658089066630514\",\n        },\n{\n            'v': 0.22805524370104996,\n            'f': \"0.22805524370104996\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6675830469644902,\n            'f': \"0.6675830469644902\",\n        },\n{\n            'v': 0.775601528856061,\n            'f': \"0.775601528856061\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 19,\n            'f': \"19\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23012308565406864,\n            'f': \"0.23012308565406864\",\n        },\n{\n            'v': 0.2281344659754501,\n            'f': \"0.2281344659754501\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8380585970171258,\n            'f': \"0.8380585970171258\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 20,\n            'f': \"20\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24781396114539445,\n            'f': \"0.24781396114539445\",\n        },\n{\n            'v': 0.22780490974380388,\n            'f': \"0.22780490974380388\",\n        },\n{\n            'v': 0.7802280990089148,\n            'f': \"0.7802280990089148\",\n        },\n{\n            'v': 0.5706758304696449,\n            'f': \"0.5706758304696449\",\n        },\n{\n            'v': 0.6541961675540556,\n            'f': \"0.6541961675540556\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 21,\n            'f': \"21\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2344763796422353,\n            'f': \"0.2344763796422353\",\n        },\n{\n            'v': 0.22773042042640476,\n            'f': \"0.22773042042640476\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6696449026345933,\n            'f': \"0.6696449026345933\",\n        },\n{\n            'v': 0.7769669688757592,\n            'f': \"0.7769669688757592\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 22,\n            'f': \"22\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23270658465253802,\n            'f': \"0.23270658465253802\",\n        },\n{\n            'v': 0.22759577791715405,\n            'f': \"0.22759577791715405\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379593694587404,\n            'f': \"0.8379593694587404\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 23,\n            'f': \"23\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23113079530540065,\n            'f': \"0.23113079530540065\",\n        },\n{\n            'v': 0.22782862943267496,\n            'f': \"0.22782862943267496\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7010309278350515,\n            'f': \"0.7010309278350515\",\n        },\n{\n            'v': 0.8134646001971478,\n            'f': \"0.8134646001971478\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 24,\n            'f': \"24\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24239094583370976,\n            'f': \"0.24239094583370976\",\n        },\n{\n            'v': 0.22818102553947683,\n            'f': \"0.22818102553947683\",\n        },\n{\n            'v': 0.8680412371134021,\n            'f': \"0.8680412371134021\",\n        },\n{\n            'v': 0.6262313860252005,\n            'f': \"0.6262313860252005\",\n        },\n{\n            'v': 0.7268249415921961,\n            'f': \"0.7268249415921961\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 25,\n            'f': \"25\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2518623998826039,\n            'f': \"0.2518623998826039\",\n        },\n{\n            'v': 0.22761076536170397,\n            'f': \"0.22761076536170397\",\n        },\n{\n            'v': 0.7360824742268042,\n            'f': \"0.7360824742268042\",\n        },\n{\n            'v': 0.5312714776632302,\n            'f': \"0.5312714776632302\",\n        },\n{\n            'v': 0.6165380489730845,\n            'f': \"0.6165380489730845\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 26,\n            'f': \"26\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.232329122092306,\n            'f': \"0.232329122092306\",\n        },\n{\n            'v': 0.2277810290507025,\n            'f': \"0.2277810290507025\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8381023417873185,\n            'f': \"0.8381023417873185\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 27,\n            'f': \"27\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23091984057003143,\n            'f': \"0.23091984057003143\",\n        },\n{\n            'v': 0.22761146157877551,\n            'f': \"0.22761146157877551\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.711340206185567,\n            'f': \"0.711340206185567\",\n        },\n{\n            'v': 0.8256305016925937,\n            'f': \"0.8256305016925937\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 28,\n            'f': \"28\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24659692590452414,\n            'f': \"0.24659692590452414\",\n        },\n{\n            'v': 0.2301524313557189,\n            'f': \"0.2301524313557189\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6688430698739977,\n            'f': \"0.6688430698739977\",\n        },\n{\n            'v': 0.7764289639170969,\n            'f': \"0.7764289639170969\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 29,\n            'f': \"29\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24808622859802443,\n            'f': \"0.24808622859802443\",\n        },\n{\n            'v': 0.22897008621610726,\n            'f': \"0.22897008621610726\",\n        },\n{\n            'v': 0.8093928980526919,\n            'f': \"0.8093928980526919\",\n        },\n{\n            'v': 0.5835051546391753,\n            'f': \"0.5835051546391753\",\n        },\n{\n            'v': 0.6777182478215532,\n            'f': \"0.6777182478215532\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 30,\n            'f': \"30\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24036915460676125,\n            'f': \"0.24036915460676125\",\n        },\n{\n            'v': 0.22881030144355552,\n            'f': \"0.22881030144355552\",\n        },\n{\n            'v': 0.9413516609392898,\n            'f': \"0.9413516609392898\",\n        },\n{\n            'v': 0.6799541809851088,\n            'f': \"0.6799541809851088\",\n        },\n{\n            'v': 0.7890973790044737,\n            'f': \"0.7890973790044737\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 31,\n            'f': \"31\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.233591164599033,\n            'f': \"0.233591164599033\",\n        },\n{\n            'v': 0.2288613215549705,\n            'f': \"0.2288613215549705\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379850303243254,\n            'f': \"0.8379850303243254\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 32,\n            'f': \"32\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24404066760105775,\n            'f': \"0.24404066760105775\",\n        },\n{\n            'v': 0.22964662380030065,\n            'f': \"0.22964662380030065\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.7116838487972509,\n            'f': \"0.7116838487972509\",\n        },\n{\n            'v': 0.8257649556000859,\n            'f': \"0.8257649556000859\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 33,\n            'f': \"33\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24605310336011382,\n            'f': \"0.24605310336011382\",\n        },\n{\n            'v': 0.22872167988536285,\n            'f': \"0.22872167988536285\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5956471935853379,\n            'f': \"0.5956471935853379\",\n        },\n{\n            'v': 0.6910118096782196,\n            'f': \"0.6910118096782196\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 34,\n            'f': \"34\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2334126355790054,\n            'f': \"0.2334126355790054\",\n        },\n{\n            'v': 0.2294187471321768,\n            'f': \"0.2294187471321768\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379098005498145,\n            'f': \"0.8379098005498145\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 35,\n            'f': \"35\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23930862564742905,\n            'f': \"0.23930862564742905\",\n        },\n{\n            'v': 0.22784182982346446,\n            'f': \"0.22784182982346446\",\n        },\n{\n            'v': 0.8973654066437572,\n            'f': \"0.8973654066437572\",\n        },\n{\n            'v': 0.6483390607101948,\n            'f': \"0.6483390607101948\",\n        },\n{\n            'v': 0.7523462888410456,\n            'f': \"0.7523462888410456\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 36,\n            'f': \"36\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24829804776460446,\n            'f': \"0.24829804776460446\",\n        },\n{\n            'v': 0.23096789029455678,\n            'f': \"0.23096789029455678\",\n        },\n{\n            'v': 0.9120274914089347,\n            'f': \"0.9120274914089347\",\n        },\n{\n            'v': 0.6575028636884307,\n            'f': \"0.6575028636884307\",\n        },\n{\n            'v': 0.7637263087855324,\n            'f': \"0.7637263087855324\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 37,\n            'f': \"37\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23854704901046905,\n            'f': \"0.23854704901046905\",\n        },\n{\n            'v': 0.2289582008860775,\n            'f': \"0.2289582008860775\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.837937023496059,\n            'f': \"0.837937023496059\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 38,\n            'f': \"38\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24648009746208496,\n            'f': \"0.24648009746208496\",\n        },\n{\n            'v': 0.22909368780470385,\n            'f': \"0.22909368780470385\",\n        },\n{\n            'v': 0.8973654066437572,\n            'f': \"0.8973654066437572\",\n        },\n{\n            'v': 0.6467353951890035,\n            'f': \"0.6467353951890035\",\n        },\n{\n            'v': 0.7511256735881737,\n            'f': \"0.7511256735881737\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 39,\n            'f': \"39\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24160942262868565,\n            'f': \"0.24160942262868565\",\n        },\n{\n            'v': 0.23032080584375308,\n            'f': \"0.23032080584375308\",\n        },\n{\n            'v': 0.9560137457044674,\n            'f': \"0.9560137457044674\",\n        },\n{\n            'v': 0.6904925544100802,\n            'f': \"0.6904925544100802\",\n        },\n{\n            'v': 0.8012523489553404,\n            'f': \"0.8012523489553404\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 40,\n            'f': \"40\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23757384251483118,\n            'f': \"0.23757384251483118\",\n        },\n{\n            'v': 0.2279223526159103,\n            'f': \"0.2279223526159103\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7012600229095074,\n            'f': \"0.7012600229095074\",\n        },\n{\n            'v': 0.8137792079514921,\n            'f': \"0.8137792079514921\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 41,\n            'f': \"41\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23180127218621702,\n            'f': \"0.23180127218621702\",\n        },\n{\n            'v': 0.22847400504289214,\n            'f': \"0.22847400504289214\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378703062107288,\n            'f': \"0.8378703062107288\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 42,\n            'f': \"42\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23798671251788037,\n            'f': \"0.23798671251788037\",\n        },\n{\n            'v': 0.22795328791813343,\n            'f': \"0.22795328791813343\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.66815578465063,\n            'f': \"0.66815578465063\",\n        },\n{\n            'v': 0.7758350982211567,\n            'f': \"0.7758350982211567\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 43,\n            'f': \"43\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23300043067981288,\n            'f': \"0.23300043067981288\",\n        },\n{\n            'v': 0.22760143634379934,\n            'f': \"0.22760143634379934\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.7112256586483391,\n            'f': \"0.7112256586483391\",\n        },\n{\n            'v': 0.8255556394557227,\n            'f': \"0.8255556394557227\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 44,\n            'f': \"44\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23394868339050268,\n            'f': \"0.23394868339050268\",\n        },\n{\n            'v': 0.22764059617552151,\n            'f': \"0.22764059617552151\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8380657319957353,\n            'f': \"0.8380657319957353\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 45,\n            'f': \"45\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24325722402723385,\n            'f': \"0.24325722402723385\",\n        },\n{\n            'v': 0.22764322254870764,\n            'f': \"0.22764322254870764\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5942726231386025,\n            'f': \"0.5942726231386025\",\n        },\n{\n            'v': 0.6899781748662641,\n            'f': \"0.6899781748662641\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 46,\n            'f': \"46\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23448481141223668,\n            'f': \"0.23448481141223668\",\n        },\n{\n            'v': 0.22770088012890308,\n            'f': \"0.22770088012890308\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7005727376861397,\n            'f': \"0.7005727376861397\",\n        },\n{\n            'v': 0.8132708898924303,\n            'f': \"0.8132708898924303\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 47,\n            'f': \"47\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2330947979892106,\n            'f': \"0.2330947979892106\",\n        },\n{\n            'v': 0.22759252305293,\n            'f': \"0.22759252305293\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7009163802978235,\n            'f': \"0.7009163802978235\",\n        },\n{\n            'v': 0.8135473343847958,\n            'f': \"0.8135473343847958\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 48,\n            'f': \"48\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23362354004983912,\n            'f': \"0.23362354004983912\",\n        },\n{\n            'v': 0.22762263073134667,\n            'f': \"0.22762263073134667\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377483941926969,\n            'f': \"0.8377483941926969\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 49,\n            'f': \"49\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2353885561330212,\n            'f': \"0.2353885561330212\",\n        },\n{\n            'v': 0.22773463401392974,\n            'f': \"0.22773463401392974\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377726111607469,\n            'f': \"0.8377726111607469\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 50,\n            'f': \"50\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23610532127090336,\n            'f': \"0.23610532127090336\",\n        },\n{\n            'v': 0.22759167250898696,\n            'f': \"0.22759167250898696\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378901998288699,\n            'f': \"0.8378901998288699\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 51,\n            'f': \"51\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23536989676925052,\n            'f': \"0.23536989676925052\",\n        },\n{\n            'v': 0.22773557217260407,\n            'f': \"0.22773557217260407\",\n        },\n{\n            'v': 0.9413516609392898,\n            'f': \"0.9413516609392898\",\n        },\n{\n            'v': 0.6781214203894617,\n            'f': \"0.6781214203894617\",\n        },\n{\n            'v': 0.7877237131052639,\n            'f': \"0.7877237131052639\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 52,\n            'f': \"52\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128}\",\n{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': 0.022594121824800625,\n            'f': \"0.022594121824800625\",\n        },\n{\n            'v': 0.0324072357766407,\n            'f': \"0.0324072357766407\",\n        },\n{\n            'v': 0.9930491495835343,\n            'f': \"0.9930491495835343\",\n        },\n{\n            'v': 0.9888522839808216,\n            'f': \"0.9888522839808216\",\n        },\n{\n            'v': 0.9908897871389477,\n            'f': \"0.9908897871389477\",\n        },\n{\n            'v': 0.9876317632765976,\n            'f': \"0.9876317632765976\",\n        },\n{\n            'v': 0.9820149289352244,\n            'f': \"0.9820149289352244\",\n        },\n{\n            'v': 0.9848132951156789,\n            'f': \"0.9848132951156789\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 53,\n            'f': \"53\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128}\",\n{\n            'v': 48,\n            'f': \"48\",\n        },\n{\n            'v': 0.0508416385021816,\n            'f': \"0.0508416385021816\",\n        },\n{\n            'v': 0.0645379028369471,\n            'f': \"0.0645379028369471\",\n        },\n{\n            'v': 0.9771973487937176,\n            'f': \"0.9771973487937176\",\n        },\n{\n            'v': 0.9632434459063973,\n            'f': \"0.9632434459063973\",\n        },\n{\n            'v': 0.969978386968445,\n            'f': \"0.969978386968445\",\n        },\n{\n            'v': 0.9772559884770459,\n            'f': \"0.9772559884770459\",\n        },\n{\n            'v': 0.9513458385652597,\n            'f': \"0.9513458385652597\",\n        },\n{\n            'v': 0.9641136834435939,\n            'f': \"0.9641136834435939\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 54,\n            'f': \"54\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128}\",\n{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': 0.02343625199211866,\n            'f': \"0.02343625199211866\",\n        },\n{\n            'v': 0.033765379019414433,\n            'f': \"0.033765379019414433\",\n        },\n{\n            'v': 0.9927152671716448,\n            'f': \"0.9927152671716448\",\n        },\n{\n            'v': 0.9894789600382609,\n            'f': \"0.9894789600382609\",\n        },\n{\n            'v': 0.991033007737149,\n            'f': \"0.991033007737149\",\n        },\n{\n            'v': 0.9895085101265405,\n            'f': \"0.9895085101265405\",\n        },\n{\n            'v': 0.9792537562341926,\n            'f': \"0.9792537562341926\",\n        },\n{\n            'v': 0.9843540435633988,\n            'f': \"0.9843540435633988\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 55,\n            'f': \"55\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128}\",\n{\n            'v': 48,\n            'f': \"48\",\n        },\n{\n            'v': 0.041094861244452094,\n            'f': \"0.041094861244452094\",\n        },\n{\n            'v': 0.05397045065447227,\n            'f': \"0.05397045065447227\",\n        },\n{\n            'v': 0.9844684149339925,\n            'f': \"0.9844684149339925\",\n        },\n{\n            'v': 0.9735874425621157,\n            'f': \"0.9735874425621157\",\n        },\n{\n            'v': 0.9789105681946668,\n            'f': \"0.9789105681946668\",\n        },\n{\n            'v': 0.9800461991949599,\n            'f': \"0.9800461991949599\",\n        },\n{\n            'v': 0.9585436117727595,\n            'f': \"0.9585436117727595\",\n        },\n{\n            'v': 0.9691670516937745,\n            'f': \"0.9691670516937745\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"model_name\"], [\"string\", \"hyperparameter\"], [\"number\", \"best_epoch\"], [\"number\", \"best_train_cost\"], [\"number\", \"best_val_cost\"], [\"number\", \"best_train_recall\"], [\"number\", \"best_train_precision\"], [\"number\", \"best_train_f1\"], [\"number\", \"best_val_recall\"], [\"number\", \"best_val_precision\"], [\"number\", \"best_val_f1\"], [\"number\", \"best_test_recall\"], [\"number\", \"best_test_precision\"], [\"number\", \"best_test_f1\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: \"0\",\n      });\n    "
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After comparing the validation F1 scores, it was found that the model with an initial learning rate of 1e-3 performed the best. Therefore, in the next step, we will increase the number of epochs to allow the model to converge even further.\n",
        "\n",
        "Additionally, since the default pre-trained model freezes only the first 3 residual blocks but not the fourth and fifth blocks in the feature extraction layer, we need to decide which residual block to stop freezing during training. This will allow us to fine-tune the model more effectively and improve its performance on the current task."
      ],
      "metadata": {
        "id": "JVyClc085drq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#benchmark.to_csv(PATH + '/benchmark.csv', index=False)"
      ],
      "metadata": {
        "id": "xIUUxD28D5x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Define the hyperparameters to search over\n",
        "def experiment_3(benchmark, X_train = X_train ,y_train = y_train, X_val = X_val, y_val = y_val):\n",
        "    name = ['deep_rescnn']\n",
        "    weight_decay = [1e-2]\n",
        "    lr = [10**-random.uniform(2.52,3.1) for i in range(5)] #10**-3.1 ~ 8e-4\n",
        "    batch_size = [64,128]\n",
        "    pretrained = [True]\n",
        "    start_layer = [2,3,4,5]\n",
        "    # Create a Cartesian product of all hyperparameter combinations\n",
        "    hyperparams = list(itertools.product(name, weight_decay, lr, batch_size, pretrained, start_layer))\n",
        "    print(f'grid search : perform {len(hyperparams)} searchs')\n",
        "\n",
        "    # Loop over each combination of hyperparameters and train/evaluate the model\n",
        "    best_val_f1 = 0.0\n",
        "    num_epochs = 75\n",
        "\n",
        "    train_set = MI_Dataset(X_train,y_train, truncate_to_max=False)\n",
        "    val_set = MI_Dataset(X_val,y_val, truncate_to_max=False)\n",
        "\n",
        "    history = {}\n",
        "    for i, (name, wd, lr, bs, pt, sl) in enumerate(hyperparams):\n",
        "\n",
        "        print(f'\\n 🔎 search {i} : {name} --- lr : {lr}, weight_decay : {wd}, batch_size : {bs}, pretrain : {pt}, start_layer : conv{sl}.1_weight')\n",
        "        search_id = f'{name}(pretrain={pt},weight_decay={wd},learning_rate={lr},batch_size={bs}, start_layer = conv{sl}.1_weight)' \n",
        "        \n",
        "        # Setup Device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Create the model, optimizer, and loss function\n",
        "        model = get_architecture(name = name, device = device, pretrained = pt, start_layer = sl)\n",
        "        pos_weight = get_weighted_for_bce(y_train)\n",
        "        criterion = nn.BCEWithLogitsLoss(weight=pos_weight.to(device))\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=wd)\n",
        "\n",
        "        # Create the data loaders\n",
        "        train_loader, val_loader, test_loader = get_loader(train_set, val_set, test_set, train_batch_size=bs)\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n",
        "        model, model_stat, train_loss_lst, train_f1_lst, val_loss_lst, val_f1_lst = train(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, get_history=True)\n",
        "        \n",
        "        history.update({search_id : {'train_loss_lst' : train_loss_lst, \n",
        "                                    'train_f1_lst' : train_f1_lst,\n",
        "                                    'val_loss_lst' : val_loss_lst,\n",
        "                                    'val_f1_lst' : val_f1_lst}})\n",
        "        \n",
        "        # save the best model configuration based on validation F1 score\n",
        "        if model_stat['best_val_f1'] > best_val_f1:\n",
        "            best_val_f1 = model_stat['best_val_f1']\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "        \n",
        "        row = {'model_name' : name,\n",
        "            'hyperparameter' : {'weight_decay' : wd, 'learning_rate' : lr,\n",
        "                                'batch_size' : bs, 'start_layer' : f'conv{sl}.1_weight'},\n",
        "            'best_epoch' : model_stat['best_epoch'],\n",
        "            'best_train_cost' : model_stat['best_train_cost'],\n",
        "            'best_val_cost' : model_stat['best_val_cost'],\n",
        "            'best_train_recall' : model_stat['best_train_recall'],\n",
        "            'best_train_precision' : model_stat['best_train_precision'],\n",
        "            'best_train_f1' : model_stat['best_train_f1'],\n",
        "            'best_val_recall' : model_stat['best_val_recall'],\n",
        "            'best_val_precision' : model_stat['best_val_precision'],\n",
        "            'best_val_f1' : model_stat['best_val_f1'],\n",
        "            'best_test_recall' : None,\n",
        "            'best_test_precision' : None,\n",
        "            'best_test_f1' : None,\n",
        "            }\n",
        "\n",
        "        benchmark =benchmark.append(row, ignore_index=True)\n",
        "    \n",
        "    return benchmark, best_model, history"
      ],
      "metadata": {
        "id": "V9w7feVI48Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To clarify, the start_layer hyperparameter indicates the last layer of the feature extraction layer that is frozen during training. In this case, selecting conv2.1_weight means that the model will stop freezing the parameters after the second residual block, and selecting conv3.1_weight means that the model will stop freezing the parameters after the third residual block.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ygk4GnxSCYz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "benchmark, best_model, history = experiment_3(benchmark)\n",
        "\n",
        "# Take about 1 hr."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VyA9pBu7A1s",
        "outputId": "a5a43f48-1c97-4cd8-ea67-3470bd0e3288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid search : perform 40 searchs\n",
            "\n",
            " 🔎 search 0 : deep_rescnn --- lr : 0.0008753516269381944, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv2.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1321, Train Acc: 0.8570, Train F1: 0.8967 Val Loss: 0.1190, Val Acc: 0.8629, Val F1: 0.9119\n",
            "Epoch: 2/75, Train Loss: 0.0931, Train Acc: 0.9141, Train F1: 0.9404 Val Loss: 0.0952, Val Acc: 0.9103, Val F1: 0.9360\n",
            "Epoch: 3/75, Train Loss: 0.0854, Train Acc: 0.9220, Train F1: 0.9456 Val Loss: 0.0880, Val Acc: 0.9199, Val F1: 0.9436\n",
            "Epoch: 4/75, Train Loss: 0.0812, Train Acc: 0.9262, Train F1: 0.9486 Val Loss: 0.0873, Val Acc: 0.9210, Val F1: 0.9453\n",
            "Epoch: 5/75, Train Loss: 0.0774, Train Acc: 0.9309, Train F1: 0.9519 Val Loss: 0.1047, Val Acc: 0.8838, Val F1: 0.9251\n",
            "Epoch: 6/75, Train Loss: 0.0782, Train Acc: 0.9281, Train F1: 0.9500 Val Loss: 0.0775, Val Acc: 0.9354, Val F1: 0.9555\n",
            "Epoch: 7/75, Train Loss: 0.0772, Train Acc: 0.9285, Train F1: 0.9499 Val Loss: 0.0800, Val Acc: 0.9316, Val F1: 0.9540\n",
            "Epoch: 8/75, Train Loss: 0.0725, Train Acc: 0.9353, Train F1: 0.9547 Val Loss: 0.0796, Val Acc: 0.9337, Val F1: 0.9535\n",
            "Epoch: 9/75, Train Loss: 0.0715, Train Acc: 0.9370, Train F1: 0.9562 Val Loss: 0.0809, Val Acc: 0.9148, Val F1: 0.9398\n",
            "Epoch: 10/75, Train Loss: 0.0703, Train Acc: 0.9368, Train F1: 0.9559 Val Loss: 0.0775, Val Acc: 0.9344, Val F1: 0.9556\n",
            "Epoch: 11/75, Train Loss: 0.0707, Train Acc: 0.9355, Train F1: 0.9550 Val Loss: 0.0724, Val Acc: 0.9351, Val F1: 0.9559\n",
            "Epoch: 12/75, Train Loss: 0.0675, Train Acc: 0.9399, Train F1: 0.9580 Val Loss: 0.0718, Val Acc: 0.9395, Val F1: 0.9586\n",
            "Epoch: 13/75, Train Loss: 0.0664, Train Acc: 0.9412, Train F1: 0.9589 Val Loss: 0.0689, Val Acc: 0.9426, Val F1: 0.9606\n",
            "Epoch: 14/75, Train Loss: 0.0633, Train Acc: 0.9447, Train F1: 0.9614 Val Loss: 0.0712, Val Acc: 0.9399, Val F1: 0.9595\n",
            "Epoch: 15/75, Train Loss: 0.0647, Train Acc: 0.9381, Train F1: 0.9568 Val Loss: 0.0732, Val Acc: 0.9271, Val F1: 0.9505\n",
            "Epoch: 16/75, Train Loss: 0.0646, Train Acc: 0.9433, Train F1: 0.9606 Val Loss: 0.0673, Val Acc: 0.9416, Val F1: 0.9594\n",
            "Epoch: 17/75, Train Loss: 0.0621, Train Acc: 0.9435, Train F1: 0.9605 Val Loss: 0.0641, Val Acc: 0.9447, Val F1: 0.9620\n",
            "Epoch: 18/75, Train Loss: 0.0609, Train Acc: 0.9433, Train F1: 0.9604 Val Loss: 0.0654, Val Acc: 0.9454, Val F1: 0.9631\n",
            "Epoch: 19/75, Train Loss: 0.0573, Train Acc: 0.9495, Train F1: 0.9648 Val Loss: 0.0653, Val Acc: 0.9454, Val F1: 0.9629\n",
            "Epoch: 20/75, Train Loss: 0.0602, Train Acc: 0.9449, Train F1: 0.9615 Val Loss: 0.0705, Val Acc: 0.9278, Val F1: 0.9492\n",
            "Epoch: 21/75, Train Loss: 0.0574, Train Acc: 0.9451, Train F1: 0.9615 Val Loss: 0.0609, Val Acc: 0.9471, Val F1: 0.9634\n",
            "Epoch: 22/75, Train Loss: 0.0523, Train Acc: 0.9507, Train F1: 0.9658 Val Loss: 0.0579, Val Acc: 0.9436, Val F1: 0.9609\n",
            "Epoch: 23/75, Train Loss: 0.0497, Train Acc: 0.9548, Train F1: 0.9685 Val Loss: 0.0607, Val Acc: 0.9364, Val F1: 0.9555\n",
            "Epoch: 24/75, Train Loss: 0.0529, Train Acc: 0.9487, Train F1: 0.9643 Val Loss: 0.0567, Val Acc: 0.9464, Val F1: 0.9636\n",
            "Epoch: 25/75, Train Loss: 0.0528, Train Acc: 0.9513, Train F1: 0.9662 Val Loss: 0.0581, Val Acc: 0.9433, Val F1: 0.9605\n",
            "Epoch: 26/75, Train Loss: 0.0494, Train Acc: 0.9525, Train F1: 0.9670 Val Loss: 0.0545, Val Acc: 0.9536, Val F1: 0.9682\n",
            "Epoch: 27/75, Train Loss: 0.0500, Train Acc: 0.9529, Train F1: 0.9674 Val Loss: 0.0616, Val Acc: 0.9357, Val F1: 0.9547\n",
            "Epoch: 28/75, Train Loss: 0.0498, Train Acc: 0.9510, Train F1: 0.9658 Val Loss: 0.0712, Val Acc: 0.9265, Val F1: 0.9474\n",
            "Epoch: 29/75, Train Loss: 0.0505, Train Acc: 0.9514, Train F1: 0.9660 Val Loss: 0.0580, Val Acc: 0.9457, Val F1: 0.9632\n",
            "Epoch: 30/75, Train Loss: 0.0455, Train Acc: 0.9588, Train F1: 0.9712 Val Loss: 0.0611, Val Acc: 0.9464, Val F1: 0.9637\n",
            "Epoch: 31/75, Train Loss: 0.0466, Train Acc: 0.9557, Train F1: 0.9690 Val Loss: 0.0532, Val Acc: 0.9505, Val F1: 0.9657\n",
            "Epoch: 32/75, Train Loss: 0.0439, Train Acc: 0.9601, Train F1: 0.9723 Val Loss: 0.0504, Val Acc: 0.9598, Val F1: 0.9726\n",
            "Epoch: 33/75, Train Loss: 0.0446, Train Acc: 0.9578, Train F1: 0.9708 Val Loss: 0.0538, Val Acc: 0.9533, Val F1: 0.9681\n",
            "Epoch: 34/75, Train Loss: 0.0452, Train Acc: 0.9580, Train F1: 0.9706 Val Loss: 0.0530, Val Acc: 0.9471, Val F1: 0.9631\n",
            "Epoch: 35/75, Train Loss: 0.0433, Train Acc: 0.9596, Train F1: 0.9718 Val Loss: 0.0542, Val Acc: 0.9536, Val F1: 0.9683\n",
            "Epoch: 36/75, Train Loss: 0.0428, Train Acc: 0.9598, Train F1: 0.9718 Val Loss: 0.0483, Val Acc: 0.9632, Val F1: 0.9746\n",
            "Epoch: 37/75, Train Loss: 0.0415, Train Acc: 0.9609, Train F1: 0.9728 Val Loss: 0.0523, Val Acc: 0.9491, Val F1: 0.9645\n",
            "Epoch: 38/75, Train Loss: 0.0414, Train Acc: 0.9617, Train F1: 0.9732 Val Loss: 0.0482, Val Acc: 0.9543, Val F1: 0.9685\n",
            "Epoch: 39/75, Train Loss: 0.0414, Train Acc: 0.9625, Train F1: 0.9739 Val Loss: 0.0482, Val Acc: 0.9543, Val F1: 0.9685\n",
            "Epoch: 40/75, Train Loss: 0.0421, Train Acc: 0.9615, Train F1: 0.9730 Val Loss: 0.0499, Val Acc: 0.9612, Val F1: 0.9736\n",
            "Epoch: 41/75, Train Loss: 0.0418, Train Acc: 0.9632, Train F1: 0.9743 Val Loss: 0.0506, Val Acc: 0.9564, Val F1: 0.9701\n",
            "Epoch: 42/75, Train Loss: 0.0397, Train Acc: 0.9664, Train F1: 0.9766 Val Loss: 0.0487, Val Acc: 0.9570, Val F1: 0.9708\n",
            "Epoch: 43/75, Train Loss: 0.0402, Train Acc: 0.9620, Train F1: 0.9735 Val Loss: 0.0469, Val Acc: 0.9619, Val F1: 0.9736\n",
            "Epoch: 44/75, Train Loss: 0.0410, Train Acc: 0.9621, Train F1: 0.9735 Val Loss: 0.0469, Val Acc: 0.9612, Val F1: 0.9730\n",
            "Epoch: 45/75, Train Loss: 0.0389, Train Acc: 0.9672, Train F1: 0.9773 Val Loss: 0.0467, Val Acc: 0.9550, Val F1: 0.9689\n",
            "Epoch: 46/75, Train Loss: 0.0379, Train Acc: 0.9676, Train F1: 0.9774 Val Loss: 0.0462, Val Acc: 0.9605, Val F1: 0.9729\n",
            "Epoch: 47/75, Train Loss: 0.0373, Train Acc: 0.9699, Train F1: 0.9791 Val Loss: 0.0475, Val Acc: 0.9570, Val F1: 0.9701\n",
            "Epoch: 48/75, Train Loss: 0.0380, Train Acc: 0.9682, Train F1: 0.9777 Val Loss: 0.0448, Val Acc: 0.9625, Val F1: 0.9743\n",
            "Epoch: 49/75, Train Loss: 0.0374, Train Acc: 0.9672, Train F1: 0.9771 Val Loss: 0.0441, Val Acc: 0.9622, Val F1: 0.9741\n",
            "Epoch: 50/75, Train Loss: 0.0370, Train Acc: 0.9680, Train F1: 0.9776 Val Loss: 0.0500, Val Acc: 0.9581, Val F1: 0.9716\n",
            "Epoch: 51/75, Train Loss: 0.0363, Train Acc: 0.9695, Train F1: 0.9786 Val Loss: 0.0437, Val Acc: 0.9674, Val F1: 0.9776\n",
            "Epoch: 52/75, Train Loss: 0.0350, Train Acc: 0.9711, Train F1: 0.9798 Val Loss: 0.0459, Val Acc: 0.9619, Val F1: 0.9740\n",
            "Epoch: 53/75, Train Loss: 0.0352, Train Acc: 0.9706, Train F1: 0.9795 Val Loss: 0.0495, Val Acc: 0.9598, Val F1: 0.9727\n",
            "Epoch: 54/75, Train Loss: 0.0350, Train Acc: 0.9715, Train F1: 0.9800 Val Loss: 0.0483, Val Acc: 0.9546, Val F1: 0.9682\n",
            "Epoch: 55/75, Train Loss: 0.0361, Train Acc: 0.9690, Train F1: 0.9782 Val Loss: 0.0453, Val Acc: 0.9577, Val F1: 0.9705\n",
            "Epoch: 56/75, Train Loss: 0.0352, Train Acc: 0.9717, Train F1: 0.9803 Val Loss: 0.0441, Val Acc: 0.9687, Val F1: 0.9786\n",
            "Epoch: 57/75, Train Loss: 0.0341, Train Acc: 0.9733, Train F1: 0.9815 Val Loss: 0.0431, Val Acc: 0.9656, Val F1: 0.9762\n",
            "Epoch: 58/75, Train Loss: 0.0339, Train Acc: 0.9722, Train F1: 0.9806 Val Loss: 0.0430, Val Acc: 0.9632, Val F1: 0.9745\n",
            "Epoch: 59/75, Train Loss: 0.0342, Train Acc: 0.9714, Train F1: 0.9801 Val Loss: 0.0434, Val Acc: 0.9667, Val F1: 0.9772\n",
            "Epoch: 60/75, Train Loss: 0.0340, Train Acc: 0.9722, Train F1: 0.9807 Val Loss: 0.0425, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 61/75, Train Loss: 0.0333, Train Acc: 0.9729, Train F1: 0.9810 Val Loss: 0.0421, Val Acc: 0.9663, Val F1: 0.9768\n",
            "Epoch: 62/75, Train Loss: 0.0329, Train Acc: 0.9739, Train F1: 0.9817 Val Loss: 0.0422, Val Acc: 0.9680, Val F1: 0.9781\n",
            "Epoch: 63/75, Train Loss: 0.0326, Train Acc: 0.9747, Train F1: 0.9823 Val Loss: 0.0418, Val Acc: 0.9674, Val F1: 0.9775\n",
            "Epoch: 64/75, Train Loss: 0.0326, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0416, Val Acc: 0.9711, Val F1: 0.9802\n",
            "Epoch: 65/75, Train Loss: 0.0323, Train Acc: 0.9732, Train F1: 0.9812 Val Loss: 0.0414, Val Acc: 0.9704, Val F1: 0.9797\n",
            "Epoch: 66/75, Train Loss: 0.0321, Train Acc: 0.9763, Train F1: 0.9835 Val Loss: 0.0420, Val Acc: 0.9691, Val F1: 0.9788\n",
            "Epoch: 67/75, Train Loss: 0.0320, Train Acc: 0.9755, Train F1: 0.9828 Val Loss: 0.0416, Val Acc: 0.9718, Val F1: 0.9806\n",
            "Epoch: 68/75, Train Loss: 0.0319, Train Acc: 0.9762, Train F1: 0.9834 Val Loss: 0.0417, Val Acc: 0.9701, Val F1: 0.9794\n",
            "Epoch: 69/75, Train Loss: 0.0317, Train Acc: 0.9753, Train F1: 0.9827 Val Loss: 0.0414, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 70/75, Train Loss: 0.0318, Train Acc: 0.9767, Train F1: 0.9837 Val Loss: 0.0410, Val Acc: 0.9698, Val F1: 0.9792\n",
            "Epoch: 71/75, Train Loss: 0.0317, Train Acc: 0.9766, Train F1: 0.9837 Val Loss: 0.0412, Val Acc: 0.9680, Val F1: 0.9781\n",
            "Epoch: 72/75, Train Loss: 0.0315, Train Acc: 0.9758, Train F1: 0.9830 Val Loss: 0.0411, Val Acc: 0.9701, Val F1: 0.9794\n",
            "Epoch: 73/75, Train Loss: 0.0315, Train Acc: 0.9765, Train F1: 0.9836 Val Loss: 0.0411, Val Acc: 0.9701, Val F1: 0.9794\n",
            "Epoch: 74/75, Train Loss: 0.0314, Train Acc: 0.9767, Train F1: 0.9838 Val Loss: 0.0410, Val Acc: 0.9701, Val F1: 0.9794\n",
            "Epoch: 75/75, Train Loss: 0.0314, Train Acc: 0.9765, Train F1: 0.9836 Val Loss: 0.0410, Val Acc: 0.9701, Val F1: 0.9794\n",
            "\n",
            " 🔎 search 1 : deep_rescnn --- lr : 0.0008753516269381944, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv3.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1088, Train Acc: 0.8844, Train F1: 0.9192 Val Loss: 0.0757, Val Acc: 0.9351, Val F1: 0.9548\n",
            "Epoch: 2/75, Train Loss: 0.0719, Train Acc: 0.9296, Train F1: 0.9511 Val Loss: 0.0646, Val Acc: 0.9485, Val F1: 0.9647\n",
            "Epoch: 3/75, Train Loss: 0.0595, Train Acc: 0.9416, Train F1: 0.9590 Val Loss: 0.0845, Val Acc: 0.9072, Val F1: 0.9392\n",
            "Epoch: 4/75, Train Loss: 0.0538, Train Acc: 0.9518, Train F1: 0.9665 Val Loss: 0.0561, Val Acc: 0.9536, Val F1: 0.9675\n",
            "Epoch: 5/75, Train Loss: 0.0544, Train Acc: 0.9471, Train F1: 0.9631 Val Loss: 0.0621, Val Acc: 0.9467, Val F1: 0.9641\n",
            "Epoch: 6/75, Train Loss: 0.0513, Train Acc: 0.9494, Train F1: 0.9644 Val Loss: 0.0555, Val Acc: 0.9536, Val F1: 0.9674\n",
            "Epoch: 7/75, Train Loss: 0.0479, Train Acc: 0.9533, Train F1: 0.9671 Val Loss: 0.0527, Val Acc: 0.9502, Val F1: 0.9650\n",
            "Epoch: 8/75, Train Loss: 0.0454, Train Acc: 0.9588, Train F1: 0.9712 Val Loss: 0.0475, Val Acc: 0.9588, Val F1: 0.9717\n",
            "Epoch: 9/75, Train Loss: 0.0426, Train Acc: 0.9613, Train F1: 0.9729 Val Loss: 0.0488, Val Acc: 0.9584, Val F1: 0.9716\n",
            "Epoch: 10/75, Train Loss: 0.0416, Train Acc: 0.9633, Train F1: 0.9744 Val Loss: 0.0509, Val Acc: 0.9608, Val F1: 0.9735\n",
            "Epoch: 11/75, Train Loss: 0.0405, Train Acc: 0.9639, Train F1: 0.9748 Val Loss: 0.0423, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 12/75, Train Loss: 0.0390, Train Acc: 0.9663, Train F1: 0.9765 Val Loss: 0.0523, Val Acc: 0.9584, Val F1: 0.9715\n",
            "Epoch: 13/75, Train Loss: 0.0402, Train Acc: 0.9653, Train F1: 0.9757 Val Loss: 0.0572, Val Acc: 0.9440, Val F1: 0.9622\n",
            "Epoch: 14/75, Train Loss: 0.0433, Train Acc: 0.9611, Train F1: 0.9728 Val Loss: 0.0427, Val Acc: 0.9646, Val F1: 0.9755\n",
            "Epoch: 15/75, Train Loss: 0.0394, Train Acc: 0.9659, Train F1: 0.9761 Val Loss: 0.0505, Val Acc: 0.9612, Val F1: 0.9736\n",
            "Epoch: 16/75, Train Loss: 0.0367, Train Acc: 0.9684, Train F1: 0.9779 Val Loss: 0.0453, Val Acc: 0.9595, Val F1: 0.9716\n",
            "Epoch: 17/75, Train Loss: 0.0392, Train Acc: 0.9652, Train F1: 0.9754 Val Loss: 0.0455, Val Acc: 0.9567, Val F1: 0.9696\n",
            "Epoch: 18/75, Train Loss: 0.0392, Train Acc: 0.9644, Train F1: 0.9749 Val Loss: 0.0449, Val Acc: 0.9660, Val F1: 0.9768\n",
            "Epoch: 19/75, Train Loss: 0.0364, Train Acc: 0.9696, Train F1: 0.9788 Val Loss: 0.0482, Val Acc: 0.9588, Val F1: 0.9719\n",
            "Epoch: 20/75, Train Loss: 0.0387, Train Acc: 0.9655, Train F1: 0.9760 Val Loss: 0.0620, Val Acc: 0.9330, Val F1: 0.9519\n",
            "Epoch: 21/75, Train Loss: 0.0379, Train Acc: 0.9677, Train F1: 0.9775 Val Loss: 0.0415, Val Acc: 0.9670, Val F1: 0.9773\n",
            "Epoch: 22/75, Train Loss: 0.0357, Train Acc: 0.9696, Train F1: 0.9788 Val Loss: 0.0433, Val Acc: 0.9687, Val F1: 0.9787\n",
            "Epoch: 23/75, Train Loss: 0.0350, Train Acc: 0.9695, Train F1: 0.9788 Val Loss: 0.0621, Val Acc: 0.9320, Val F1: 0.9547\n",
            "Epoch: 24/75, Train Loss: 0.0350, Train Acc: 0.9696, Train F1: 0.9788 Val Loss: 0.0375, Val Acc: 0.9701, Val F1: 0.9794\n",
            "Epoch: 25/75, Train Loss: 0.0364, Train Acc: 0.9680, Train F1: 0.9776 Val Loss: 0.0592, Val Acc: 0.9392, Val F1: 0.9565\n",
            "Epoch: 26/75, Train Loss: 0.0353, Train Acc: 0.9678, Train F1: 0.9773 Val Loss: 0.0440, Val Acc: 0.9608, Val F1: 0.9725\n",
            "Epoch: 27/75, Train Loss: 0.0327, Train Acc: 0.9725, Train F1: 0.9805 Val Loss: 0.0371, Val Acc: 0.9674, Val F1: 0.9773\n",
            "Epoch: 28/75, Train Loss: 0.0337, Train Acc: 0.9717, Train F1: 0.9802 Val Loss: 0.0547, Val Acc: 0.9409, Val F1: 0.9578\n",
            "Epoch: 29/75, Train Loss: 0.0352, Train Acc: 0.9687, Train F1: 0.9782 Val Loss: 0.0381, Val Acc: 0.9694, Val F1: 0.9790\n",
            "Epoch: 30/75, Train Loss: 0.0321, Train Acc: 0.9741, Train F1: 0.9818 Val Loss: 0.0407, Val Acc: 0.9656, Val F1: 0.9760\n",
            "Epoch: 31/75, Train Loss: 0.0328, Train Acc: 0.9731, Train F1: 0.9814 Val Loss: 0.0407, Val Acc: 0.9684, Val F1: 0.9783\n",
            "Epoch: 32/75, Train Loss: 0.0359, Train Acc: 0.9692, Train F1: 0.9783 Val Loss: 0.0400, Val Acc: 0.9684, Val F1: 0.9783\n",
            "Epoch: 33/75, Train Loss: 0.0313, Train Acc: 0.9759, Train F1: 0.9833 Val Loss: 0.0365, Val Acc: 0.9715, Val F1: 0.9805\n",
            "Epoch: 34/75, Train Loss: 0.0298, Train Acc: 0.9788, Train F1: 0.9852 Val Loss: 0.0434, Val Acc: 0.9560, Val F1: 0.9690\n",
            "Epoch: 35/75, Train Loss: 0.0312, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0347, Val Acc: 0.9711, Val F1: 0.9800\n",
            "Epoch: 36/75, Train Loss: 0.0312, Train Acc: 0.9754, Train F1: 0.9827 Val Loss: 0.0463, Val Acc: 0.9515, Val F1: 0.9657\n",
            "Epoch: 37/75, Train Loss: 0.0314, Train Acc: 0.9742, Train F1: 0.9818 Val Loss: 0.0435, Val Acc: 0.9567, Val F1: 0.9695\n",
            "Epoch: 38/75, Train Loss: 0.0303, Train Acc: 0.9755, Train F1: 0.9828 Val Loss: 0.0366, Val Acc: 0.9656, Val F1: 0.9760\n",
            "Epoch: 39/75, Train Loss: 0.0306, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0344, Val Acc: 0.9711, Val F1: 0.9801\n",
            "Epoch: 40/75, Train Loss: 0.0288, Train Acc: 0.9784, Train F1: 0.9849 Val Loss: 0.0343, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 41/75, Train Loss: 0.0284, Train Acc: 0.9785, Train F1: 0.9849 Val Loss: 0.0372, Val Acc: 0.9639, Val F1: 0.9748\n",
            "Epoch: 42/75, Train Loss: 0.0304, Train Acc: 0.9756, Train F1: 0.9831 Val Loss: 0.0356, Val Acc: 0.9687, Val F1: 0.9783\n",
            "Epoch: 43/75, Train Loss: 0.0274, Train Acc: 0.9822, Train F1: 0.9876 Val Loss: 0.0375, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 44/75, Train Loss: 0.0290, Train Acc: 0.9771, Train F1: 0.9841 Val Loss: 0.0393, Val Acc: 0.9684, Val F1: 0.9784\n",
            "Epoch: 45/75, Train Loss: 0.0277, Train Acc: 0.9801, Train F1: 0.9861 Val Loss: 0.0360, Val Acc: 0.9684, Val F1: 0.9779\n",
            "Epoch: 46/75, Train Loss: 0.0273, Train Acc: 0.9804, Train F1: 0.9863 Val Loss: 0.0329, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 47/75, Train Loss: 0.0268, Train Acc: 0.9817, Train F1: 0.9872 Val Loss: 0.0380, Val Acc: 0.9622, Val F1: 0.9735\n",
            "Epoch: 48/75, Train Loss: 0.0283, Train Acc: 0.9782, Train F1: 0.9846 Val Loss: 0.0343, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 49/75, Train Loss: 0.0277, Train Acc: 0.9801, Train F1: 0.9859 Val Loss: 0.0337, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 50/75, Train Loss: 0.0260, Train Acc: 0.9826, Train F1: 0.9878 Val Loss: 0.0348, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 51/75, Train Loss: 0.0263, Train Acc: 0.9816, Train F1: 0.9871 Val Loss: 0.0355, Val Acc: 0.9708, Val F1: 0.9800\n",
            "Epoch: 52/75, Train Loss: 0.0262, Train Acc: 0.9821, Train F1: 0.9875 Val Loss: 0.0331, Val Acc: 0.9749, Val F1: 0.9826\n",
            "Epoch: 53/75, Train Loss: 0.0260, Train Acc: 0.9825, Train F1: 0.9877 Val Loss: 0.0356, Val Acc: 0.9663, Val F1: 0.9765\n",
            "Epoch: 54/75, Train Loss: 0.0258, Train Acc: 0.9820, Train F1: 0.9874 Val Loss: 0.0340, Val Acc: 0.9698, Val F1: 0.9790\n",
            "Epoch: 55/75, Train Loss: 0.0252, Train Acc: 0.9822, Train F1: 0.9876 Val Loss: 0.0324, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 56/75, Train Loss: 0.0246, Train Acc: 0.9845, Train F1: 0.9893 Val Loss: 0.0316, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 57/75, Train Loss: 0.0244, Train Acc: 0.9853, Train F1: 0.9897 Val Loss: 0.0328, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 58/75, Train Loss: 0.0246, Train Acc: 0.9844, Train F1: 0.9892 Val Loss: 0.0317, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 59/75, Train Loss: 0.0239, Train Acc: 0.9843, Train F1: 0.9890 Val Loss: 0.0347, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 60/75, Train Loss: 0.0242, Train Acc: 0.9855, Train F1: 0.9898 Val Loss: 0.0312, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 61/75, Train Loss: 0.0239, Train Acc: 0.9843, Train F1: 0.9891 Val Loss: 0.0320, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 62/75, Train Loss: 0.0237, Train Acc: 0.9857, Train F1: 0.9900 Val Loss: 0.0326, Val Acc: 0.9725, Val F1: 0.9809\n",
            "Epoch: 63/75, Train Loss: 0.0239, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0312, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 64/75, Train Loss: 0.0234, Train Acc: 0.9861, Train F1: 0.9903 Val Loss: 0.0315, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 65/75, Train Loss: 0.0235, Train Acc: 0.9847, Train F1: 0.9891 Val Loss: 0.0312, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 66/75, Train Loss: 0.0231, Train Acc: 0.9865, Train F1: 0.9905 Val Loss: 0.0316, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 67/75, Train Loss: 0.0231, Train Acc: 0.9863, Train F1: 0.9904 Val Loss: 0.0310, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 68/75, Train Loss: 0.0229, Train Acc: 0.9863, Train F1: 0.9903 Val Loss: 0.0312, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 69/75, Train Loss: 0.0229, Train Acc: 0.9855, Train F1: 0.9897 Val Loss: 0.0312, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 70/75, Train Loss: 0.0228, Train Acc: 0.9868, Train F1: 0.9909 Val Loss: 0.0310, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 71/75, Train Loss: 0.0228, Train Acc: 0.9868, Train F1: 0.9908 Val Loss: 0.0310, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 72/75, Train Loss: 0.0227, Train Acc: 0.9873, Train F1: 0.9911 Val Loss: 0.0309, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 73/75, Train Loss: 0.0226, Train Acc: 0.9867, Train F1: 0.9909 Val Loss: 0.0310, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 74/75, Train Loss: 0.0226, Train Acc: 0.9865, Train F1: 0.9906 Val Loss: 0.0309, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 75/75, Train Loss: 0.0226, Train Acc: 0.9865, Train F1: 0.9905 Val Loss: 0.0310, Val Acc: 0.9773, Val F1: 0.9843\n",
            "\n",
            " 🔎 search 2 : deep_rescnn --- lr : 0.0008753516269381944, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv4.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1154, Train Acc: 0.8759, Train F1: 0.9166 Val Loss: 0.0879, Val Acc: 0.9210, Val F1: 0.9454\n",
            "Epoch: 2/75, Train Loss: 0.0809, Train Acc: 0.9186, Train F1: 0.9429 Val Loss: 0.0700, Val Acc: 0.9450, Val F1: 0.9620\n",
            "Epoch: 3/75, Train Loss: 0.0645, Train Acc: 0.9380, Train F1: 0.9567 Val Loss: 0.0682, Val Acc: 0.9337, Val F1: 0.9530\n",
            "Epoch: 4/75, Train Loss: 0.0592, Train Acc: 0.9419, Train F1: 0.9593 Val Loss: 0.0580, Val Acc: 0.9495, Val F1: 0.9655\n",
            "Epoch: 5/75, Train Loss: 0.0548, Train Acc: 0.9485, Train F1: 0.9640 Val Loss: 0.0559, Val Acc: 0.9440, Val F1: 0.9608\n",
            "Epoch: 6/75, Train Loss: 0.0471, Train Acc: 0.9566, Train F1: 0.9697 Val Loss: 0.0558, Val Acc: 0.9409, Val F1: 0.9583\n",
            "Epoch: 7/75, Train Loss: 0.0455, Train Acc: 0.9578, Train F1: 0.9705 Val Loss: 0.0454, Val Acc: 0.9636, Val F1: 0.9748\n",
            "Epoch: 8/75, Train Loss: 0.0428, Train Acc: 0.9616, Train F1: 0.9735 Val Loss: 0.0447, Val Acc: 0.9619, Val F1: 0.9738\n",
            "Epoch: 9/75, Train Loss: 0.0410, Train Acc: 0.9645, Train F1: 0.9751 Val Loss: 0.0454, Val Acc: 0.9522, Val F1: 0.9666\n",
            "Epoch: 10/75, Train Loss: 0.0367, Train Acc: 0.9694, Train F1: 0.9786 Val Loss: 0.0540, Val Acc: 0.9474, Val F1: 0.9647\n",
            "Epoch: 11/75, Train Loss: 0.0381, Train Acc: 0.9656, Train F1: 0.9761 Val Loss: 0.0638, Val Acc: 0.9375, Val F1: 0.9583\n",
            "Epoch: 12/75, Train Loss: 0.0347, Train Acc: 0.9730, Train F1: 0.9812 Val Loss: 0.0388, Val Acc: 0.9656, Val F1: 0.9761\n",
            "Epoch: 13/75, Train Loss: 0.0355, Train Acc: 0.9664, Train F1: 0.9765 Val Loss: 0.0388, Val Acc: 0.9722, Val F1: 0.9806\n",
            "Epoch: 14/75, Train Loss: 0.0327, Train Acc: 0.9733, Train F1: 0.9814 Val Loss: 0.0398, Val Acc: 0.9660, Val F1: 0.9768\n",
            "Epoch: 15/75, Train Loss: 0.0331, Train Acc: 0.9735, Train F1: 0.9816 Val Loss: 0.0362, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 16/75, Train Loss: 0.0348, Train Acc: 0.9704, Train F1: 0.9795 Val Loss: 0.0394, Val Acc: 0.9698, Val F1: 0.9793\n",
            "Epoch: 17/75, Train Loss: 0.0296, Train Acc: 0.9779, Train F1: 0.9846 Val Loss: 0.0448, Val Acc: 0.9619, Val F1: 0.9740\n",
            "Epoch: 18/75, Train Loss: 0.0308, Train Acc: 0.9762, Train F1: 0.9834 Val Loss: 0.0585, Val Acc: 0.9402, Val F1: 0.9571\n",
            "Epoch: 19/75, Train Loss: 0.0308, Train Acc: 0.9755, Train F1: 0.9829 Val Loss: 0.0349, Val Acc: 0.9680, Val F1: 0.9779\n",
            "Epoch: 20/75, Train Loss: 0.0299, Train Acc: 0.9771, Train F1: 0.9841 Val Loss: 0.0485, Val Acc: 0.9519, Val F1: 0.9658\n",
            "Epoch: 21/75, Train Loss: 0.0295, Train Acc: 0.9773, Train F1: 0.9843 Val Loss: 0.0317, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 22/75, Train Loss: 0.0273, Train Acc: 0.9796, Train F1: 0.9859 Val Loss: 0.0335, Val Acc: 0.9677, Val F1: 0.9776\n",
            "Epoch: 23/75, Train Loss: 0.0285, Train Acc: 0.9766, Train F1: 0.9837 Val Loss: 0.0413, Val Acc: 0.9667, Val F1: 0.9773\n",
            "Epoch: 24/75, Train Loss: 0.0285, Train Acc: 0.9759, Train F1: 0.9833 Val Loss: 0.0366, Val Acc: 0.9701, Val F1: 0.9796\n",
            "Epoch: 25/75, Train Loss: 0.0266, Train Acc: 0.9834, Train F1: 0.9883 Val Loss: 0.0312, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 26/75, Train Loss: 0.0274, Train Acc: 0.9806, Train F1: 0.9865 Val Loss: 0.0431, Val Acc: 0.9564, Val F1: 0.9692\n",
            "Epoch: 27/75, Train Loss: 0.0278, Train Acc: 0.9792, Train F1: 0.9854 Val Loss: 0.0303, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 28/75, Train Loss: 0.0279, Train Acc: 0.9800, Train F1: 0.9860 Val Loss: 0.0295, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 29/75, Train Loss: 0.0271, Train Acc: 0.9806, Train F1: 0.9864 Val Loss: 0.0308, Val Acc: 0.9735, Val F1: 0.9816\n",
            "Epoch: 30/75, Train Loss: 0.0244, Train Acc: 0.9824, Train F1: 0.9876 Val Loss: 0.0299, Val Acc: 0.9770, Val F1: 0.9840\n",
            "Epoch: 31/75, Train Loss: 0.0256, Train Acc: 0.9827, Train F1: 0.9879 Val Loss: 0.0328, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 32/75, Train Loss: 0.0265, Train Acc: 0.9806, Train F1: 0.9864 Val Loss: 0.0486, Val Acc: 0.9515, Val F1: 0.9655\n",
            "Epoch: 33/75, Train Loss: 0.0261, Train Acc: 0.9805, Train F1: 0.9864 Val Loss: 0.0351, Val Acc: 0.9694, Val F1: 0.9791\n",
            "Epoch: 34/75, Train Loss: 0.0255, Train Acc: 0.9824, Train F1: 0.9878 Val Loss: 0.0336, Val Acc: 0.9739, Val F1: 0.9821\n",
            "Epoch: 35/75, Train Loss: 0.0255, Train Acc: 0.9830, Train F1: 0.9881 Val Loss: 0.0331, Val Acc: 0.9732, Val F1: 0.9817\n",
            "Epoch: 36/75, Train Loss: 0.0246, Train Acc: 0.9825, Train F1: 0.9877 Val Loss: 0.0308, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 37/75, Train Loss: 0.0232, Train Acc: 0.9842, Train F1: 0.9890 Val Loss: 0.0314, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 38/75, Train Loss: 0.0246, Train Acc: 0.9835, Train F1: 0.9885 Val Loss: 0.0315, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 39/75, Train Loss: 0.0258, Train Acc: 0.9808, Train F1: 0.9865 Val Loss: 0.0315, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 40/75, Train Loss: 0.0245, Train Acc: 0.9825, Train F1: 0.9878 Val Loss: 0.0292, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 41/75, Train Loss: 0.0226, Train Acc: 0.9864, Train F1: 0.9905 Val Loss: 0.0290, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 42/75, Train Loss: 0.0226, Train Acc: 0.9851, Train F1: 0.9897 Val Loss: 0.0320, Val Acc: 0.9739, Val F1: 0.9821\n",
            "Epoch: 43/75, Train Loss: 0.0230, Train Acc: 0.9849, Train F1: 0.9894 Val Loss: 0.0280, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 44/75, Train Loss: 0.0219, Train Acc: 0.9875, Train F1: 0.9913 Val Loss: 0.0300, Val Acc: 0.9749, Val F1: 0.9826\n",
            "Epoch: 45/75, Train Loss: 0.0232, Train Acc: 0.9844, Train F1: 0.9889 Val Loss: 0.0287, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 46/75, Train Loss: 0.0233, Train Acc: 0.9838, Train F1: 0.9887 Val Loss: 0.0291, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 47/75, Train Loss: 0.0226, Train Acc: 0.9850, Train F1: 0.9895 Val Loss: 0.0291, Val Acc: 0.9770, Val F1: 0.9840\n",
            "Epoch: 48/75, Train Loss: 0.0211, Train Acc: 0.9869, Train F1: 0.9909 Val Loss: 0.0302, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 49/75, Train Loss: 0.0221, Train Acc: 0.9856, Train F1: 0.9899 Val Loss: 0.0285, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 50/75, Train Loss: 0.0217, Train Acc: 0.9875, Train F1: 0.9912 Val Loss: 0.0297, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 51/75, Train Loss: 0.0214, Train Acc: 0.9856, Train F1: 0.9899 Val Loss: 0.0278, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 52/75, Train Loss: 0.0221, Train Acc: 0.9864, Train F1: 0.9905 Val Loss: 0.0272, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 53/75, Train Loss: 0.0213, Train Acc: 0.9865, Train F1: 0.9904 Val Loss: 0.0301, Val Acc: 0.9753, Val F1: 0.9828\n",
            "Epoch: 54/75, Train Loss: 0.0203, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0272, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 55/75, Train Loss: 0.0203, Train Acc: 0.9893, Train F1: 0.9925 Val Loss: 0.0274, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 56/75, Train Loss: 0.0204, Train Acc: 0.9868, Train F1: 0.9907 Val Loss: 0.0273, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 57/75, Train Loss: 0.0199, Train Acc: 0.9896, Train F1: 0.9927 Val Loss: 0.0294, Val Acc: 0.9739, Val F1: 0.9818\n",
            "Epoch: 58/75, Train Loss: 0.0199, Train Acc: 0.9891, Train F1: 0.9924 Val Loss: 0.0272, Val Acc: 0.9797, Val F1: 0.9861\n",
            "Epoch: 59/75, Train Loss: 0.0193, Train Acc: 0.9896, Train F1: 0.9926 Val Loss: 0.0275, Val Acc: 0.9780, Val F1: 0.9847\n",
            "Epoch: 60/75, Train Loss: 0.0198, Train Acc: 0.9888, Train F1: 0.9922 Val Loss: 0.0268, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 61/75, Train Loss: 0.0195, Train Acc: 0.9901, Train F1: 0.9931 Val Loss: 0.0268, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 62/75, Train Loss: 0.0193, Train Acc: 0.9897, Train F1: 0.9928 Val Loss: 0.0272, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 63/75, Train Loss: 0.0192, Train Acc: 0.9892, Train F1: 0.9924 Val Loss: 0.0277, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 64/75, Train Loss: 0.0188, Train Acc: 0.9904, Train F1: 0.9932 Val Loss: 0.0272, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 65/75, Train Loss: 0.0192, Train Acc: 0.9897, Train F1: 0.9927 Val Loss: 0.0266, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 66/75, Train Loss: 0.0188, Train Acc: 0.9900, Train F1: 0.9931 Val Loss: 0.0277, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 67/75, Train Loss: 0.0188, Train Acc: 0.9908, Train F1: 0.9936 Val Loss: 0.0268, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 68/75, Train Loss: 0.0189, Train Acc: 0.9897, Train F1: 0.9928 Val Loss: 0.0274, Val Acc: 0.9780, Val F1: 0.9847\n",
            "Epoch: 69/75, Train Loss: 0.0187, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0267, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 70/75, Train Loss: 0.0186, Train Acc: 0.9904, Train F1: 0.9932 Val Loss: 0.0266, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 71/75, Train Loss: 0.0185, Train Acc: 0.9910, Train F1: 0.9937 Val Loss: 0.0266, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 72/75, Train Loss: 0.0185, Train Acc: 0.9907, Train F1: 0.9934 Val Loss: 0.0266, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 73/75, Train Loss: 0.0184, Train Acc: 0.9911, Train F1: 0.9937 Val Loss: 0.0266, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 74/75, Train Loss: 0.0184, Train Acc: 0.9907, Train F1: 0.9935 Val Loss: 0.0266, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 75/75, Train Loss: 0.0184, Train Acc: 0.9910, Train F1: 0.9937 Val Loss: 0.0266, Val Acc: 0.9794, Val F1: 0.9857\n",
            "\n",
            " 🔎 search 3 : deep_rescnn --- lr : 0.0008753516269381944, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv5.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1385, Train Acc: 0.8354, Train F1: 0.8817 Val Loss: 0.0940, Val Acc: 0.9034, Val F1: 0.9340\n",
            "Epoch: 2/75, Train Loss: 0.0795, Train Acc: 0.9227, Train F1: 0.9460 Val Loss: 0.0757, Val Acc: 0.9265, Val F1: 0.9500\n",
            "Epoch: 3/75, Train Loss: 0.0672, Train Acc: 0.9332, Train F1: 0.9530 Val Loss: 0.0744, Val Acc: 0.9285, Val F1: 0.9522\n",
            "Epoch: 4/75, Train Loss: 0.0567, Train Acc: 0.9444, Train F1: 0.9615 Val Loss: 0.0548, Val Acc: 0.9526, Val F1: 0.9671\n",
            "Epoch: 5/75, Train Loss: 0.0509, Train Acc: 0.9532, Train F1: 0.9671 Val Loss: 0.0777, Val Acc: 0.9120, Val F1: 0.9423\n",
            "Epoch: 6/75, Train Loss: 0.0522, Train Acc: 0.9486, Train F1: 0.9641 Val Loss: 0.0546, Val Acc: 0.9574, Val F1: 0.9711\n",
            "Epoch: 7/75, Train Loss: 0.0455, Train Acc: 0.9583, Train F1: 0.9708 Val Loss: 0.0455, Val Acc: 0.9660, Val F1: 0.9765\n",
            "Epoch: 8/75, Train Loss: 0.0398, Train Acc: 0.9675, Train F1: 0.9774 Val Loss: 0.0450, Val Acc: 0.9625, Val F1: 0.9740\n",
            "Epoch: 9/75, Train Loss: 0.0379, Train Acc: 0.9677, Train F1: 0.9778 Val Loss: 0.0423, Val Acc: 0.9663, Val F1: 0.9768\n",
            "Epoch: 10/75, Train Loss: 0.0378, Train Acc: 0.9668, Train F1: 0.9768 Val Loss: 0.0533, Val Acc: 0.9464, Val F1: 0.9620\n",
            "Epoch: 11/75, Train Loss: 0.0357, Train Acc: 0.9699, Train F1: 0.9789 Val Loss: 0.0512, Val Acc: 0.9495, Val F1: 0.9642\n",
            "Epoch: 12/75, Train Loss: 0.0339, Train Acc: 0.9710, Train F1: 0.9797 Val Loss: 0.0398, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 13/75, Train Loss: 0.0313, Train Acc: 0.9743, Train F1: 0.9820 Val Loss: 0.0496, Val Acc: 0.9498, Val F1: 0.9644\n",
            "Epoch: 14/75, Train Loss: 0.0339, Train Acc: 0.9721, Train F1: 0.9805 Val Loss: 0.0372, Val Acc: 0.9722, Val F1: 0.9807\n",
            "Epoch: 15/75, Train Loss: 0.0294, Train Acc: 0.9766, Train F1: 0.9837 Val Loss: 0.0462, Val Acc: 0.9646, Val F1: 0.9760\n",
            "Epoch: 16/75, Train Loss: 0.0334, Train Acc: 0.9730, Train F1: 0.9811 Val Loss: 0.0350, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 17/75, Train Loss: 0.0297, Train Acc: 0.9774, Train F1: 0.9841 Val Loss: 0.0348, Val Acc: 0.9759, Val F1: 0.9835\n",
            "Epoch: 18/75, Train Loss: 0.0288, Train Acc: 0.9769, Train F1: 0.9839 Val Loss: 0.0376, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 19/75, Train Loss: 0.0289, Train Acc: 0.9773, Train F1: 0.9842 Val Loss: 0.0347, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 20/75, Train Loss: 0.0268, Train Acc: 0.9808, Train F1: 0.9867 Val Loss: 0.0338, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 21/75, Train Loss: 0.0273, Train Acc: 0.9797, Train F1: 0.9857 Val Loss: 0.0634, Val Acc: 0.9371, Val F1: 0.9582\n",
            "Epoch: 22/75, Train Loss: 0.0292, Train Acc: 0.9758, Train F1: 0.9833 Val Loss: 0.0386, Val Acc: 0.9667, Val F1: 0.9767\n",
            "Epoch: 23/75, Train Loss: 0.0270, Train Acc: 0.9785, Train F1: 0.9849 Val Loss: 0.0347, Val Acc: 0.9732, Val F1: 0.9813\n",
            "Epoch: 24/75, Train Loss: 0.0280, Train Acc: 0.9766, Train F1: 0.9837 Val Loss: 0.0329, Val Acc: 0.9749, Val F1: 0.9826\n",
            "Epoch: 25/75, Train Loss: 0.0241, Train Acc: 0.9829, Train F1: 0.9881 Val Loss: 0.0354, Val Acc: 0.9735, Val F1: 0.9816\n",
            "Epoch: 26/75, Train Loss: 0.0259, Train Acc: 0.9800, Train F1: 0.9860 Val Loss: 0.0337, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 27/75, Train Loss: 0.0253, Train Acc: 0.9813, Train F1: 0.9869 Val Loss: 0.0393, Val Acc: 0.9708, Val F1: 0.9801\n",
            "Epoch: 28/75, Train Loss: 0.0266, Train Acc: 0.9792, Train F1: 0.9854 Val Loss: 0.0320, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 29/75, Train Loss: 0.0282, Train Acc: 0.9764, Train F1: 0.9834 Val Loss: 0.0312, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 30/75, Train Loss: 0.0242, Train Acc: 0.9848, Train F1: 0.9894 Val Loss: 0.0325, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 31/75, Train Loss: 0.0241, Train Acc: 0.9828, Train F1: 0.9880 Val Loss: 0.0389, Val Acc: 0.9725, Val F1: 0.9812\n",
            "Epoch: 32/75, Train Loss: 0.0241, Train Acc: 0.9828, Train F1: 0.9880 Val Loss: 0.0353, Val Acc: 0.9746, Val F1: 0.9826\n",
            "Epoch: 33/75, Train Loss: 0.0237, Train Acc: 0.9828, Train F1: 0.9878 Val Loss: 0.0319, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 34/75, Train Loss: 0.0234, Train Acc: 0.9845, Train F1: 0.9891 Val Loss: 0.0331, Val Acc: 0.9742, Val F1: 0.9821\n",
            "Epoch: 35/75, Train Loss: 0.0225, Train Acc: 0.9858, Train F1: 0.9901 Val Loss: 0.0316, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 36/75, Train Loss: 0.0230, Train Acc: 0.9850, Train F1: 0.9895 Val Loss: 0.0310, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 37/75, Train Loss: 0.0224, Train Acc: 0.9840, Train F1: 0.9888 Val Loss: 0.0331, Val Acc: 0.9729, Val F1: 0.9812\n",
            "Epoch: 38/75, Train Loss: 0.0222, Train Acc: 0.9868, Train F1: 0.9909 Val Loss: 0.0351, Val Acc: 0.9704, Val F1: 0.9794\n",
            "Epoch: 39/75, Train Loss: 0.0221, Train Acc: 0.9851, Train F1: 0.9896 Val Loss: 0.0307, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 40/75, Train Loss: 0.0227, Train Acc: 0.9853, Train F1: 0.9897 Val Loss: 0.0309, Val Acc: 0.9780, Val F1: 0.9849\n",
            "Epoch: 41/75, Train Loss: 0.0226, Train Acc: 0.9852, Train F1: 0.9897 Val Loss: 0.0335, Val Acc: 0.9711, Val F1: 0.9799\n",
            "Epoch: 42/75, Train Loss: 0.0216, Train Acc: 0.9858, Train F1: 0.9900 Val Loss: 0.0341, Val Acc: 0.9704, Val F1: 0.9793\n",
            "Epoch: 43/75, Train Loss: 0.0216, Train Acc: 0.9850, Train F1: 0.9894 Val Loss: 0.0344, Val Acc: 0.9711, Val F1: 0.9798\n",
            "Epoch: 44/75, Train Loss: 0.0219, Train Acc: 0.9860, Train F1: 0.9902 Val Loss: 0.0301, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 45/75, Train Loss: 0.0226, Train Acc: 0.9850, Train F1: 0.9896 Val Loss: 0.0297, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 46/75, Train Loss: 0.0213, Train Acc: 0.9858, Train F1: 0.9901 Val Loss: 0.0301, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 47/75, Train Loss: 0.0210, Train Acc: 0.9880, Train F1: 0.9916 Val Loss: 0.0296, Val Acc: 0.9770, Val F1: 0.9840\n",
            "Epoch: 48/75, Train Loss: 0.0203, Train Acc: 0.9876, Train F1: 0.9915 Val Loss: 0.0303, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 49/75, Train Loss: 0.0200, Train Acc: 0.9883, Train F1: 0.9917 Val Loss: 0.0288, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 50/75, Train Loss: 0.0200, Train Acc: 0.9889, Train F1: 0.9922 Val Loss: 0.0303, Val Acc: 0.9770, Val F1: 0.9840\n",
            "Epoch: 51/75, Train Loss: 0.0202, Train Acc: 0.9866, Train F1: 0.9907 Val Loss: 0.0292, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 52/75, Train Loss: 0.0199, Train Acc: 0.9877, Train F1: 0.9914 Val Loss: 0.0302, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 53/75, Train Loss: 0.0194, Train Acc: 0.9882, Train F1: 0.9918 Val Loss: 0.0289, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 54/75, Train Loss: 0.0196, Train Acc: 0.9892, Train F1: 0.9925 Val Loss: 0.0303, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 55/75, Train Loss: 0.0194, Train Acc: 0.9884, Train F1: 0.9918 Val Loss: 0.0301, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 56/75, Train Loss: 0.0191, Train Acc: 0.9897, Train F1: 0.9928 Val Loss: 0.0301, Val Acc: 0.9770, Val F1: 0.9840\n",
            "Epoch: 57/75, Train Loss: 0.0194, Train Acc: 0.9888, Train F1: 0.9922 Val Loss: 0.0290, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 58/75, Train Loss: 0.0187, Train Acc: 0.9907, Train F1: 0.9935 Val Loss: 0.0289, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 59/75, Train Loss: 0.0190, Train Acc: 0.9891, Train F1: 0.9924 Val Loss: 0.0282, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 60/75, Train Loss: 0.0186, Train Acc: 0.9900, Train F1: 0.9930 Val Loss: 0.0289, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 61/75, Train Loss: 0.0187, Train Acc: 0.9901, Train F1: 0.9931 Val Loss: 0.0291, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 62/75, Train Loss: 0.0185, Train Acc: 0.9912, Train F1: 0.9939 Val Loss: 0.0283, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 63/75, Train Loss: 0.0184, Train Acc: 0.9907, Train F1: 0.9935 Val Loss: 0.0286, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 64/75, Train Loss: 0.0182, Train Acc: 0.9908, Train F1: 0.9936 Val Loss: 0.0287, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 65/75, Train Loss: 0.0183, Train Acc: 0.9911, Train F1: 0.9938 Val Loss: 0.0283, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 66/75, Train Loss: 0.0181, Train Acc: 0.9907, Train F1: 0.9936 Val Loss: 0.0286, Val Acc: 0.9814, Val F1: 0.9872\n",
            "Epoch: 67/75, Train Loss: 0.0180, Train Acc: 0.9905, Train F1: 0.9933 Val Loss: 0.0283, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 68/75, Train Loss: 0.0180, Train Acc: 0.9911, Train F1: 0.9938 Val Loss: 0.0284, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 69/75, Train Loss: 0.0178, Train Acc: 0.9914, Train F1: 0.9940 Val Loss: 0.0282, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 70/75, Train Loss: 0.0177, Train Acc: 0.9918, Train F1: 0.9941 Val Loss: 0.0282, Val Acc: 0.9811, Val F1: 0.9870\n",
            "Epoch: 71/75, Train Loss: 0.0178, Train Acc: 0.9915, Train F1: 0.9941 Val Loss: 0.0282, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 72/75, Train Loss: 0.0177, Train Acc: 0.9914, Train F1: 0.9941 Val Loss: 0.0282, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 73/75, Train Loss: 0.0177, Train Acc: 0.9914, Train F1: 0.9940 Val Loss: 0.0282, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 74/75, Train Loss: 0.0176, Train Acc: 0.9918, Train F1: 0.9942 Val Loss: 0.0282, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 75/75, Train Loss: 0.0176, Train Acc: 0.9918, Train F1: 0.9943 Val Loss: 0.0282, Val Acc: 0.9808, Val F1: 0.9867\n",
            "\n",
            " 🔎 search 4 : deep_rescnn --- lr : 0.0008753516269381944, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv2.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1329, Train Acc: 0.8554, Train F1: 0.9055 Val Loss: 0.1038, Val Acc: 0.9055, Val F1: 0.9338\n",
            "Epoch: 2/75, Train Loss: 0.0956, Train Acc: 0.9126, Train F1: 0.9397 Val Loss: 0.1011, Val Acc: 0.9024, Val F1: 0.9353\n",
            "Epoch: 3/75, Train Loss: 0.0871, Train Acc: 0.9243, Train F1: 0.9478 Val Loss: 0.0931, Val Acc: 0.9137, Val F1: 0.9426\n",
            "Epoch: 4/75, Train Loss: 0.0829, Train Acc: 0.9244, Train F1: 0.9476 Val Loss: 0.0844, Val Acc: 0.9278, Val F1: 0.9497\n",
            "Epoch: 5/75, Train Loss: 0.0815, Train Acc: 0.9234, Train F1: 0.9468 Val Loss: 0.0838, Val Acc: 0.9337, Val F1: 0.9548\n",
            "Epoch: 6/75, Train Loss: 0.0781, Train Acc: 0.9302, Train F1: 0.9517 Val Loss: 0.0835, Val Acc: 0.9251, Val F1: 0.9489\n",
            "Epoch: 7/75, Train Loss: 0.0725, Train Acc: 0.9334, Train F1: 0.9541 Val Loss: 0.0986, Val Acc: 0.8893, Val F1: 0.9283\n",
            "Epoch: 8/75, Train Loss: 0.0720, Train Acc: 0.9294, Train F1: 0.9509 Val Loss: 0.0798, Val Acc: 0.9196, Val F1: 0.9432\n",
            "Epoch: 9/75, Train Loss: 0.0684, Train Acc: 0.9357, Train F1: 0.9554 Val Loss: 0.0790, Val Acc: 0.9265, Val F1: 0.9503\n",
            "Epoch: 10/75, Train Loss: 0.0681, Train Acc: 0.9361, Train F1: 0.9557 Val Loss: 0.0698, Val Acc: 0.9447, Val F1: 0.9623\n",
            "Epoch: 11/75, Train Loss: 0.0634, Train Acc: 0.9389, Train F1: 0.9574 Val Loss: 0.0688, Val Acc: 0.9433, Val F1: 0.9613\n",
            "Epoch: 12/75, Train Loss: 0.0632, Train Acc: 0.9391, Train F1: 0.9576 Val Loss: 0.0688, Val Acc: 0.9347, Val F1: 0.9549\n",
            "Epoch: 13/75, Train Loss: 0.0622, Train Acc: 0.9405, Train F1: 0.9589 Val Loss: 0.0737, Val Acc: 0.9244, Val F1: 0.9495\n",
            "Epoch: 14/75, Train Loss: 0.0585, Train Acc: 0.9424, Train F1: 0.9602 Val Loss: 0.0664, Val Acc: 0.9412, Val F1: 0.9605\n",
            "Epoch: 15/75, Train Loss: 0.0590, Train Acc: 0.9407, Train F1: 0.9586 Val Loss: 0.0698, Val Acc: 0.9316, Val F1: 0.9523\n",
            "Epoch: 16/75, Train Loss: 0.0592, Train Acc: 0.9434, Train F1: 0.9609 Val Loss: 0.0696, Val Acc: 0.9344, Val F1: 0.9559\n",
            "Epoch: 17/75, Train Loss: 0.0593, Train Acc: 0.9425, Train F1: 0.9602 Val Loss: 0.0718, Val Acc: 0.9258, Val F1: 0.9507\n",
            "Epoch: 18/75, Train Loss: 0.0523, Train Acc: 0.9499, Train F1: 0.9654 Val Loss: 0.0607, Val Acc: 0.9440, Val F1: 0.9609\n",
            "Epoch: 19/75, Train Loss: 0.0542, Train Acc: 0.9482, Train F1: 0.9643 Val Loss: 0.0572, Val Acc: 0.9498, Val F1: 0.9657\n",
            "Epoch: 20/75, Train Loss: 0.0494, Train Acc: 0.9541, Train F1: 0.9683 Val Loss: 0.0573, Val Acc: 0.9460, Val F1: 0.9625\n",
            "Epoch: 21/75, Train Loss: 0.0510, Train Acc: 0.9511, Train F1: 0.9662 Val Loss: 0.0576, Val Acc: 0.9457, Val F1: 0.9624\n",
            "Epoch: 22/75, Train Loss: 0.0515, Train Acc: 0.9494, Train F1: 0.9649 Val Loss: 0.0615, Val Acc: 0.9416, Val F1: 0.9605\n",
            "Epoch: 23/75, Train Loss: 0.0524, Train Acc: 0.9491, Train F1: 0.9647 Val Loss: 0.0569, Val Acc: 0.9485, Val F1: 0.9649\n",
            "Epoch: 24/75, Train Loss: 0.0560, Train Acc: 0.9456, Train F1: 0.9622 Val Loss: 0.0635, Val Acc: 0.9430, Val F1: 0.9611\n",
            "Epoch: 25/75, Train Loss: 0.0502, Train Acc: 0.9512, Train F1: 0.9660 Val Loss: 0.0600, Val Acc: 0.9457, Val F1: 0.9621\n",
            "Epoch: 26/75, Train Loss: 0.0483, Train Acc: 0.9552, Train F1: 0.9689 Val Loss: 0.0576, Val Acc: 0.9495, Val F1: 0.9646\n",
            "Epoch: 27/75, Train Loss: 0.0475, Train Acc: 0.9545, Train F1: 0.9686 Val Loss: 0.0570, Val Acc: 0.9495, Val F1: 0.9658\n",
            "Epoch: 28/75, Train Loss: 0.0454, Train Acc: 0.9589, Train F1: 0.9716 Val Loss: 0.0547, Val Acc: 0.9502, Val F1: 0.9663\n",
            "Epoch: 29/75, Train Loss: 0.0482, Train Acc: 0.9567, Train F1: 0.9701 Val Loss: 0.0621, Val Acc: 0.9464, Val F1: 0.9630\n",
            "Epoch: 30/75, Train Loss: 0.0478, Train Acc: 0.9550, Train F1: 0.9688 Val Loss: 0.0513, Val Acc: 0.9529, Val F1: 0.9675\n",
            "Epoch: 31/75, Train Loss: 0.0486, Train Acc: 0.9533, Train F1: 0.9679 Val Loss: 0.0663, Val Acc: 0.9354, Val F1: 0.9569\n",
            "Epoch: 32/75, Train Loss: 0.0472, Train Acc: 0.9586, Train F1: 0.9714 Val Loss: 0.0526, Val Acc: 0.9460, Val F1: 0.9629\n",
            "Epoch: 33/75, Train Loss: 0.0435, Train Acc: 0.9599, Train F1: 0.9723 Val Loss: 0.0517, Val Acc: 0.9553, Val F1: 0.9693\n",
            "Epoch: 34/75, Train Loss: 0.0486, Train Acc: 0.9541, Train F1: 0.9683 Val Loss: 0.0559, Val Acc: 0.9526, Val F1: 0.9676\n",
            "Epoch: 35/75, Train Loss: 0.0468, Train Acc: 0.9584, Train F1: 0.9710 Val Loss: 0.0503, Val Acc: 0.9526, Val F1: 0.9675\n",
            "Epoch: 36/75, Train Loss: 0.0436, Train Acc: 0.9600, Train F1: 0.9722 Val Loss: 0.0521, Val Acc: 0.9550, Val F1: 0.9691\n",
            "Epoch: 37/75, Train Loss: 0.0415, Train Acc: 0.9620, Train F1: 0.9737 Val Loss: 0.0499, Val Acc: 0.9577, Val F1: 0.9708\n",
            "Epoch: 38/75, Train Loss: 0.0434, Train Acc: 0.9604, Train F1: 0.9724 Val Loss: 0.0565, Val Acc: 0.9464, Val F1: 0.9637\n",
            "Epoch: 39/75, Train Loss: 0.0400, Train Acc: 0.9641, Train F1: 0.9753 Val Loss: 0.0481, Val Acc: 0.9601, Val F1: 0.9728\n",
            "Epoch: 40/75, Train Loss: 0.0416, Train Acc: 0.9619, Train F1: 0.9736 Val Loss: 0.0494, Val Acc: 0.9622, Val F1: 0.9738\n",
            "Epoch: 41/75, Train Loss: 0.0436, Train Acc: 0.9596, Train F1: 0.9719 Val Loss: 0.0531, Val Acc: 0.9553, Val F1: 0.9697\n",
            "Epoch: 42/75, Train Loss: 0.0434, Train Acc: 0.9582, Train F1: 0.9710 Val Loss: 0.0522, Val Acc: 0.9588, Val F1: 0.9714\n",
            "Epoch: 43/75, Train Loss: 0.0408, Train Acc: 0.9641, Train F1: 0.9752 Val Loss: 0.0478, Val Acc: 0.9519, Val F1: 0.9670\n",
            "Epoch: 44/75, Train Loss: 0.0390, Train Acc: 0.9648, Train F1: 0.9758 Val Loss: 0.0514, Val Acc: 0.9550, Val F1: 0.9694\n",
            "Epoch: 45/75, Train Loss: 0.0390, Train Acc: 0.9655, Train F1: 0.9760 Val Loss: 0.0470, Val Acc: 0.9619, Val F1: 0.9735\n",
            "Epoch: 46/75, Train Loss: 0.0397, Train Acc: 0.9655, Train F1: 0.9762 Val Loss: 0.0548, Val Acc: 0.9450, Val F1: 0.9630\n",
            "Epoch: 47/75, Train Loss: 0.0375, Train Acc: 0.9690, Train F1: 0.9785 Val Loss: 0.0486, Val Acc: 0.9557, Val F1: 0.9699\n",
            "Epoch: 48/75, Train Loss: 0.0381, Train Acc: 0.9670, Train F1: 0.9772 Val Loss: 0.0486, Val Acc: 0.9574, Val F1: 0.9710\n",
            "Epoch: 49/75, Train Loss: 0.0376, Train Acc: 0.9683, Train F1: 0.9780 Val Loss: 0.0450, Val Acc: 0.9632, Val F1: 0.9747\n",
            "Epoch: 50/75, Train Loss: 0.0358, Train Acc: 0.9707, Train F1: 0.9797 Val Loss: 0.0460, Val Acc: 0.9595, Val F1: 0.9720\n",
            "Epoch: 51/75, Train Loss: 0.0360, Train Acc: 0.9686, Train F1: 0.9783 Val Loss: 0.0462, Val Acc: 0.9619, Val F1: 0.9737\n",
            "Epoch: 52/75, Train Loss: 0.0356, Train Acc: 0.9703, Train F1: 0.9795 Val Loss: 0.0493, Val Acc: 0.9581, Val F1: 0.9715\n",
            "Epoch: 53/75, Train Loss: 0.0365, Train Acc: 0.9706, Train F1: 0.9796 Val Loss: 0.0491, Val Acc: 0.9584, Val F1: 0.9709\n",
            "Epoch: 54/75, Train Loss: 0.0365, Train Acc: 0.9692, Train F1: 0.9786 Val Loss: 0.0461, Val Acc: 0.9619, Val F1: 0.9740\n",
            "Epoch: 55/75, Train Loss: 0.0364, Train Acc: 0.9691, Train F1: 0.9785 Val Loss: 0.0454, Val Acc: 0.9653, Val F1: 0.9759\n",
            "Epoch: 56/75, Train Loss: 0.0347, Train Acc: 0.9702, Train F1: 0.9794 Val Loss: 0.0443, Val Acc: 0.9660, Val F1: 0.9765\n",
            "Epoch: 57/75, Train Loss: 0.0340, Train Acc: 0.9740, Train F1: 0.9819 Val Loss: 0.0446, Val Acc: 0.9663, Val F1: 0.9770\n",
            "Epoch: 58/75, Train Loss: 0.0344, Train Acc: 0.9718, Train F1: 0.9804 Val Loss: 0.0435, Val Acc: 0.9667, Val F1: 0.9770\n",
            "Epoch: 59/75, Train Loss: 0.0339, Train Acc: 0.9730, Train F1: 0.9812 Val Loss: 0.0443, Val Acc: 0.9629, Val F1: 0.9743\n",
            "Epoch: 60/75, Train Loss: 0.0340, Train Acc: 0.9737, Train F1: 0.9817 Val Loss: 0.0434, Val Acc: 0.9636, Val F1: 0.9749\n",
            "Epoch: 61/75, Train Loss: 0.0338, Train Acc: 0.9732, Train F1: 0.9815 Val Loss: 0.0433, Val Acc: 0.9660, Val F1: 0.9766\n",
            "Epoch: 62/75, Train Loss: 0.0334, Train Acc: 0.9743, Train F1: 0.9822 Val Loss: 0.0431, Val Acc: 0.9656, Val F1: 0.9763\n",
            "Epoch: 63/75, Train Loss: 0.0330, Train Acc: 0.9740, Train F1: 0.9819 Val Loss: 0.0431, Val Acc: 0.9653, Val F1: 0.9760\n",
            "Epoch: 64/75, Train Loss: 0.0330, Train Acc: 0.9746, Train F1: 0.9824 Val Loss: 0.0431, Val Acc: 0.9660, Val F1: 0.9765\n",
            "Epoch: 65/75, Train Loss: 0.0331, Train Acc: 0.9754, Train F1: 0.9829 Val Loss: 0.0433, Val Acc: 0.9674, Val F1: 0.9775\n",
            "Epoch: 66/75, Train Loss: 0.0328, Train Acc: 0.9743, Train F1: 0.9823 Val Loss: 0.0426, Val Acc: 0.9674, Val F1: 0.9775\n",
            "Epoch: 67/75, Train Loss: 0.0327, Train Acc: 0.9758, Train F1: 0.9833 Val Loss: 0.0429, Val Acc: 0.9656, Val F1: 0.9764\n",
            "Epoch: 68/75, Train Loss: 0.0325, Train Acc: 0.9770, Train F1: 0.9840 Val Loss: 0.0429, Val Acc: 0.9653, Val F1: 0.9762\n",
            "Epoch: 69/75, Train Loss: 0.0323, Train Acc: 0.9766, Train F1: 0.9838 Val Loss: 0.0426, Val Acc: 0.9677, Val F1: 0.9778\n",
            "Epoch: 70/75, Train Loss: 0.0325, Train Acc: 0.9766, Train F1: 0.9838 Val Loss: 0.0427, Val Acc: 0.9656, Val F1: 0.9764\n",
            "Epoch: 71/75, Train Loss: 0.0321, Train Acc: 0.9770, Train F1: 0.9841 Val Loss: 0.0426, Val Acc: 0.9670, Val F1: 0.9773\n",
            "Epoch: 72/75, Train Loss: 0.0321, Train Acc: 0.9770, Train F1: 0.9840 Val Loss: 0.0425, Val Acc: 0.9674, Val F1: 0.9776\n",
            "Epoch: 73/75, Train Loss: 0.0320, Train Acc: 0.9769, Train F1: 0.9840 Val Loss: 0.0424, Val Acc: 0.9677, Val F1: 0.9778\n",
            "Epoch: 74/75, Train Loss: 0.0320, Train Acc: 0.9777, Train F1: 0.9846 Val Loss: 0.0425, Val Acc: 0.9667, Val F1: 0.9771\n",
            "Epoch: 75/75, Train Loss: 0.0320, Train Acc: 0.9769, Train F1: 0.9840 Val Loss: 0.0425, Val Acc: 0.9667, Val F1: 0.9771\n",
            "\n",
            " 🔎 search 5 : deep_rescnn --- lr : 0.0008753516269381944, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv3.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1080, Train Acc: 0.8858, Train F1: 0.9221 Val Loss: 0.0809, Val Acc: 0.9271, Val F1: 0.9507\n",
            "Epoch: 2/75, Train Loss: 0.0721, Train Acc: 0.9324, Train F1: 0.9532 Val Loss: 0.0817, Val Acc: 0.9100, Val F1: 0.9354\n",
            "Epoch: 3/75, Train Loss: 0.0636, Train Acc: 0.9389, Train F1: 0.9577 Val Loss: 0.0613, Val Acc: 0.9488, Val F1: 0.9650\n",
            "Epoch: 4/75, Train Loss: 0.0566, Train Acc: 0.9456, Train F1: 0.9622 Val Loss: 0.0704, Val Acc: 0.9306, Val F1: 0.9539\n",
            "Epoch: 5/75, Train Loss: 0.0530, Train Acc: 0.9499, Train F1: 0.9653 Val Loss: 0.0635, Val Acc: 0.9423, Val F1: 0.9612\n",
            "Epoch: 6/75, Train Loss: 0.0548, Train Acc: 0.9439, Train F1: 0.9610 Val Loss: 0.0573, Val Acc: 0.9546, Val F1: 0.9692\n",
            "Epoch: 7/75, Train Loss: 0.0497, Train Acc: 0.9542, Train F1: 0.9684 Val Loss: 0.0577, Val Acc: 0.9519, Val F1: 0.9674\n",
            "Epoch: 8/75, Train Loss: 0.0494, Train Acc: 0.9540, Train F1: 0.9681 Val Loss: 0.0517, Val Acc: 0.9553, Val F1: 0.9692\n",
            "Epoch: 9/75, Train Loss: 0.0466, Train Acc: 0.9586, Train F1: 0.9714 Val Loss: 0.0509, Val Acc: 0.9540, Val F1: 0.9681\n",
            "Epoch: 10/75, Train Loss: 0.0489, Train Acc: 0.9541, Train F1: 0.9679 Val Loss: 0.0540, Val Acc: 0.9540, Val F1: 0.9686\n",
            "Epoch: 11/75, Train Loss: 0.0446, Train Acc: 0.9591, Train F1: 0.9717 Val Loss: 0.0622, Val Acc: 0.9457, Val F1: 0.9634\n",
            "Epoch: 12/75, Train Loss: 0.0465, Train Acc: 0.9553, Train F1: 0.9689 Val Loss: 0.0517, Val Acc: 0.9536, Val F1: 0.9685\n",
            "Epoch: 13/75, Train Loss: 0.0437, Train Acc: 0.9603, Train F1: 0.9724 Val Loss: 0.0493, Val Acc: 0.9557, Val F1: 0.9694\n",
            "Epoch: 14/75, Train Loss: 0.0414, Train Acc: 0.9631, Train F1: 0.9745 Val Loss: 0.0507, Val Acc: 0.9567, Val F1: 0.9702\n",
            "Epoch: 15/75, Train Loss: 0.0403, Train Acc: 0.9648, Train F1: 0.9755 Val Loss: 0.0462, Val Acc: 0.9595, Val F1: 0.9723\n",
            "Epoch: 16/75, Train Loss: 0.0396, Train Acc: 0.9651, Train F1: 0.9758 Val Loss: 0.0533, Val Acc: 0.9505, Val F1: 0.9666\n",
            "Epoch: 17/75, Train Loss: 0.0388, Train Acc: 0.9655, Train F1: 0.9762 Val Loss: 0.0506, Val Acc: 0.9478, Val F1: 0.9633\n",
            "Epoch: 18/75, Train Loss: 0.0421, Train Acc: 0.9614, Train F1: 0.9733 Val Loss: 0.0459, Val Acc: 0.9598, Val F1: 0.9721\n",
            "Epoch: 19/75, Train Loss: 0.0367, Train Acc: 0.9685, Train F1: 0.9781 Val Loss: 0.0465, Val Acc: 0.9543, Val F1: 0.9680\n",
            "Epoch: 20/75, Train Loss: 0.0414, Train Acc: 0.9639, Train F1: 0.9750 Val Loss: 0.0489, Val Acc: 0.9622, Val F1: 0.9743\n",
            "Epoch: 21/75, Train Loss: 0.0402, Train Acc: 0.9656, Train F1: 0.9762 Val Loss: 0.0576, Val Acc: 0.9512, Val F1: 0.9672\n",
            "Epoch: 22/75, Train Loss: 0.0353, Train Acc: 0.9696, Train F1: 0.9788 Val Loss: 0.0456, Val Acc: 0.9625, Val F1: 0.9738\n",
            "Epoch: 23/75, Train Loss: 0.0347, Train Acc: 0.9711, Train F1: 0.9799 Val Loss: 0.0447, Val Acc: 0.9656, Val F1: 0.9765\n",
            "Epoch: 24/75, Train Loss: 0.0353, Train Acc: 0.9695, Train F1: 0.9788 Val Loss: 0.0426, Val Acc: 0.9646, Val F1: 0.9755\n",
            "Epoch: 25/75, Train Loss: 0.0417, Train Acc: 0.9598, Train F1: 0.9722 Val Loss: 0.0434, Val Acc: 0.9615, Val F1: 0.9732\n",
            "Epoch: 26/75, Train Loss: 0.0376, Train Acc: 0.9679, Train F1: 0.9778 Val Loss: 0.0490, Val Acc: 0.9584, Val F1: 0.9718\n",
            "Epoch: 27/75, Train Loss: 0.0359, Train Acc: 0.9695, Train F1: 0.9788 Val Loss: 0.0435, Val Acc: 0.9670, Val F1: 0.9774\n",
            "Epoch: 28/75, Train Loss: 0.0341, Train Acc: 0.9716, Train F1: 0.9802 Val Loss: 0.0411, Val Acc: 0.9674, Val F1: 0.9775\n",
            "Epoch: 29/75, Train Loss: 0.0371, Train Acc: 0.9701, Train F1: 0.9792 Val Loss: 0.0529, Val Acc: 0.9498, Val F1: 0.9663\n",
            "Epoch: 30/75, Train Loss: 0.0337, Train Acc: 0.9724, Train F1: 0.9808 Val Loss: 0.0424, Val Acc: 0.9622, Val F1: 0.9736\n",
            "Epoch: 31/75, Train Loss: 0.0347, Train Acc: 0.9721, Train F1: 0.9805 Val Loss: 0.0451, Val Acc: 0.9646, Val F1: 0.9757\n",
            "Epoch: 32/75, Train Loss: 0.0320, Train Acc: 0.9763, Train F1: 0.9835 Val Loss: 0.0417, Val Acc: 0.9704, Val F1: 0.9798\n",
            "Epoch: 33/75, Train Loss: 0.0326, Train Acc: 0.9718, Train F1: 0.9805 Val Loss: 0.0376, Val Acc: 0.9704, Val F1: 0.9797\n",
            "Epoch: 34/75, Train Loss: 0.0309, Train Acc: 0.9756, Train F1: 0.9831 Val Loss: 0.0450, Val Acc: 0.9619, Val F1: 0.9734\n",
            "Epoch: 35/75, Train Loss: 0.0324, Train Acc: 0.9723, Train F1: 0.9807 Val Loss: 0.0412, Val Acc: 0.9632, Val F1: 0.9750\n",
            "Epoch: 36/75, Train Loss: 0.0332, Train Acc: 0.9725, Train F1: 0.9810 Val Loss: 0.0376, Val Acc: 0.9704, Val F1: 0.9795\n",
            "Epoch: 37/75, Train Loss: 0.0310, Train Acc: 0.9762, Train F1: 0.9834 Val Loss: 0.0424, Val Acc: 0.9636, Val F1: 0.9752\n",
            "Epoch: 38/75, Train Loss: 0.0332, Train Acc: 0.9729, Train F1: 0.9812 Val Loss: 0.0423, Val Acc: 0.9612, Val F1: 0.9735\n",
            "Epoch: 39/75, Train Loss: 0.0316, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0455, Val Acc: 0.9574, Val F1: 0.9700\n",
            "Epoch: 40/75, Train Loss: 0.0307, Train Acc: 0.9756, Train F1: 0.9830 Val Loss: 0.0389, Val Acc: 0.9649, Val F1: 0.9756\n",
            "Epoch: 41/75, Train Loss: 0.0315, Train Acc: 0.9756, Train F1: 0.9830 Val Loss: 0.0418, Val Acc: 0.9691, Val F1: 0.9789\n",
            "Epoch: 42/75, Train Loss: 0.0298, Train Acc: 0.9785, Train F1: 0.9851 Val Loss: 0.0378, Val Acc: 0.9704, Val F1: 0.9797\n",
            "Epoch: 43/75, Train Loss: 0.0320, Train Acc: 0.9756, Train F1: 0.9830 Val Loss: 0.0408, Val Acc: 0.9629, Val F1: 0.9747\n",
            "Epoch: 44/75, Train Loss: 0.0319, Train Acc: 0.9729, Train F1: 0.9811 Val Loss: 0.0389, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 45/75, Train Loss: 0.0286, Train Acc: 0.9793, Train F1: 0.9855 Val Loss: 0.0370, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 46/75, Train Loss: 0.0291, Train Acc: 0.9767, Train F1: 0.9839 Val Loss: 0.0378, Val Acc: 0.9653, Val F1: 0.9759\n",
            "Epoch: 47/75, Train Loss: 0.0290, Train Acc: 0.9777, Train F1: 0.9845 Val Loss: 0.0425, Val Acc: 0.9625, Val F1: 0.9746\n",
            "Epoch: 48/75, Train Loss: 0.0300, Train Acc: 0.9754, Train F1: 0.9829 Val Loss: 0.0396, Val Acc: 0.9619, Val F1: 0.9734\n",
            "Epoch: 49/75, Train Loss: 0.0299, Train Acc: 0.9764, Train F1: 0.9837 Val Loss: 0.0371, Val Acc: 0.9698, Val F1: 0.9791\n",
            "Epoch: 50/75, Train Loss: 0.0299, Train Acc: 0.9761, Train F1: 0.9834 Val Loss: 0.0359, Val Acc: 0.9694, Val F1: 0.9788\n",
            "Epoch: 51/75, Train Loss: 0.0286, Train Acc: 0.9774, Train F1: 0.9842 Val Loss: 0.0370, Val Acc: 0.9684, Val F1: 0.9784\n",
            "Epoch: 52/75, Train Loss: 0.0285, Train Acc: 0.9778, Train F1: 0.9847 Val Loss: 0.0363, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 53/75, Train Loss: 0.0280, Train Acc: 0.9796, Train F1: 0.9859 Val Loss: 0.0374, Val Acc: 0.9680, Val F1: 0.9777\n",
            "Epoch: 54/75, Train Loss: 0.0281, Train Acc: 0.9794, Train F1: 0.9857 Val Loss: 0.0365, Val Acc: 0.9711, Val F1: 0.9802\n",
            "Epoch: 55/75, Train Loss: 0.0272, Train Acc: 0.9808, Train F1: 0.9866 Val Loss: 0.0374, Val Acc: 0.9649, Val F1: 0.9756\n",
            "Epoch: 56/75, Train Loss: 0.0270, Train Acc: 0.9817, Train F1: 0.9872 Val Loss: 0.0353, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 57/75, Train Loss: 0.0267, Train Acc: 0.9813, Train F1: 0.9870 Val Loss: 0.0350, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 58/75, Train Loss: 0.0266, Train Acc: 0.9821, Train F1: 0.9876 Val Loss: 0.0358, Val Acc: 0.9698, Val F1: 0.9793\n",
            "Epoch: 59/75, Train Loss: 0.0265, Train Acc: 0.9814, Train F1: 0.9872 Val Loss: 0.0352, Val Acc: 0.9711, Val F1: 0.9802\n",
            "Epoch: 60/75, Train Loss: 0.0263, Train Acc: 0.9818, Train F1: 0.9873 Val Loss: 0.0350, Val Acc: 0.9718, Val F1: 0.9806\n",
            "Epoch: 61/75, Train Loss: 0.0261, Train Acc: 0.9820, Train F1: 0.9875 Val Loss: 0.0345, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 62/75, Train Loss: 0.0261, Train Acc: 0.9822, Train F1: 0.9876 Val Loss: 0.0345, Val Acc: 0.9725, Val F1: 0.9811\n",
            "Epoch: 63/75, Train Loss: 0.0263, Train Acc: 0.9809, Train F1: 0.9868 Val Loss: 0.0348, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 64/75, Train Loss: 0.0259, Train Acc: 0.9828, Train F1: 0.9880 Val Loss: 0.0368, Val Acc: 0.9691, Val F1: 0.9788\n",
            "Epoch: 65/75, Train Loss: 0.0258, Train Acc: 0.9825, Train F1: 0.9879 Val Loss: 0.0359, Val Acc: 0.9708, Val F1: 0.9800\n",
            "Epoch: 66/75, Train Loss: 0.0260, Train Acc: 0.9816, Train F1: 0.9872 Val Loss: 0.0343, Val Acc: 0.9722, Val F1: 0.9808\n",
            "Epoch: 67/75, Train Loss: 0.0256, Train Acc: 0.9824, Train F1: 0.9877 Val Loss: 0.0343, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 68/75, Train Loss: 0.0254, Train Acc: 0.9829, Train F1: 0.9881 Val Loss: 0.0344, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 69/75, Train Loss: 0.0255, Train Acc: 0.9826, Train F1: 0.9880 Val Loss: 0.0343, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 70/75, Train Loss: 0.0254, Train Acc: 0.9829, Train F1: 0.9880 Val Loss: 0.0343, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 71/75, Train Loss: 0.0254, Train Acc: 0.9830, Train F1: 0.9883 Val Loss: 0.0344, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 72/75, Train Loss: 0.0253, Train Acc: 0.9829, Train F1: 0.9880 Val Loss: 0.0342, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 73/75, Train Loss: 0.0252, Train Acc: 0.9830, Train F1: 0.9882 Val Loss: 0.0342, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 74/75, Train Loss: 0.0252, Train Acc: 0.9828, Train F1: 0.9880 Val Loss: 0.0342, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 75/75, Train Loss: 0.0252, Train Acc: 0.9833, Train F1: 0.9883 Val Loss: 0.0342, Val Acc: 0.9735, Val F1: 0.9818\n",
            "\n",
            " 🔎 search 6 : deep_rescnn --- lr : 0.0008753516269381944, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv4.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1420, Train Acc: 0.8423, Train F1: 0.8881 Val Loss: 0.0988, Val Acc: 0.9014, Val F1: 0.9308\n",
            "Epoch: 2/75, Train Loss: 0.0865, Train Acc: 0.9110, Train F1: 0.9386 Val Loss: 0.0884, Val Acc: 0.9093, Val F1: 0.9353\n",
            "Epoch: 3/75, Train Loss: 0.0695, Train Acc: 0.9341, Train F1: 0.9542 Val Loss: 0.0722, Val Acc: 0.9347, Val F1: 0.9561\n",
            "Epoch: 4/75, Train Loss: 0.0609, Train Acc: 0.9442, Train F1: 0.9613 Val Loss: 0.0641, Val Acc: 0.9430, Val F1: 0.9612\n",
            "Epoch: 5/75, Train Loss: 0.0604, Train Acc: 0.9423, Train F1: 0.9600 Val Loss: 0.0594, Val Acc: 0.9495, Val F1: 0.9649\n",
            "Epoch: 6/75, Train Loss: 0.0537, Train Acc: 0.9504, Train F1: 0.9656 Val Loss: 0.0536, Val Acc: 0.9540, Val F1: 0.9682\n",
            "Epoch: 7/75, Train Loss: 0.0467, Train Acc: 0.9600, Train F1: 0.9723 Val Loss: 0.0610, Val Acc: 0.9478, Val F1: 0.9648\n",
            "Epoch: 8/75, Train Loss: 0.0461, Train Acc: 0.9576, Train F1: 0.9707 Val Loss: 0.0486, Val Acc: 0.9567, Val F1: 0.9703\n",
            "Epoch: 9/75, Train Loss: 0.0419, Train Acc: 0.9619, Train F1: 0.9735 Val Loss: 0.0546, Val Acc: 0.9570, Val F1: 0.9710\n",
            "Epoch: 10/75, Train Loss: 0.0395, Train Acc: 0.9670, Train F1: 0.9770 Val Loss: 0.0521, Val Acc: 0.9550, Val F1: 0.9696\n",
            "Epoch: 11/75, Train Loss: 0.0384, Train Acc: 0.9669, Train F1: 0.9772 Val Loss: 0.0541, Val Acc: 0.9505, Val F1: 0.9667\n",
            "Epoch: 12/75, Train Loss: 0.0398, Train Acc: 0.9659, Train F1: 0.9762 Val Loss: 0.0463, Val Acc: 0.9622, Val F1: 0.9743\n",
            "Epoch: 13/75, Train Loss: 0.0392, Train Acc: 0.9656, Train F1: 0.9762 Val Loss: 0.0419, Val Acc: 0.9622, Val F1: 0.9737\n",
            "Epoch: 14/75, Train Loss: 0.0347, Train Acc: 0.9716, Train F1: 0.9804 Val Loss: 0.0470, Val Acc: 0.9636, Val F1: 0.9753\n",
            "Epoch: 15/75, Train Loss: 0.0318, Train Acc: 0.9774, Train F1: 0.9842 Val Loss: 0.0778, Val Acc: 0.9120, Val F1: 0.9425\n",
            "Epoch: 16/75, Train Loss: 0.0359, Train Acc: 0.9695, Train F1: 0.9788 Val Loss: 0.0391, Val Acc: 0.9722, Val F1: 0.9808\n",
            "Epoch: 17/75, Train Loss: 0.0323, Train Acc: 0.9765, Train F1: 0.9836 Val Loss: 0.0412, Val Acc: 0.9649, Val F1: 0.9755\n",
            "Epoch: 18/75, Train Loss: 0.0308, Train Acc: 0.9778, Train F1: 0.9846 Val Loss: 0.0393, Val Acc: 0.9711, Val F1: 0.9801\n",
            "Epoch: 19/75, Train Loss: 0.0313, Train Acc: 0.9769, Train F1: 0.9838 Val Loss: 0.0537, Val Acc: 0.9529, Val F1: 0.9683\n",
            "Epoch: 20/75, Train Loss: 0.0308, Train Acc: 0.9767, Train F1: 0.9838 Val Loss: 0.0502, Val Acc: 0.9557, Val F1: 0.9700\n",
            "Epoch: 21/75, Train Loss: 0.0299, Train Acc: 0.9786, Train F1: 0.9852 Val Loss: 0.0392, Val Acc: 0.9656, Val F1: 0.9760\n",
            "Epoch: 22/75, Train Loss: 0.0298, Train Acc: 0.9782, Train F1: 0.9849 Val Loss: 0.0351, Val Acc: 0.9722, Val F1: 0.9809\n",
            "Epoch: 23/75, Train Loss: 0.0320, Train Acc: 0.9747, Train F1: 0.9823 Val Loss: 0.0389, Val Acc: 0.9718, Val F1: 0.9805\n",
            "Epoch: 24/75, Train Loss: 0.0281, Train Acc: 0.9802, Train F1: 0.9863 Val Loss: 0.0399, Val Acc: 0.9698, Val F1: 0.9793\n",
            "Epoch: 25/75, Train Loss: 0.0317, Train Acc: 0.9748, Train F1: 0.9825 Val Loss: 0.0405, Val Acc: 0.9722, Val F1: 0.9809\n",
            "Epoch: 26/75, Train Loss: 0.0285, Train Acc: 0.9795, Train F1: 0.9857 Val Loss: 0.0360, Val Acc: 0.9680, Val F1: 0.9778\n",
            "Epoch: 27/75, Train Loss: 0.0267, Train Acc: 0.9822, Train F1: 0.9876 Val Loss: 0.0352, Val Acc: 0.9732, Val F1: 0.9814\n",
            "Epoch: 28/75, Train Loss: 0.0319, Train Acc: 0.9734, Train F1: 0.9816 Val Loss: 0.0345, Val Acc: 0.9701, Val F1: 0.9793\n",
            "Epoch: 29/75, Train Loss: 0.0266, Train Acc: 0.9833, Train F1: 0.9883 Val Loss: 0.0390, Val Acc: 0.9722, Val F1: 0.9810\n",
            "Epoch: 30/75, Train Loss: 0.0282, Train Acc: 0.9787, Train F1: 0.9852 Val Loss: 0.0374, Val Acc: 0.9646, Val F1: 0.9755\n",
            "Epoch: 31/75, Train Loss: 0.0260, Train Acc: 0.9822, Train F1: 0.9877 Val Loss: 0.0337, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 32/75, Train Loss: 0.0266, Train Acc: 0.9828, Train F1: 0.9881 Val Loss: 0.0382, Val Acc: 0.9708, Val F1: 0.9796\n",
            "Epoch: 33/75, Train Loss: 0.0260, Train Acc: 0.9817, Train F1: 0.9872 Val Loss: 0.0355, Val Acc: 0.9729, Val F1: 0.9812\n",
            "Epoch: 34/75, Train Loss: 0.0253, Train Acc: 0.9836, Train F1: 0.9886 Val Loss: 0.0362, Val Acc: 0.9698, Val F1: 0.9793\n",
            "Epoch: 35/75, Train Loss: 0.0265, Train Acc: 0.9806, Train F1: 0.9866 Val Loss: 0.0339, Val Acc: 0.9732, Val F1: 0.9816\n",
            "Epoch: 36/75, Train Loss: 0.0245, Train Acc: 0.9848, Train F1: 0.9894 Val Loss: 0.0362, Val Acc: 0.9725, Val F1: 0.9808\n",
            "Epoch: 37/75, Train Loss: 0.0254, Train Acc: 0.9837, Train F1: 0.9887 Val Loss: 0.0334, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 38/75, Train Loss: 0.0260, Train Acc: 0.9827, Train F1: 0.9880 Val Loss: 0.0340, Val Acc: 0.9725, Val F1: 0.9809\n",
            "Epoch: 39/75, Train Loss: 0.0263, Train Acc: 0.9803, Train F1: 0.9864 Val Loss: 0.0335, Val Acc: 0.9753, Val F1: 0.9828\n",
            "Epoch: 40/75, Train Loss: 0.0240, Train Acc: 0.9849, Train F1: 0.9894 Val Loss: 0.0341, Val Acc: 0.9732, Val F1: 0.9816\n",
            "Epoch: 41/75, Train Loss: 0.0239, Train Acc: 0.9855, Train F1: 0.9898 Val Loss: 0.0350, Val Acc: 0.9732, Val F1: 0.9816\n",
            "Epoch: 42/75, Train Loss: 0.0238, Train Acc: 0.9851, Train F1: 0.9897 Val Loss: 0.0335, Val Acc: 0.9739, Val F1: 0.9821\n",
            "Epoch: 43/75, Train Loss: 0.0258, Train Acc: 0.9828, Train F1: 0.9882 Val Loss: 0.0355, Val Acc: 0.9708, Val F1: 0.9796\n",
            "Epoch: 44/75, Train Loss: 0.0239, Train Acc: 0.9851, Train F1: 0.9896 Val Loss: 0.0352, Val Acc: 0.9729, Val F1: 0.9810\n",
            "Epoch: 45/75, Train Loss: 0.0238, Train Acc: 0.9845, Train F1: 0.9892 Val Loss: 0.0329, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 46/75, Train Loss: 0.0229, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0332, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 47/75, Train Loss: 0.0242, Train Acc: 0.9847, Train F1: 0.9893 Val Loss: 0.0326, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 48/75, Train Loss: 0.0235, Train Acc: 0.9845, Train F1: 0.9893 Val Loss: 0.0350, Val Acc: 0.9718, Val F1: 0.9803\n",
            "Epoch: 49/75, Train Loss: 0.0232, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0319, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 50/75, Train Loss: 0.0223, Train Acc: 0.9869, Train F1: 0.9909 Val Loss: 0.0342, Val Acc: 0.9742, Val F1: 0.9820\n",
            "Epoch: 51/75, Train Loss: 0.0228, Train Acc: 0.9861, Train F1: 0.9904 Val Loss: 0.0334, Val Acc: 0.9770, Val F1: 0.9840\n",
            "Epoch: 52/75, Train Loss: 0.0225, Train Acc: 0.9855, Train F1: 0.9898 Val Loss: 0.0324, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 53/75, Train Loss: 0.0222, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0317, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 54/75, Train Loss: 0.0214, Train Acc: 0.9879, Train F1: 0.9916 Val Loss: 0.0324, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 55/75, Train Loss: 0.0216, Train Acc: 0.9873, Train F1: 0.9911 Val Loss: 0.0329, Val Acc: 0.9732, Val F1: 0.9816\n",
            "Epoch: 56/75, Train Loss: 0.0215, Train Acc: 0.9873, Train F1: 0.9911 Val Loss: 0.0321, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 57/75, Train Loss: 0.0220, Train Acc: 0.9869, Train F1: 0.9910 Val Loss: 0.0327, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 58/75, Train Loss: 0.0210, Train Acc: 0.9885, Train F1: 0.9920 Val Loss: 0.0316, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 59/75, Train Loss: 0.0214, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0317, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 60/75, Train Loss: 0.0215, Train Acc: 0.9865, Train F1: 0.9906 Val Loss: 0.0323, Val Acc: 0.9749, Val F1: 0.9825\n",
            "Epoch: 61/75, Train Loss: 0.0208, Train Acc: 0.9884, Train F1: 0.9920 Val Loss: 0.0313, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 62/75, Train Loss: 0.0208, Train Acc: 0.9880, Train F1: 0.9917 Val Loss: 0.0315, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 63/75, Train Loss: 0.0204, Train Acc: 0.9881, Train F1: 0.9917 Val Loss: 0.0317, Val Acc: 0.9770, Val F1: 0.9840\n",
            "Epoch: 64/75, Train Loss: 0.0205, Train Acc: 0.9884, Train F1: 0.9920 Val Loss: 0.0310, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 65/75, Train Loss: 0.0204, Train Acc: 0.9887, Train F1: 0.9922 Val Loss: 0.0314, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 66/75, Train Loss: 0.0204, Train Acc: 0.9885, Train F1: 0.9920 Val Loss: 0.0315, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 67/75, Train Loss: 0.0203, Train Acc: 0.9892, Train F1: 0.9926 Val Loss: 0.0312, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 68/75, Train Loss: 0.0203, Train Acc: 0.9888, Train F1: 0.9922 Val Loss: 0.0311, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 69/75, Train Loss: 0.0202, Train Acc: 0.9889, Train F1: 0.9923 Val Loss: 0.0312, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 70/75, Train Loss: 0.0202, Train Acc: 0.9892, Train F1: 0.9924 Val Loss: 0.0311, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 71/75, Train Loss: 0.0201, Train Acc: 0.9896, Train F1: 0.9928 Val Loss: 0.0310, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 72/75, Train Loss: 0.0201, Train Acc: 0.9895, Train F1: 0.9927 Val Loss: 0.0311, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 73/75, Train Loss: 0.0200, Train Acc: 0.9893, Train F1: 0.9925 Val Loss: 0.0311, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 74/75, Train Loss: 0.0200, Train Acc: 0.9897, Train F1: 0.9928 Val Loss: 0.0310, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 75/75, Train Loss: 0.0200, Train Acc: 0.9897, Train F1: 0.9928 Val Loss: 0.0310, Val Acc: 0.9780, Val F1: 0.9848\n",
            "\n",
            " 🔎 search 7 : deep_rescnn --- lr : 0.0008753516269381944, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv5.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1334, Train Acc: 0.8507, Train F1: 0.8999 Val Loss: 0.0999, Val Acc: 0.9021, Val F1: 0.9314\n",
            "Epoch: 2/75, Train Loss: 0.0832, Train Acc: 0.9170, Train F1: 0.9424 Val Loss: 0.0793, Val Acc: 0.9162, Val F1: 0.9440\n",
            "Epoch: 3/75, Train Loss: 0.0721, Train Acc: 0.9275, Train F1: 0.9496 Val Loss: 0.0667, Val Acc: 0.9378, Val F1: 0.9569\n",
            "Epoch: 4/75, Train Loss: 0.0630, Train Acc: 0.9379, Train F1: 0.9567 Val Loss: 0.0656, Val Acc: 0.9447, Val F1: 0.9625\n",
            "Epoch: 5/75, Train Loss: 0.0553, Train Acc: 0.9472, Train F1: 0.9633 Val Loss: 0.0565, Val Acc: 0.9546, Val F1: 0.9690\n",
            "Epoch: 6/75, Train Loss: 0.0490, Train Acc: 0.9561, Train F1: 0.9694 Val Loss: 0.0518, Val Acc: 0.9543, Val F1: 0.9682\n",
            "Epoch: 7/75, Train Loss: 0.0484, Train Acc: 0.9543, Train F1: 0.9683 Val Loss: 0.0477, Val Acc: 0.9615, Val F1: 0.9734\n",
            "Epoch: 8/75, Train Loss: 0.0449, Train Acc: 0.9616, Train F1: 0.9734 Val Loss: 0.0472, Val Acc: 0.9546, Val F1: 0.9685\n",
            "Epoch: 9/75, Train Loss: 0.0416, Train Acc: 0.9628, Train F1: 0.9741 Val Loss: 0.0543, Val Acc: 0.9457, Val F1: 0.9614\n",
            "Epoch: 10/75, Train Loss: 0.0406, Train Acc: 0.9644, Train F1: 0.9752 Val Loss: 0.0434, Val Acc: 0.9649, Val F1: 0.9760\n",
            "Epoch: 11/75, Train Loss: 0.0387, Train Acc: 0.9674, Train F1: 0.9774 Val Loss: 0.0413, Val Acc: 0.9677, Val F1: 0.9776\n",
            "Epoch: 12/75, Train Loss: 0.0365, Train Acc: 0.9702, Train F1: 0.9792 Val Loss: 0.0443, Val Acc: 0.9660, Val F1: 0.9765\n",
            "Epoch: 13/75, Train Loss: 0.0374, Train Acc: 0.9690, Train F1: 0.9784 Val Loss: 0.0461, Val Acc: 0.9619, Val F1: 0.9741\n",
            "Epoch: 14/75, Train Loss: 0.0340, Train Acc: 0.9747, Train F1: 0.9825 Val Loss: 0.0412, Val Acc: 0.9715, Val F1: 0.9804\n",
            "Epoch: 15/75, Train Loss: 0.0329, Train Acc: 0.9743, Train F1: 0.9821 Val Loss: 0.0446, Val Acc: 0.9570, Val F1: 0.9698\n",
            "Epoch: 16/75, Train Loss: 0.0317, Train Acc: 0.9770, Train F1: 0.9840 Val Loss: 0.0386, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 17/75, Train Loss: 0.0317, Train Acc: 0.9771, Train F1: 0.9839 Val Loss: 0.0435, Val Acc: 0.9584, Val F1: 0.9707\n",
            "Epoch: 18/75, Train Loss: 0.0331, Train Acc: 0.9738, Train F1: 0.9818 Val Loss: 0.0369, Val Acc: 0.9718, Val F1: 0.9807\n",
            "Epoch: 19/75, Train Loss: 0.0311, Train Acc: 0.9765, Train F1: 0.9837 Val Loss: 0.0360, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 20/75, Train Loss: 0.0293, Train Acc: 0.9786, Train F1: 0.9851 Val Loss: 0.0376, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 21/75, Train Loss: 0.0281, Train Acc: 0.9804, Train F1: 0.9863 Val Loss: 0.0362, Val Acc: 0.9667, Val F1: 0.9767\n",
            "Epoch: 22/75, Train Loss: 0.0292, Train Acc: 0.9794, Train F1: 0.9857 Val Loss: 0.0366, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 23/75, Train Loss: 0.0288, Train Acc: 0.9808, Train F1: 0.9866 Val Loss: 0.0361, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 24/75, Train Loss: 0.0267, Train Acc: 0.9829, Train F1: 0.9881 Val Loss: 0.0354, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 25/75, Train Loss: 0.0310, Train Acc: 0.9731, Train F1: 0.9814 Val Loss: 0.0460, Val Acc: 0.9543, Val F1: 0.9677\n",
            "Epoch: 26/75, Train Loss: 0.0273, Train Acc: 0.9803, Train F1: 0.9863 Val Loss: 0.0332, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 27/75, Train Loss: 0.0273, Train Acc: 0.9817, Train F1: 0.9872 Val Loss: 0.0354, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 28/75, Train Loss: 0.0263, Train Acc: 0.9830, Train F1: 0.9882 Val Loss: 0.0416, Val Acc: 0.9639, Val F1: 0.9755\n",
            "Epoch: 29/75, Train Loss: 0.0266, Train Acc: 0.9821, Train F1: 0.9876 Val Loss: 0.0381, Val Acc: 0.9691, Val F1: 0.9789\n",
            "Epoch: 30/75, Train Loss: 0.0316, Train Acc: 0.9740, Train F1: 0.9820 Val Loss: 0.0387, Val Acc: 0.9632, Val F1: 0.9741\n",
            "Epoch: 31/75, Train Loss: 0.0258, Train Acc: 0.9835, Train F1: 0.9886 Val Loss: 0.0332, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 32/75, Train Loss: 0.0250, Train Acc: 0.9835, Train F1: 0.9886 Val Loss: 0.0456, Val Acc: 0.9629, Val F1: 0.9749\n",
            "Epoch: 33/75, Train Loss: 0.0267, Train Acc: 0.9819, Train F1: 0.9874 Val Loss: 0.0325, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 34/75, Train Loss: 0.0262, Train Acc: 0.9814, Train F1: 0.9871 Val Loss: 0.0334, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 35/75, Train Loss: 0.0259, Train Acc: 0.9813, Train F1: 0.9869 Val Loss: 0.0370, Val Acc: 0.9649, Val F1: 0.9754\n",
            "Epoch: 36/75, Train Loss: 0.0242, Train Acc: 0.9843, Train F1: 0.9891 Val Loss: 0.0319, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 37/75, Train Loss: 0.0232, Train Acc: 0.9856, Train F1: 0.9900 Val Loss: 0.0326, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 38/75, Train Loss: 0.0237, Train Acc: 0.9857, Train F1: 0.9901 Val Loss: 0.0321, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 39/75, Train Loss: 0.0236, Train Acc: 0.9866, Train F1: 0.9907 Val Loss: 0.0391, Val Acc: 0.9701, Val F1: 0.9797\n",
            "Epoch: 40/75, Train Loss: 0.0239, Train Acc: 0.9861, Train F1: 0.9904 Val Loss: 0.0325, Val Acc: 0.9753, Val F1: 0.9828\n",
            "Epoch: 41/75, Train Loss: 0.0225, Train Acc: 0.9874, Train F1: 0.9912 Val Loss: 0.0314, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 42/75, Train Loss: 0.0230, Train Acc: 0.9865, Train F1: 0.9906 Val Loss: 0.0327, Val Acc: 0.9739, Val F1: 0.9818\n",
            "Epoch: 43/75, Train Loss: 0.0242, Train Acc: 0.9832, Train F1: 0.9882 Val Loss: 0.0562, Val Acc: 0.9450, Val F1: 0.9633\n",
            "Epoch: 44/75, Train Loss: 0.0253, Train Acc: 0.9824, Train F1: 0.9877 Val Loss: 0.0318, Val Acc: 0.9777, Val F1: 0.9847\n",
            "Epoch: 45/75, Train Loss: 0.0225, Train Acc: 0.9856, Train F1: 0.9900 Val Loss: 0.0326, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 46/75, Train Loss: 0.0235, Train Acc: 0.9858, Train F1: 0.9901 Val Loss: 0.0324, Val Acc: 0.9759, Val F1: 0.9835\n",
            "Epoch: 47/75, Train Loss: 0.0223, Train Acc: 0.9871, Train F1: 0.9910 Val Loss: 0.0308, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 48/75, Train Loss: 0.0220, Train Acc: 0.9881, Train F1: 0.9917 Val Loss: 0.0302, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 49/75, Train Loss: 0.0222, Train Acc: 0.9875, Train F1: 0.9913 Val Loss: 0.0316, Val Acc: 0.9746, Val F1: 0.9823\n",
            "Epoch: 50/75, Train Loss: 0.0216, Train Acc: 0.9881, Train F1: 0.9917 Val Loss: 0.0307, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 51/75, Train Loss: 0.0216, Train Acc: 0.9887, Train F1: 0.9921 Val Loss: 0.0317, Val Acc: 0.9780, Val F1: 0.9849\n",
            "Epoch: 52/75, Train Loss: 0.0214, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0313, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 53/75, Train Loss: 0.0210, Train Acc: 0.9903, Train F1: 0.9932 Val Loss: 0.0335, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 54/75, Train Loss: 0.0213, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0298, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 55/75, Train Loss: 0.0209, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0300, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 56/75, Train Loss: 0.0207, Train Acc: 0.9891, Train F1: 0.9924 Val Loss: 0.0299, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 57/75, Train Loss: 0.0209, Train Acc: 0.9888, Train F1: 0.9922 Val Loss: 0.0302, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 58/75, Train Loss: 0.0207, Train Acc: 0.9892, Train F1: 0.9925 Val Loss: 0.0335, Val Acc: 0.9715, Val F1: 0.9801\n",
            "Epoch: 59/75, Train Loss: 0.0210, Train Acc: 0.9890, Train F1: 0.9924 Val Loss: 0.0301, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 60/75, Train Loss: 0.0204, Train Acc: 0.9903, Train F1: 0.9933 Val Loss: 0.0297, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 61/75, Train Loss: 0.0205, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0297, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 62/75, Train Loss: 0.0202, Train Acc: 0.9906, Train F1: 0.9934 Val Loss: 0.0319, Val Acc: 0.9773, Val F1: 0.9845\n",
            "Epoch: 63/75, Train Loss: 0.0202, Train Acc: 0.9911, Train F1: 0.9938 Val Loss: 0.0297, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 64/75, Train Loss: 0.0201, Train Acc: 0.9912, Train F1: 0.9939 Val Loss: 0.0299, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 65/75, Train Loss: 0.0200, Train Acc: 0.9906, Train F1: 0.9935 Val Loss: 0.0298, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 66/75, Train Loss: 0.0202, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0296, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 67/75, Train Loss: 0.0200, Train Acc: 0.9901, Train F1: 0.9931 Val Loss: 0.0296, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 68/75, Train Loss: 0.0199, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0297, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 69/75, Train Loss: 0.0200, Train Acc: 0.9903, Train F1: 0.9932 Val Loss: 0.0298, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 70/75, Train Loss: 0.0198, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0296, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 71/75, Train Loss: 0.0198, Train Acc: 0.9913, Train F1: 0.9939 Val Loss: 0.0296, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 72/75, Train Loss: 0.0197, Train Acc: 0.9905, Train F1: 0.9933 Val Loss: 0.0296, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 73/75, Train Loss: 0.0197, Train Acc: 0.9901, Train F1: 0.9931 Val Loss: 0.0296, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 74/75, Train Loss: 0.0196, Train Acc: 0.9907, Train F1: 0.9937 Val Loss: 0.0296, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 75/75, Train Loss: 0.0196, Train Acc: 0.9907, Train F1: 0.9936 Val Loss: 0.0296, Val Acc: 0.9780, Val F1: 0.9848\n",
            "\n",
            " 🔎 search 8 : deep_rescnn --- lr : 0.001981796079782972, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv2.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1225, Train Acc: 0.8721, Train F1: 0.9124 Val Loss: 0.1096, Val Acc: 0.8918, Val F1: 0.9235\n",
            "Epoch: 2/75, Train Loss: 0.1093, Train Acc: 0.8892, Train F1: 0.9231 Val Loss: 0.1098, Val Acc: 0.8955, Val F1: 0.9264\n",
            "Epoch: 3/75, Train Loss: 0.1009, Train Acc: 0.9005, Train F1: 0.9306 Val Loss: 0.1064, Val Acc: 0.9010, Val F1: 0.9324\n",
            "Epoch: 4/75, Train Loss: 0.0950, Train Acc: 0.9077, Train F1: 0.9357 Val Loss: 0.1018, Val Acc: 0.9045, Val F1: 0.9321\n",
            "Epoch: 5/75, Train Loss: 0.0990, Train Acc: 0.9005, Train F1: 0.9306 Val Loss: 0.1059, Val Acc: 0.8966, Val F1: 0.9320\n",
            "Epoch: 6/75, Train Loss: 0.0892, Train Acc: 0.9159, Train F1: 0.9417 Val Loss: 0.0925, Val Acc: 0.9158, Val F1: 0.9439\n",
            "Epoch: 7/75, Train Loss: 0.0887, Train Acc: 0.9126, Train F1: 0.9394 Val Loss: 0.0936, Val Acc: 0.9072, Val F1: 0.9340\n",
            "Epoch: 8/75, Train Loss: 0.0869, Train Acc: 0.9197, Train F1: 0.9439 Val Loss: 0.0908, Val Acc: 0.9110, Val F1: 0.9400\n",
            "Epoch: 9/75, Train Loss: 0.0826, Train Acc: 0.9223, Train F1: 0.9461 Val Loss: 0.0846, Val Acc: 0.9251, Val F1: 0.9488\n",
            "Epoch: 10/75, Train Loss: 0.0843, Train Acc: 0.9219, Train F1: 0.9458 Val Loss: 0.0872, Val Acc: 0.9179, Val F1: 0.9434\n",
            "Epoch: 11/75, Train Loss: 0.0806, Train Acc: 0.9238, Train F1: 0.9471 Val Loss: 0.0955, Val Acc: 0.9065, Val F1: 0.9361\n",
            "Epoch: 12/75, Train Loss: 0.0792, Train Acc: 0.9241, Train F1: 0.9474 Val Loss: 0.0843, Val Acc: 0.9210, Val F1: 0.9442\n",
            "Epoch: 13/75, Train Loss: 0.0778, Train Acc: 0.9276, Train F1: 0.9494 Val Loss: 0.0776, Val Acc: 0.9261, Val F1: 0.9487\n",
            "Epoch: 14/75, Train Loss: 0.0754, Train Acc: 0.9296, Train F1: 0.9513 Val Loss: 0.0849, Val Acc: 0.9223, Val F1: 0.9462\n",
            "Epoch: 15/75, Train Loss: 0.0747, Train Acc: 0.9306, Train F1: 0.9516 Val Loss: 0.0832, Val Acc: 0.9172, Val F1: 0.9427\n",
            "Epoch: 16/75, Train Loss: 0.0739, Train Acc: 0.9294, Train F1: 0.9508 Val Loss: 0.0746, Val Acc: 0.9337, Val F1: 0.9546\n",
            "Epoch: 17/75, Train Loss: 0.0716, Train Acc: 0.9361, Train F1: 0.9553 Val Loss: 0.0742, Val Acc: 0.9326, Val F1: 0.9530\n",
            "Epoch: 18/75, Train Loss: 0.0706, Train Acc: 0.9364, Train F1: 0.9557 Val Loss: 0.0752, Val Acc: 0.9265, Val F1: 0.9491\n",
            "Epoch: 19/75, Train Loss: 0.0744, Train Acc: 0.9312, Train F1: 0.9523 Val Loss: 0.0846, Val Acc: 0.9137, Val F1: 0.9391\n",
            "Epoch: 20/75, Train Loss: 0.0714, Train Acc: 0.9364, Train F1: 0.9558 Val Loss: 0.0772, Val Acc: 0.9292, Val F1: 0.9521\n",
            "Epoch: 21/75, Train Loss: 0.0713, Train Acc: 0.9354, Train F1: 0.9550 Val Loss: 0.0710, Val Acc: 0.9371, Val F1: 0.9567\n",
            "Epoch: 22/75, Train Loss: 0.0689, Train Acc: 0.9363, Train F1: 0.9556 Val Loss: 0.0797, Val Acc: 0.9213, Val F1: 0.9473\n",
            "Epoch: 23/75, Train Loss: 0.0693, Train Acc: 0.9364, Train F1: 0.9555 Val Loss: 0.0823, Val Acc: 0.9206, Val F1: 0.9474\n",
            "Epoch: 24/75, Train Loss: 0.0680, Train Acc: 0.9373, Train F1: 0.9562 Val Loss: 0.0689, Val Acc: 0.9399, Val F1: 0.9584\n",
            "Epoch: 25/75, Train Loss: 0.0670, Train Acc: 0.9375, Train F1: 0.9566 Val Loss: 0.0833, Val Acc: 0.9244, Val F1: 0.9494\n",
            "Epoch: 26/75, Train Loss: 0.0662, Train Acc: 0.9381, Train F1: 0.9571 Val Loss: 0.0775, Val Acc: 0.9271, Val F1: 0.9493\n",
            "Epoch: 27/75, Train Loss: 0.0636, Train Acc: 0.9415, Train F1: 0.9591 Val Loss: 0.0782, Val Acc: 0.9316, Val F1: 0.9542\n",
            "Epoch: 28/75, Train Loss: 0.0645, Train Acc: 0.9403, Train F1: 0.9586 Val Loss: 0.0749, Val Acc: 0.9254, Val F1: 0.9473\n",
            "Epoch: 29/75, Train Loss: 0.0631, Train Acc: 0.9430, Train F1: 0.9602 Val Loss: 0.0686, Val Acc: 0.9381, Val F1: 0.9571\n",
            "Epoch: 30/75, Train Loss: 0.0620, Train Acc: 0.9436, Train F1: 0.9607 Val Loss: 0.0770, Val Acc: 0.9302, Val F1: 0.9534\n",
            "Epoch: 31/75, Train Loss: 0.0650, Train Acc: 0.9401, Train F1: 0.9583 Val Loss: 0.0682, Val Acc: 0.9412, Val F1: 0.9596\n",
            "Epoch: 32/75, Train Loss: 0.0620, Train Acc: 0.9434, Train F1: 0.9606 Val Loss: 0.0658, Val Acc: 0.9392, Val F1: 0.9580\n",
            "Epoch: 33/75, Train Loss: 0.0626, Train Acc: 0.9426, Train F1: 0.9600 Val Loss: 0.0675, Val Acc: 0.9399, Val F1: 0.9588\n",
            "Epoch: 34/75, Train Loss: 0.0594, Train Acc: 0.9457, Train F1: 0.9623 Val Loss: 0.0646, Val Acc: 0.9426, Val F1: 0.9604\n",
            "Epoch: 35/75, Train Loss: 0.0586, Train Acc: 0.9465, Train F1: 0.9629 Val Loss: 0.0643, Val Acc: 0.9419, Val F1: 0.9597\n",
            "Epoch: 36/75, Train Loss: 0.0592, Train Acc: 0.9462, Train F1: 0.9627 Val Loss: 0.0664, Val Acc: 0.9371, Val F1: 0.9560\n",
            "Epoch: 37/75, Train Loss: 0.0571, Train Acc: 0.9479, Train F1: 0.9638 Val Loss: 0.0701, Val Acc: 0.9354, Val F1: 0.9550\n",
            "Epoch: 38/75, Train Loss: 0.0582, Train Acc: 0.9491, Train F1: 0.9647 Val Loss: 0.0647, Val Acc: 0.9402, Val F1: 0.9591\n",
            "Epoch: 39/75, Train Loss: 0.0601, Train Acc: 0.9442, Train F1: 0.9610 Val Loss: 0.0646, Val Acc: 0.9440, Val F1: 0.9618\n",
            "Epoch: 40/75, Train Loss: 0.0583, Train Acc: 0.9482, Train F1: 0.9642 Val Loss: 0.0630, Val Acc: 0.9392, Val F1: 0.9577\n",
            "Epoch: 41/75, Train Loss: 0.0559, Train Acc: 0.9518, Train F1: 0.9665 Val Loss: 0.0625, Val Acc: 0.9430, Val F1: 0.9606\n",
            "Epoch: 42/75, Train Loss: 0.0548, Train Acc: 0.9509, Train F1: 0.9659 Val Loss: 0.0634, Val Acc: 0.9405, Val F1: 0.9585\n",
            "Epoch: 43/75, Train Loss: 0.0549, Train Acc: 0.9509, Train F1: 0.9660 Val Loss: 0.0612, Val Acc: 0.9419, Val F1: 0.9596\n",
            "Epoch: 44/75, Train Loss: 0.0541, Train Acc: 0.9501, Train F1: 0.9652 Val Loss: 0.0659, Val Acc: 0.9361, Val F1: 0.9552\n",
            "Epoch: 45/75, Train Loss: 0.0531, Train Acc: 0.9505, Train F1: 0.9655 Val Loss: 0.0636, Val Acc: 0.9409, Val F1: 0.9586\n",
            "Epoch: 46/75, Train Loss: 0.0535, Train Acc: 0.9520, Train F1: 0.9666 Val Loss: 0.0630, Val Acc: 0.9405, Val F1: 0.9586\n",
            "Epoch: 47/75, Train Loss: 0.0527, Train Acc: 0.9520, Train F1: 0.9667 Val Loss: 0.0604, Val Acc: 0.9464, Val F1: 0.9628\n",
            "Epoch: 48/75, Train Loss: 0.0512, Train Acc: 0.9527, Train F1: 0.9669 Val Loss: 0.0575, Val Acc: 0.9505, Val F1: 0.9662\n",
            "Epoch: 49/75, Train Loss: 0.0522, Train Acc: 0.9512, Train F1: 0.9660 Val Loss: 0.0583, Val Acc: 0.9491, Val F1: 0.9651\n",
            "Epoch: 50/75, Train Loss: 0.0521, Train Acc: 0.9517, Train F1: 0.9663 Val Loss: 0.0631, Val Acc: 0.9440, Val F1: 0.9623\n",
            "Epoch: 51/75, Train Loss: 0.0505, Train Acc: 0.9540, Train F1: 0.9679 Val Loss: 0.0579, Val Acc: 0.9481, Val F1: 0.9646\n",
            "Epoch: 52/75, Train Loss: 0.0516, Train Acc: 0.9534, Train F1: 0.9678 Val Loss: 0.0583, Val Acc: 0.9498, Val F1: 0.9653\n",
            "Epoch: 53/75, Train Loss: 0.0486, Train Acc: 0.9554, Train F1: 0.9690 Val Loss: 0.0612, Val Acc: 0.9474, Val F1: 0.9639\n",
            "Epoch: 54/75, Train Loss: 0.0487, Train Acc: 0.9551, Train F1: 0.9687 Val Loss: 0.0591, Val Acc: 0.9509, Val F1: 0.9666\n",
            "Epoch: 55/75, Train Loss: 0.0494, Train Acc: 0.9546, Train F1: 0.9684 Val Loss: 0.0567, Val Acc: 0.9505, Val F1: 0.9658\n",
            "Epoch: 56/75, Train Loss: 0.0479, Train Acc: 0.9574, Train F1: 0.9704 Val Loss: 0.0550, Val Acc: 0.9543, Val F1: 0.9687\n",
            "Epoch: 57/75, Train Loss: 0.0477, Train Acc: 0.9562, Train F1: 0.9697 Val Loss: 0.0548, Val Acc: 0.9515, Val F1: 0.9669\n",
            "Epoch: 58/75, Train Loss: 0.0475, Train Acc: 0.9558, Train F1: 0.9694 Val Loss: 0.0551, Val Acc: 0.9512, Val F1: 0.9664\n",
            "Epoch: 59/75, Train Loss: 0.0471, Train Acc: 0.9569, Train F1: 0.9702 Val Loss: 0.0555, Val Acc: 0.9543, Val F1: 0.9688\n",
            "Epoch: 60/75, Train Loss: 0.0463, Train Acc: 0.9589, Train F1: 0.9712 Val Loss: 0.0547, Val Acc: 0.9536, Val F1: 0.9680\n",
            "Epoch: 61/75, Train Loss: 0.0462, Train Acc: 0.9573, Train F1: 0.9703 Val Loss: 0.0543, Val Acc: 0.9540, Val F1: 0.9685\n",
            "Epoch: 62/75, Train Loss: 0.0457, Train Acc: 0.9581, Train F1: 0.9709 Val Loss: 0.0535, Val Acc: 0.9546, Val F1: 0.9688\n",
            "Epoch: 63/75, Train Loss: 0.0454, Train Acc: 0.9592, Train F1: 0.9717 Val Loss: 0.0534, Val Acc: 0.9529, Val F1: 0.9676\n",
            "Epoch: 64/75, Train Loss: 0.0451, Train Acc: 0.9584, Train F1: 0.9710 Val Loss: 0.0534, Val Acc: 0.9522, Val F1: 0.9671\n",
            "Epoch: 65/75, Train Loss: 0.0446, Train Acc: 0.9592, Train F1: 0.9715 Val Loss: 0.0533, Val Acc: 0.9567, Val F1: 0.9703\n",
            "Epoch: 66/75, Train Loss: 0.0446, Train Acc: 0.9600, Train F1: 0.9723 Val Loss: 0.0533, Val Acc: 0.9546, Val F1: 0.9688\n",
            "Epoch: 67/75, Train Loss: 0.0443, Train Acc: 0.9598, Train F1: 0.9721 Val Loss: 0.0533, Val Acc: 0.9546, Val F1: 0.9688\n",
            "Epoch: 68/75, Train Loss: 0.0440, Train Acc: 0.9614, Train F1: 0.9732 Val Loss: 0.0532, Val Acc: 0.9550, Val F1: 0.9691\n",
            "Epoch: 69/75, Train Loss: 0.0438, Train Acc: 0.9604, Train F1: 0.9725 Val Loss: 0.0526, Val Acc: 0.9546, Val F1: 0.9688\n",
            "Epoch: 70/75, Train Loss: 0.0437, Train Acc: 0.9609, Train F1: 0.9727 Val Loss: 0.0526, Val Acc: 0.9557, Val F1: 0.9696\n",
            "Epoch: 71/75, Train Loss: 0.0435, Train Acc: 0.9615, Train F1: 0.9734 Val Loss: 0.0526, Val Acc: 0.9560, Val F1: 0.9698\n",
            "Epoch: 72/75, Train Loss: 0.0434, Train Acc: 0.9614, Train F1: 0.9732 Val Loss: 0.0526, Val Acc: 0.9557, Val F1: 0.9696\n",
            "Epoch: 73/75, Train Loss: 0.0434, Train Acc: 0.9611, Train F1: 0.9730 Val Loss: 0.0525, Val Acc: 0.9546, Val F1: 0.9688\n",
            "Epoch: 74/75, Train Loss: 0.0433, Train Acc: 0.9614, Train F1: 0.9730 Val Loss: 0.0525, Val Acc: 0.9553, Val F1: 0.9693\n",
            "Epoch: 75/75, Train Loss: 0.0433, Train Acc: 0.9614, Train F1: 0.9731 Val Loss: 0.0525, Val Acc: 0.9553, Val F1: 0.9693\n",
            "\n",
            " 🔎 search 9 : deep_rescnn --- lr : 0.001981796079782972, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv3.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1131, Train Acc: 0.8794, Train F1: 0.9179 Val Loss: 0.1084, Val Acc: 0.8773, Val F1: 0.9115\n",
            "Epoch: 2/75, Train Loss: 0.0888, Train Acc: 0.9124, Train F1: 0.9388 Val Loss: 0.0901, Val Acc: 0.9271, Val F1: 0.9506\n",
            "Epoch: 3/75, Train Loss: 0.0862, Train Acc: 0.9176, Train F1: 0.9425 Val Loss: 0.0818, Val Acc: 0.9230, Val F1: 0.9456\n",
            "Epoch: 4/75, Train Loss: 0.0763, Train Acc: 0.9276, Train F1: 0.9494 Val Loss: 0.1017, Val Acc: 0.8814, Val F1: 0.9130\n",
            "Epoch: 5/75, Train Loss: 0.0718, Train Acc: 0.9322, Train F1: 0.9526 Val Loss: 0.0742, Val Acc: 0.9337, Val F1: 0.9552\n",
            "Epoch: 6/75, Train Loss: 0.0667, Train Acc: 0.9376, Train F1: 0.9565 Val Loss: 0.0722, Val Acc: 0.9385, Val F1: 0.9587\n",
            "Epoch: 7/75, Train Loss: 0.0657, Train Acc: 0.9377, Train F1: 0.9566 Val Loss: 0.0657, Val Acc: 0.9412, Val F1: 0.9601\n",
            "Epoch: 8/75, Train Loss: 0.0617, Train Acc: 0.9414, Train F1: 0.9592 Val Loss: 0.0697, Val Acc: 0.9313, Val F1: 0.9540\n",
            "Epoch: 9/75, Train Loss: 0.0632, Train Acc: 0.9393, Train F1: 0.9576 Val Loss: 0.0765, Val Acc: 0.9230, Val F1: 0.9453\n",
            "Epoch: 10/75, Train Loss: 0.0574, Train Acc: 0.9452, Train F1: 0.9617 Val Loss: 0.0590, Val Acc: 0.9447, Val F1: 0.9621\n",
            "Epoch: 11/75, Train Loss: 0.0563, Train Acc: 0.9448, Train F1: 0.9615 Val Loss: 0.0720, Val Acc: 0.9320, Val F1: 0.9524\n",
            "Epoch: 12/75, Train Loss: 0.0556, Train Acc: 0.9487, Train F1: 0.9641 Val Loss: 0.0572, Val Acc: 0.9481, Val F1: 0.9640\n",
            "Epoch: 13/75, Train Loss: 0.0556, Train Acc: 0.9467, Train F1: 0.9629 Val Loss: 0.0599, Val Acc: 0.9495, Val F1: 0.9655\n",
            "Epoch: 14/75, Train Loss: 0.0553, Train Acc: 0.9448, Train F1: 0.9613 Val Loss: 0.0588, Val Acc: 0.9491, Val F1: 0.9655\n",
            "Epoch: 15/75, Train Loss: 0.0531, Train Acc: 0.9468, Train F1: 0.9629 Val Loss: 0.0521, Val Acc: 0.9560, Val F1: 0.9699\n",
            "Epoch: 16/75, Train Loss: 0.0518, Train Acc: 0.9494, Train F1: 0.9647 Val Loss: 0.0655, Val Acc: 0.9340, Val F1: 0.9538\n",
            "Epoch: 17/75, Train Loss: 0.0510, Train Acc: 0.9506, Train F1: 0.9653 Val Loss: 0.0533, Val Acc: 0.9471, Val F1: 0.9631\n",
            "Epoch: 18/75, Train Loss: 0.0505, Train Acc: 0.9523, Train F1: 0.9668 Val Loss: 0.0732, Val Acc: 0.9344, Val F1: 0.9562\n",
            "Epoch: 19/75, Train Loss: 0.0487, Train Acc: 0.9522, Train F1: 0.9670 Val Loss: 0.0597, Val Acc: 0.9460, Val F1: 0.9619\n",
            "Epoch: 20/75, Train Loss: 0.0477, Train Acc: 0.9525, Train F1: 0.9670 Val Loss: 0.0570, Val Acc: 0.9474, Val F1: 0.9645\n",
            "Epoch: 21/75, Train Loss: 0.0479, Train Acc: 0.9518, Train F1: 0.9663 Val Loss: 0.0526, Val Acc: 0.9533, Val F1: 0.9684\n",
            "Epoch: 22/75, Train Loss: 0.0426, Train Acc: 0.9589, Train F1: 0.9711 Val Loss: 0.0460, Val Acc: 0.9574, Val F1: 0.9707\n",
            "Epoch: 23/75, Train Loss: 0.0447, Train Acc: 0.9580, Train F1: 0.9706 Val Loss: 0.0488, Val Acc: 0.9622, Val F1: 0.9737\n",
            "Epoch: 24/75, Train Loss: 0.0442, Train Acc: 0.9573, Train F1: 0.9703 Val Loss: 0.0497, Val Acc: 0.9533, Val F1: 0.9674\n",
            "Epoch: 25/75, Train Loss: 0.0466, Train Acc: 0.9551, Train F1: 0.9685 Val Loss: 0.0577, Val Acc: 0.9460, Val F1: 0.9637\n",
            "Epoch: 26/75, Train Loss: 0.0462, Train Acc: 0.9551, Train F1: 0.9686 Val Loss: 0.0471, Val Acc: 0.9584, Val F1: 0.9711\n",
            "Epoch: 27/75, Train Loss: 0.0429, Train Acc: 0.9592, Train F1: 0.9717 Val Loss: 0.0535, Val Acc: 0.9577, Val F1: 0.9704\n",
            "Epoch: 28/75, Train Loss: 0.0437, Train Acc: 0.9546, Train F1: 0.9685 Val Loss: 0.0482, Val Acc: 0.9584, Val F1: 0.9718\n",
            "Epoch: 29/75, Train Loss: 0.0412, Train Acc: 0.9620, Train F1: 0.9735 Val Loss: 0.0493, Val Acc: 0.9519, Val F1: 0.9663\n",
            "Epoch: 30/75, Train Loss: 0.0430, Train Acc: 0.9590, Train F1: 0.9712 Val Loss: 0.0453, Val Acc: 0.9615, Val F1: 0.9733\n",
            "Epoch: 31/75, Train Loss: 0.0452, Train Acc: 0.9568, Train F1: 0.9698 Val Loss: 0.0699, Val Acc: 0.9320, Val F1: 0.9513\n",
            "Epoch: 32/75, Train Loss: 0.0409, Train Acc: 0.9637, Train F1: 0.9747 Val Loss: 0.0475, Val Acc: 0.9584, Val F1: 0.9708\n",
            "Epoch: 33/75, Train Loss: 0.0406, Train Acc: 0.9627, Train F1: 0.9739 Val Loss: 0.0429, Val Acc: 0.9615, Val F1: 0.9733\n",
            "Epoch: 34/75, Train Loss: 0.0412, Train Acc: 0.9636, Train F1: 0.9746 Val Loss: 0.0461, Val Acc: 0.9601, Val F1: 0.9730\n",
            "Epoch: 35/75, Train Loss: 0.0388, Train Acc: 0.9663, Train F1: 0.9762 Val Loss: 0.0424, Val Acc: 0.9663, Val F1: 0.9768\n",
            "Epoch: 36/75, Train Loss: 0.0384, Train Acc: 0.9655, Train F1: 0.9759 Val Loss: 0.0476, Val Acc: 0.9567, Val F1: 0.9702\n",
            "Epoch: 37/75, Train Loss: 0.0389, Train Acc: 0.9636, Train F1: 0.9748 Val Loss: 0.0469, Val Acc: 0.9612, Val F1: 0.9734\n",
            "Epoch: 38/75, Train Loss: 0.0380, Train Acc: 0.9638, Train F1: 0.9745 Val Loss: 0.0439, Val Acc: 0.9625, Val F1: 0.9743\n",
            "Epoch: 39/75, Train Loss: 0.0374, Train Acc: 0.9674, Train F1: 0.9771 Val Loss: 0.0403, Val Acc: 0.9639, Val F1: 0.9752\n",
            "Epoch: 40/75, Train Loss: 0.0372, Train Acc: 0.9676, Train F1: 0.9775 Val Loss: 0.0520, Val Acc: 0.9512, Val F1: 0.9672\n",
            "Epoch: 41/75, Train Loss: 0.0361, Train Acc: 0.9675, Train F1: 0.9772 Val Loss: 0.0451, Val Acc: 0.9622, Val F1: 0.9743\n",
            "Epoch: 42/75, Train Loss: 0.0353, Train Acc: 0.9685, Train F1: 0.9781 Val Loss: 0.0418, Val Acc: 0.9643, Val F1: 0.9754\n",
            "Epoch: 43/75, Train Loss: 0.0369, Train Acc: 0.9688, Train F1: 0.9783 Val Loss: 0.0396, Val Acc: 0.9639, Val F1: 0.9753\n",
            "Epoch: 44/75, Train Loss: 0.0363, Train Acc: 0.9679, Train F1: 0.9775 Val Loss: 0.0400, Val Acc: 0.9694, Val F1: 0.9790\n",
            "Epoch: 45/75, Train Loss: 0.0350, Train Acc: 0.9688, Train F1: 0.9783 Val Loss: 0.0408, Val Acc: 0.9701, Val F1: 0.9793\n",
            "Epoch: 46/75, Train Loss: 0.0339, Train Acc: 0.9708, Train F1: 0.9797 Val Loss: 0.0416, Val Acc: 0.9632, Val F1: 0.9744\n",
            "Epoch: 47/75, Train Loss: 0.0358, Train Acc: 0.9682, Train F1: 0.9777 Val Loss: 0.0482, Val Acc: 0.9577, Val F1: 0.9714\n",
            "Epoch: 48/75, Train Loss: 0.0336, Train Acc: 0.9712, Train F1: 0.9799 Val Loss: 0.0387, Val Acc: 0.9649, Val F1: 0.9757\n",
            "Epoch: 49/75, Train Loss: 0.0325, Train Acc: 0.9738, Train F1: 0.9818 Val Loss: 0.0486, Val Acc: 0.9515, Val F1: 0.9658\n",
            "Epoch: 50/75, Train Loss: 0.0325, Train Acc: 0.9727, Train F1: 0.9810 Val Loss: 0.0365, Val Acc: 0.9677, Val F1: 0.9777\n",
            "Epoch: 51/75, Train Loss: 0.0320, Train Acc: 0.9737, Train F1: 0.9816 Val Loss: 0.0370, Val Acc: 0.9656, Val F1: 0.9763\n",
            "Epoch: 52/75, Train Loss: 0.0310, Train Acc: 0.9735, Train F1: 0.9816 Val Loss: 0.0392, Val Acc: 0.9698, Val F1: 0.9793\n",
            "Epoch: 53/75, Train Loss: 0.0306, Train Acc: 0.9762, Train F1: 0.9833 Val Loss: 0.0415, Val Acc: 0.9636, Val F1: 0.9753\n",
            "Epoch: 54/75, Train Loss: 0.0309, Train Acc: 0.9751, Train F1: 0.9826 Val Loss: 0.0397, Val Acc: 0.9691, Val F1: 0.9789\n",
            "Epoch: 55/75, Train Loss: 0.0311, Train Acc: 0.9739, Train F1: 0.9819 Val Loss: 0.0362, Val Acc: 0.9680, Val F1: 0.9781\n",
            "Epoch: 56/75, Train Loss: 0.0301, Train Acc: 0.9756, Train F1: 0.9830 Val Loss: 0.0369, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 57/75, Train Loss: 0.0296, Train Acc: 0.9765, Train F1: 0.9837 Val Loss: 0.0373, Val Acc: 0.9684, Val F1: 0.9784\n",
            "Epoch: 58/75, Train Loss: 0.0305, Train Acc: 0.9737, Train F1: 0.9816 Val Loss: 0.0369, Val Acc: 0.9656, Val F1: 0.9761\n",
            "Epoch: 59/75, Train Loss: 0.0298, Train Acc: 0.9773, Train F1: 0.9842 Val Loss: 0.0357, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 60/75, Train Loss: 0.0295, Train Acc: 0.9763, Train F1: 0.9834 Val Loss: 0.0354, Val Acc: 0.9725, Val F1: 0.9811\n",
            "Epoch: 61/75, Train Loss: 0.0288, Train Acc: 0.9780, Train F1: 0.9847 Val Loss: 0.0353, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 62/75, Train Loss: 0.0283, Train Acc: 0.9797, Train F1: 0.9858 Val Loss: 0.0353, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 63/75, Train Loss: 0.0283, Train Acc: 0.9793, Train F1: 0.9856 Val Loss: 0.0349, Val Acc: 0.9694, Val F1: 0.9788\n",
            "Epoch: 64/75, Train Loss: 0.0280, Train Acc: 0.9790, Train F1: 0.9854 Val Loss: 0.0344, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 65/75, Train Loss: 0.0274, Train Acc: 0.9798, Train F1: 0.9860 Val Loss: 0.0345, Val Acc: 0.9715, Val F1: 0.9804\n",
            "Epoch: 66/75, Train Loss: 0.0275, Train Acc: 0.9788, Train F1: 0.9854 Val Loss: 0.0353, Val Acc: 0.9756, Val F1: 0.9833\n",
            "Epoch: 67/75, Train Loss: 0.0274, Train Acc: 0.9812, Train F1: 0.9869 Val Loss: 0.0342, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 68/75, Train Loss: 0.0271, Train Acc: 0.9804, Train F1: 0.9863 Val Loss: 0.0343, Val Acc: 0.9725, Val F1: 0.9811\n",
            "Epoch: 69/75, Train Loss: 0.0269, Train Acc: 0.9817, Train F1: 0.9872 Val Loss: 0.0341, Val Acc: 0.9722, Val F1: 0.9808\n",
            "Epoch: 70/75, Train Loss: 0.0268, Train Acc: 0.9806, Train F1: 0.9866 Val Loss: 0.0345, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 71/75, Train Loss: 0.0268, Train Acc: 0.9804, Train F1: 0.9863 Val Loss: 0.0343, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 72/75, Train Loss: 0.0267, Train Acc: 0.9817, Train F1: 0.9872 Val Loss: 0.0340, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 73/75, Train Loss: 0.0266, Train Acc: 0.9811, Train F1: 0.9867 Val Loss: 0.0341, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 74/75, Train Loss: 0.0265, Train Acc: 0.9810, Train F1: 0.9867 Val Loss: 0.0341, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 75/75, Train Loss: 0.0265, Train Acc: 0.9811, Train F1: 0.9868 Val Loss: 0.0341, Val Acc: 0.9729, Val F1: 0.9813\n",
            "\n",
            " 🔎 search 10 : deep_rescnn --- lr : 0.001981796079782972, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv4.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1114, Train Acc: 0.8755, Train F1: 0.9111 Val Loss: 0.0916, Val Acc: 0.9110, Val F1: 0.9367\n",
            "Epoch: 2/75, Train Loss: 0.0785, Train Acc: 0.9213, Train F1: 0.9449 Val Loss: 0.0796, Val Acc: 0.9254, Val F1: 0.9491\n",
            "Epoch: 3/75, Train Loss: 0.0623, Train Acc: 0.9405, Train F1: 0.9584 Val Loss: 0.0667, Val Acc: 0.9381, Val F1: 0.9570\n",
            "Epoch: 4/75, Train Loss: 0.0625, Train Acc: 0.9397, Train F1: 0.9581 Val Loss: 0.0650, Val Acc: 0.9436, Val F1: 0.9616\n",
            "Epoch: 5/75, Train Loss: 0.0571, Train Acc: 0.9443, Train F1: 0.9612 Val Loss: 0.0598, Val Acc: 0.9388, Val F1: 0.9574\n",
            "Epoch: 6/75, Train Loss: 0.0586, Train Acc: 0.9411, Train F1: 0.9588 Val Loss: 0.0575, Val Acc: 0.9460, Val F1: 0.9633\n",
            "Epoch: 7/75, Train Loss: 0.0501, Train Acc: 0.9506, Train F1: 0.9655 Val Loss: 0.0529, Val Acc: 0.9436, Val F1: 0.9605\n",
            "Epoch: 8/75, Train Loss: 0.0497, Train Acc: 0.9504, Train F1: 0.9655 Val Loss: 0.0562, Val Acc: 0.9474, Val F1: 0.9634\n",
            "Epoch: 9/75, Train Loss: 0.0479, Train Acc: 0.9540, Train F1: 0.9678 Val Loss: 0.0499, Val Acc: 0.9512, Val F1: 0.9660\n",
            "Epoch: 10/75, Train Loss: 0.0440, Train Acc: 0.9556, Train F1: 0.9690 Val Loss: 0.0550, Val Acc: 0.9416, Val F1: 0.9589\n",
            "Epoch: 11/75, Train Loss: 0.0468, Train Acc: 0.9543, Train F1: 0.9683 Val Loss: 0.0498, Val Acc: 0.9570, Val F1: 0.9704\n",
            "Epoch: 12/75, Train Loss: 0.0411, Train Acc: 0.9622, Train F1: 0.9736 Val Loss: 0.0494, Val Acc: 0.9519, Val F1: 0.9672\n",
            "Epoch: 13/75, Train Loss: 0.0425, Train Acc: 0.9604, Train F1: 0.9723 Val Loss: 0.0465, Val Acc: 0.9595, Val F1: 0.9723\n",
            "Epoch: 14/75, Train Loss: 0.0388, Train Acc: 0.9639, Train F1: 0.9750 Val Loss: 0.0565, Val Acc: 0.9430, Val F1: 0.9616\n",
            "Epoch: 15/75, Train Loss: 0.0417, Train Acc: 0.9606, Train F1: 0.9724 Val Loss: 0.0431, Val Acc: 0.9667, Val F1: 0.9771\n",
            "Epoch: 16/75, Train Loss: 0.0399, Train Acc: 0.9645, Train F1: 0.9751 Val Loss: 0.0714, Val Acc: 0.9199, Val F1: 0.9473\n",
            "Epoch: 17/75, Train Loss: 0.0422, Train Acc: 0.9614, Train F1: 0.9730 Val Loss: 0.0404, Val Acc: 0.9653, Val F1: 0.9761\n",
            "Epoch: 18/75, Train Loss: 0.0374, Train Acc: 0.9668, Train F1: 0.9767 Val Loss: 0.0475, Val Acc: 0.9615, Val F1: 0.9738\n",
            "Epoch: 19/75, Train Loss: 0.0376, Train Acc: 0.9656, Train F1: 0.9760 Val Loss: 0.0390, Val Acc: 0.9684, Val F1: 0.9781\n",
            "Epoch: 20/75, Train Loss: 0.0394, Train Acc: 0.9637, Train F1: 0.9748 Val Loss: 0.0396, Val Acc: 0.9656, Val F1: 0.9762\n",
            "Epoch: 21/75, Train Loss: 0.0345, Train Acc: 0.9709, Train F1: 0.9798 Val Loss: 0.0486, Val Acc: 0.9540, Val F1: 0.9688\n",
            "Epoch: 22/75, Train Loss: 0.0350, Train Acc: 0.9708, Train F1: 0.9795 Val Loss: 0.0421, Val Acc: 0.9612, Val F1: 0.9735\n",
            "Epoch: 23/75, Train Loss: 0.0385, Train Acc: 0.9648, Train F1: 0.9755 Val Loss: 0.0413, Val Acc: 0.9643, Val F1: 0.9751\n",
            "Epoch: 24/75, Train Loss: 0.0347, Train Acc: 0.9696, Train F1: 0.9789 Val Loss: 0.0373, Val Acc: 0.9715, Val F1: 0.9802\n",
            "Epoch: 25/75, Train Loss: 0.0316, Train Acc: 0.9745, Train F1: 0.9822 Val Loss: 0.0461, Val Acc: 0.9577, Val F1: 0.9713\n",
            "Epoch: 26/75, Train Loss: 0.0340, Train Acc: 0.9704, Train F1: 0.9795 Val Loss: 0.0403, Val Acc: 0.9656, Val F1: 0.9761\n",
            "Epoch: 27/75, Train Loss: 0.0346, Train Acc: 0.9702, Train F1: 0.9789 Val Loss: 0.0388, Val Acc: 0.9653, Val F1: 0.9761\n",
            "Epoch: 28/75, Train Loss: 0.0318, Train Acc: 0.9748, Train F1: 0.9824 Val Loss: 0.0385, Val Acc: 0.9667, Val F1: 0.9768\n",
            "Epoch: 29/75, Train Loss: 0.0310, Train Acc: 0.9746, Train F1: 0.9822 Val Loss: 0.0418, Val Acc: 0.9608, Val F1: 0.9734\n",
            "Epoch: 30/75, Train Loss: 0.0314, Train Acc: 0.9746, Train F1: 0.9824 Val Loss: 0.0362, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 31/75, Train Loss: 0.0308, Train Acc: 0.9750, Train F1: 0.9827 Val Loss: 0.0393, Val Acc: 0.9674, Val F1: 0.9774\n",
            "Epoch: 32/75, Train Loss: 0.0284, Train Acc: 0.9771, Train F1: 0.9840 Val Loss: 0.0361, Val Acc: 0.9722, Val F1: 0.9807\n",
            "Epoch: 33/75, Train Loss: 0.0294, Train Acc: 0.9771, Train F1: 0.9841 Val Loss: 0.0390, Val Acc: 0.9677, Val F1: 0.9779\n",
            "Epoch: 34/75, Train Loss: 0.0291, Train Acc: 0.9764, Train F1: 0.9835 Val Loss: 0.0485, Val Acc: 0.9519, Val F1: 0.9675\n",
            "Epoch: 35/75, Train Loss: 0.0340, Train Acc: 0.9710, Train F1: 0.9799 Val Loss: 0.0462, Val Acc: 0.9560, Val F1: 0.9702\n",
            "Epoch: 36/75, Train Loss: 0.0291, Train Acc: 0.9774, Train F1: 0.9842 Val Loss: 0.0349, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 37/75, Train Loss: 0.0290, Train Acc: 0.9777, Train F1: 0.9843 Val Loss: 0.0396, Val Acc: 0.9667, Val F1: 0.9767\n",
            "Epoch: 38/75, Train Loss: 0.0268, Train Acc: 0.9814, Train F1: 0.9870 Val Loss: 0.0425, Val Acc: 0.9619, Val F1: 0.9732\n",
            "Epoch: 39/75, Train Loss: 0.0257, Train Acc: 0.9817, Train F1: 0.9872 Val Loss: 0.0328, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 40/75, Train Loss: 0.0279, Train Acc: 0.9787, Train F1: 0.9851 Val Loss: 0.0364, Val Acc: 0.9663, Val F1: 0.9764\n",
            "Epoch: 41/75, Train Loss: 0.0302, Train Acc: 0.9769, Train F1: 0.9835 Val Loss: 0.0351, Val Acc: 0.9715, Val F1: 0.9804\n",
            "Epoch: 42/75, Train Loss: 0.0261, Train Acc: 0.9804, Train F1: 0.9863 Val Loss: 0.0347, Val Acc: 0.9722, Val F1: 0.9807\n",
            "Epoch: 43/75, Train Loss: 0.0256, Train Acc: 0.9825, Train F1: 0.9878 Val Loss: 0.0334, Val Acc: 0.9729, Val F1: 0.9814\n",
            "Epoch: 44/75, Train Loss: 0.0249, Train Acc: 0.9826, Train F1: 0.9879 Val Loss: 0.0355, Val Acc: 0.9701, Val F1: 0.9792\n",
            "Epoch: 45/75, Train Loss: 0.0247, Train Acc: 0.9834, Train F1: 0.9884 Val Loss: 0.0344, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 46/75, Train Loss: 0.0244, Train Acc: 0.9841, Train F1: 0.9888 Val Loss: 0.0325, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 47/75, Train Loss: 0.0240, Train Acc: 0.9838, Train F1: 0.9888 Val Loss: 0.0317, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 48/75, Train Loss: 0.0244, Train Acc: 0.9836, Train F1: 0.9886 Val Loss: 0.0338, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 49/75, Train Loss: 0.0269, Train Acc: 0.9792, Train F1: 0.9855 Val Loss: 0.0330, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 50/75, Train Loss: 0.0244, Train Acc: 0.9821, Train F1: 0.9876 Val Loss: 0.0357, Val Acc: 0.9711, Val F1: 0.9802\n",
            "Epoch: 51/75, Train Loss: 0.0234, Train Acc: 0.9830, Train F1: 0.9881 Val Loss: 0.0326, Val Acc: 0.9759, Val F1: 0.9835\n",
            "Epoch: 52/75, Train Loss: 0.0244, Train Acc: 0.9835, Train F1: 0.9884 Val Loss: 0.0322, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 53/75, Train Loss: 0.0229, Train Acc: 0.9838, Train F1: 0.9886 Val Loss: 0.0313, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 54/75, Train Loss: 0.0217, Train Acc: 0.9864, Train F1: 0.9904 Val Loss: 0.0315, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 55/75, Train Loss: 0.0231, Train Acc: 0.9852, Train F1: 0.9897 Val Loss: 0.0317, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 56/75, Train Loss: 0.0230, Train Acc: 0.9843, Train F1: 0.9892 Val Loss: 0.0315, Val Acc: 0.9763, Val F1: 0.9835\n",
            "Epoch: 57/75, Train Loss: 0.0219, Train Acc: 0.9864, Train F1: 0.9904 Val Loss: 0.0310, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 58/75, Train Loss: 0.0219, Train Acc: 0.9863, Train F1: 0.9904 Val Loss: 0.0345, Val Acc: 0.9729, Val F1: 0.9814\n",
            "Epoch: 59/75, Train Loss: 0.0217, Train Acc: 0.9864, Train F1: 0.9904 Val Loss: 0.0328, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 60/75, Train Loss: 0.0211, Train Acc: 0.9875, Train F1: 0.9912 Val Loss: 0.0308, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 61/75, Train Loss: 0.0211, Train Acc: 0.9879, Train F1: 0.9915 Val Loss: 0.0310, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 62/75, Train Loss: 0.0212, Train Acc: 0.9880, Train F1: 0.9915 Val Loss: 0.0307, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 63/75, Train Loss: 0.0207, Train Acc: 0.9888, Train F1: 0.9923 Val Loss: 0.0306, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 64/75, Train Loss: 0.0206, Train Acc: 0.9882, Train F1: 0.9918 Val Loss: 0.0306, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 65/75, Train Loss: 0.0205, Train Acc: 0.9893, Train F1: 0.9925 Val Loss: 0.0307, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 66/75, Train Loss: 0.0200, Train Acc: 0.9900, Train F1: 0.9930 Val Loss: 0.0311, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 67/75, Train Loss: 0.0200, Train Acc: 0.9900, Train F1: 0.9931 Val Loss: 0.0309, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 68/75, Train Loss: 0.0199, Train Acc: 0.9889, Train F1: 0.9922 Val Loss: 0.0325, Val Acc: 0.9749, Val F1: 0.9828\n",
            "Epoch: 69/75, Train Loss: 0.0201, Train Acc: 0.9880, Train F1: 0.9915 Val Loss: 0.0304, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 70/75, Train Loss: 0.0198, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0303, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 71/75, Train Loss: 0.0198, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0303, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 72/75, Train Loss: 0.0196, Train Acc: 0.9900, Train F1: 0.9931 Val Loss: 0.0302, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 73/75, Train Loss: 0.0196, Train Acc: 0.9901, Train F1: 0.9931 Val Loss: 0.0303, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 74/75, Train Loss: 0.0195, Train Acc: 0.9906, Train F1: 0.9934 Val Loss: 0.0302, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 75/75, Train Loss: 0.0195, Train Acc: 0.9906, Train F1: 0.9934 Val Loss: 0.0303, Val Acc: 0.9777, Val F1: 0.9846\n",
            "\n",
            " 🔎 search 11 : deep_rescnn --- lr : 0.001981796079782972, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv5.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1102, Train Acc: 0.8729, Train F1: 0.9111 Val Loss: 0.0821, Val Acc: 0.9254, Val F1: 0.9496\n",
            "Epoch: 2/75, Train Loss: 0.0728, Train Acc: 0.9279, Train F1: 0.9496 Val Loss: 0.0677, Val Acc: 0.9347, Val F1: 0.9542\n",
            "Epoch: 3/75, Train Loss: 0.0662, Train Acc: 0.9315, Train F1: 0.9524 Val Loss: 0.0628, Val Acc: 0.9412, Val F1: 0.9596\n",
            "Epoch: 4/75, Train Loss: 0.0634, Train Acc: 0.9320, Train F1: 0.9527 Val Loss: 0.0573, Val Acc: 0.9529, Val F1: 0.9679\n",
            "Epoch: 5/75, Train Loss: 0.0569, Train Acc: 0.9410, Train F1: 0.9589 Val Loss: 0.0541, Val Acc: 0.9502, Val F1: 0.9649\n",
            "Epoch: 6/75, Train Loss: 0.0469, Train Acc: 0.9581, Train F1: 0.9708 Val Loss: 0.0654, Val Acc: 0.9364, Val F1: 0.9575\n",
            "Epoch: 7/75, Train Loss: 0.0526, Train Acc: 0.9498, Train F1: 0.9651 Val Loss: 0.0531, Val Acc: 0.9440, Val F1: 0.9605\n",
            "Epoch: 8/75, Train Loss: 0.0440, Train Acc: 0.9603, Train F1: 0.9722 Val Loss: 0.0543, Val Acc: 0.9505, Val F1: 0.9667\n",
            "Epoch: 9/75, Train Loss: 0.0478, Train Acc: 0.9549, Train F1: 0.9684 Val Loss: 0.0516, Val Acc: 0.9505, Val F1: 0.9652\n",
            "Epoch: 10/75, Train Loss: 0.0433, Train Acc: 0.9615, Train F1: 0.9732 Val Loss: 0.0517, Val Acc: 0.9412, Val F1: 0.9585\n",
            "Epoch: 11/75, Train Loss: 0.0409, Train Acc: 0.9611, Train F1: 0.9726 Val Loss: 0.0510, Val Acc: 0.9443, Val F1: 0.9609\n",
            "Epoch: 12/75, Train Loss: 0.0404, Train Acc: 0.9645, Train F1: 0.9752 Val Loss: 0.0445, Val Acc: 0.9598, Val F1: 0.9726\n",
            "Epoch: 13/75, Train Loss: 0.0376, Train Acc: 0.9658, Train F1: 0.9762 Val Loss: 0.0638, Val Acc: 0.9371, Val F1: 0.9581\n",
            "Epoch: 14/75, Train Loss: 0.0428, Train Acc: 0.9597, Train F1: 0.9719 Val Loss: 0.0385, Val Acc: 0.9711, Val F1: 0.9800\n",
            "Epoch: 15/75, Train Loss: 0.0394, Train Acc: 0.9637, Train F1: 0.9745 Val Loss: 0.0402, Val Acc: 0.9687, Val F1: 0.9783\n",
            "Epoch: 16/75, Train Loss: 0.0330, Train Acc: 0.9717, Train F1: 0.9804 Val Loss: 0.0599, Val Acc: 0.9385, Val F1: 0.9589\n",
            "Epoch: 17/75, Train Loss: 0.0374, Train Acc: 0.9648, Train F1: 0.9755 Val Loss: 0.0451, Val Acc: 0.9584, Val F1: 0.9707\n",
            "Epoch: 18/75, Train Loss: 0.0379, Train Acc: 0.9667, Train F1: 0.9766 Val Loss: 0.0416, Val Acc: 0.9646, Val F1: 0.9755\n",
            "Epoch: 19/75, Train Loss: 0.0333, Train Acc: 0.9730, Train F1: 0.9811 Val Loss: 0.0392, Val Acc: 0.9680, Val F1: 0.9779\n",
            "Epoch: 20/75, Train Loss: 0.0343, Train Acc: 0.9701, Train F1: 0.9789 Val Loss: 0.0367, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 21/75, Train Loss: 0.0354, Train Acc: 0.9687, Train F1: 0.9781 Val Loss: 0.0365, Val Acc: 0.9701, Val F1: 0.9794\n",
            "Epoch: 22/75, Train Loss: 0.0320, Train Acc: 0.9719, Train F1: 0.9804 Val Loss: 0.0360, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 23/75, Train Loss: 0.0318, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0390, Val Acc: 0.9653, Val F1: 0.9757\n",
            "Epoch: 24/75, Train Loss: 0.0331, Train Acc: 0.9717, Train F1: 0.9801 Val Loss: 0.0512, Val Acc: 0.9577, Val F1: 0.9715\n",
            "Epoch: 25/75, Train Loss: 0.0292, Train Acc: 0.9785, Train F1: 0.9850 Val Loss: 0.0348, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 26/75, Train Loss: 0.0315, Train Acc: 0.9763, Train F1: 0.9835 Val Loss: 0.0410, Val Acc: 0.9660, Val F1: 0.9768\n",
            "Epoch: 27/75, Train Loss: 0.0302, Train Acc: 0.9766, Train F1: 0.9836 Val Loss: 0.0349, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 28/75, Train Loss: 0.0319, Train Acc: 0.9733, Train F1: 0.9815 Val Loss: 0.0339, Val Acc: 0.9749, Val F1: 0.9826\n",
            "Epoch: 29/75, Train Loss: 0.0320, Train Acc: 0.9738, Train F1: 0.9818 Val Loss: 0.0458, Val Acc: 0.9512, Val F1: 0.9654\n",
            "Epoch: 30/75, Train Loss: 0.0296, Train Acc: 0.9769, Train F1: 0.9837 Val Loss: 0.0346, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 31/75, Train Loss: 0.0275, Train Acc: 0.9792, Train F1: 0.9855 Val Loss: 0.0367, Val Acc: 0.9718, Val F1: 0.9803\n",
            "Epoch: 32/75, Train Loss: 0.0281, Train Acc: 0.9782, Train F1: 0.9848 Val Loss: 0.0367, Val Acc: 0.9729, Val F1: 0.9811\n",
            "Epoch: 33/75, Train Loss: 0.0280, Train Acc: 0.9787, Train F1: 0.9851 Val Loss: 0.0365, Val Acc: 0.9718, Val F1: 0.9807\n",
            "Epoch: 34/75, Train Loss: 0.0274, Train Acc: 0.9786, Train F1: 0.9849 Val Loss: 0.0324, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 35/75, Train Loss: 0.0264, Train Acc: 0.9812, Train F1: 0.9868 Val Loss: 0.0340, Val Acc: 0.9729, Val F1: 0.9811\n",
            "Epoch: 36/75, Train Loss: 0.0272, Train Acc: 0.9786, Train F1: 0.9850 Val Loss: 0.0342, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 37/75, Train Loss: 0.0304, Train Acc: 0.9734, Train F1: 0.9815 Val Loss: 0.0357, Val Acc: 0.9715, Val F1: 0.9805\n",
            "Epoch: 38/75, Train Loss: 0.0279, Train Acc: 0.9786, Train F1: 0.9851 Val Loss: 0.0328, Val Acc: 0.9732, Val F1: 0.9814\n",
            "Epoch: 39/75, Train Loss: 0.0271, Train Acc: 0.9784, Train F1: 0.9848 Val Loss: 0.0346, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 40/75, Train Loss: 0.0287, Train Acc: 0.9775, Train F1: 0.9841 Val Loss: 0.0340, Val Acc: 0.9722, Val F1: 0.9807\n",
            "Epoch: 41/75, Train Loss: 0.0282, Train Acc: 0.9781, Train F1: 0.9845 Val Loss: 0.0346, Val Acc: 0.9722, Val F1: 0.9809\n",
            "Epoch: 42/75, Train Loss: 0.0271, Train Acc: 0.9802, Train F1: 0.9862 Val Loss: 0.0336, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 43/75, Train Loss: 0.0246, Train Acc: 0.9837, Train F1: 0.9887 Val Loss: 0.0384, Val Acc: 0.9680, Val F1: 0.9782\n",
            "Epoch: 44/75, Train Loss: 0.0249, Train Acc: 0.9818, Train F1: 0.9874 Val Loss: 0.0329, Val Acc: 0.9749, Val F1: 0.9825\n",
            "Epoch: 45/75, Train Loss: 0.0243, Train Acc: 0.9844, Train F1: 0.9889 Val Loss: 0.0398, Val Acc: 0.9636, Val F1: 0.9744\n",
            "Epoch: 46/75, Train Loss: 0.0271, Train Acc: 0.9802, Train F1: 0.9862 Val Loss: 0.0313, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 47/75, Train Loss: 0.0256, Train Acc: 0.9814, Train F1: 0.9870 Val Loss: 0.0334, Val Acc: 0.9718, Val F1: 0.9803\n",
            "Epoch: 48/75, Train Loss: 0.0245, Train Acc: 0.9836, Train F1: 0.9886 Val Loss: 0.0390, Val Acc: 0.9639, Val F1: 0.9746\n",
            "Epoch: 49/75, Train Loss: 0.0242, Train Acc: 0.9837, Train F1: 0.9885 Val Loss: 0.0321, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 50/75, Train Loss: 0.0230, Train Acc: 0.9868, Train F1: 0.9908 Val Loss: 0.0320, Val Acc: 0.9753, Val F1: 0.9828\n",
            "Epoch: 51/75, Train Loss: 0.0232, Train Acc: 0.9848, Train F1: 0.9894 Val Loss: 0.0313, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 52/75, Train Loss: 0.0232, Train Acc: 0.9852, Train F1: 0.9897 Val Loss: 0.0308, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 53/75, Train Loss: 0.0231, Train Acc: 0.9860, Train F1: 0.9903 Val Loss: 0.0312, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 54/75, Train Loss: 0.0226, Train Acc: 0.9851, Train F1: 0.9895 Val Loss: 0.0301, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 55/75, Train Loss: 0.0226, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0400, Val Acc: 0.9663, Val F1: 0.9771\n",
            "Epoch: 56/75, Train Loss: 0.0228, Train Acc: 0.9860, Train F1: 0.9902 Val Loss: 0.0310, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 57/75, Train Loss: 0.0226, Train Acc: 0.9859, Train F1: 0.9902 Val Loss: 0.0301, Val Acc: 0.9790, Val F1: 0.9854\n",
            "Epoch: 58/75, Train Loss: 0.0218, Train Acc: 0.9882, Train F1: 0.9917 Val Loss: 0.0337, Val Acc: 0.9746, Val F1: 0.9826\n",
            "Epoch: 59/75, Train Loss: 0.0224, Train Acc: 0.9863, Train F1: 0.9905 Val Loss: 0.0309, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 60/75, Train Loss: 0.0214, Train Acc: 0.9873, Train F1: 0.9911 Val Loss: 0.0301, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 61/75, Train Loss: 0.0213, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0298, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 62/75, Train Loss: 0.0211, Train Acc: 0.9875, Train F1: 0.9913 Val Loss: 0.0295, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 63/75, Train Loss: 0.0211, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0300, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 64/75, Train Loss: 0.0211, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0303, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 65/75, Train Loss: 0.0210, Train Acc: 0.9885, Train F1: 0.9921 Val Loss: 0.0311, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 66/75, Train Loss: 0.0206, Train Acc: 0.9889, Train F1: 0.9923 Val Loss: 0.0310, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 67/75, Train Loss: 0.0205, Train Acc: 0.9883, Train F1: 0.9918 Val Loss: 0.0297, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 68/75, Train Loss: 0.0205, Train Acc: 0.9882, Train F1: 0.9916 Val Loss: 0.0296, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 69/75, Train Loss: 0.0203, Train Acc: 0.9888, Train F1: 0.9922 Val Loss: 0.0300, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 70/75, Train Loss: 0.0203, Train Acc: 0.9887, Train F1: 0.9921 Val Loss: 0.0297, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 71/75, Train Loss: 0.0202, Train Acc: 0.9888, Train F1: 0.9921 Val Loss: 0.0297, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 72/75, Train Loss: 0.0201, Train Acc: 0.9893, Train F1: 0.9925 Val Loss: 0.0299, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 73/75, Train Loss: 0.0201, Train Acc: 0.9888, Train F1: 0.9921 Val Loss: 0.0296, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 74/75, Train Loss: 0.0200, Train Acc: 0.9888, Train F1: 0.9922 Val Loss: 0.0296, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 75/75, Train Loss: 0.0200, Train Acc: 0.9889, Train F1: 0.9923 Val Loss: 0.0296, Val Acc: 0.9777, Val F1: 0.9846\n",
            "\n",
            " 🔎 search 12 : deep_rescnn --- lr : 0.001981796079782972, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv2.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1211, Train Acc: 0.8690, Train F1: 0.9112 Val Loss: 0.1034, Val Acc: 0.9038, Val F1: 0.9329\n",
            "Epoch: 2/75, Train Loss: 0.0919, Train Acc: 0.9123, Train F1: 0.9392 Val Loss: 0.0879, Val Acc: 0.9179, Val F1: 0.9427\n",
            "Epoch: 3/75, Train Loss: 0.0884, Train Acc: 0.9158, Train F1: 0.9414 Val Loss: 0.0908, Val Acc: 0.9131, Val F1: 0.9417\n",
            "Epoch: 4/75, Train Loss: 0.0829, Train Acc: 0.9219, Train F1: 0.9458 Val Loss: 0.0904, Val Acc: 0.9113, Val F1: 0.9408\n",
            "Epoch: 5/75, Train Loss: 0.0786, Train Acc: 0.9292, Train F1: 0.9512 Val Loss: 0.0926, Val Acc: 0.9203, Val F1: 0.9461\n",
            "Epoch: 6/75, Train Loss: 0.0794, Train Acc: 0.9273, Train F1: 0.9497 Val Loss: 0.0790, Val Acc: 0.9254, Val F1: 0.9483\n",
            "Epoch: 7/75, Train Loss: 0.0760, Train Acc: 0.9306, Train F1: 0.9520 Val Loss: 0.0927, Val Acc: 0.9096, Val F1: 0.9354\n",
            "Epoch: 8/75, Train Loss: 0.0746, Train Acc: 0.9279, Train F1: 0.9502 Val Loss: 0.0850, Val Acc: 0.9192, Val F1: 0.9463\n",
            "Epoch: 9/75, Train Loss: 0.0810, Train Acc: 0.9172, Train F1: 0.9426 Val Loss: 0.0773, Val Acc: 0.9302, Val F1: 0.9517\n",
            "Epoch: 10/75, Train Loss: 0.0693, Train Acc: 0.9387, Train F1: 0.9576 Val Loss: 0.0731, Val Acc: 0.9316, Val F1: 0.9528\n",
            "Epoch: 11/75, Train Loss: 0.0712, Train Acc: 0.9340, Train F1: 0.9543 Val Loss: 0.0852, Val Acc: 0.9079, Val F1: 0.9342\n",
            "Epoch: 12/75, Train Loss: 0.0710, Train Acc: 0.9334, Train F1: 0.9539 Val Loss: 0.0803, Val Acc: 0.9203, Val F1: 0.9440\n",
            "Epoch: 13/75, Train Loss: 0.0644, Train Acc: 0.9412, Train F1: 0.9593 Val Loss: 0.0696, Val Acc: 0.9388, Val F1: 0.9581\n",
            "Epoch: 14/75, Train Loss: 0.0672, Train Acc: 0.9360, Train F1: 0.9560 Val Loss: 0.0831, Val Acc: 0.9175, Val F1: 0.9442\n",
            "Epoch: 15/75, Train Loss: 0.0723, Train Acc: 0.9304, Train F1: 0.9515 Val Loss: 0.0843, Val Acc: 0.9137, Val F1: 0.9432\n",
            "Epoch: 16/75, Train Loss: 0.0682, Train Acc: 0.9344, Train F1: 0.9545 Val Loss: 0.0739, Val Acc: 0.9247, Val F1: 0.9476\n",
            "Epoch: 17/75, Train Loss: 0.0642, Train Acc: 0.9405, Train F1: 0.9588 Val Loss: 0.0704, Val Acc: 0.9292, Val F1: 0.9507\n",
            "Epoch: 18/75, Train Loss: 0.0636, Train Acc: 0.9395, Train F1: 0.9582 Val Loss: 0.0739, Val Acc: 0.9179, Val F1: 0.9416\n",
            "Epoch: 19/75, Train Loss: 0.0640, Train Acc: 0.9386, Train F1: 0.9575 Val Loss: 0.0651, Val Acc: 0.9416, Val F1: 0.9599\n",
            "Epoch: 20/75, Train Loss: 0.0586, Train Acc: 0.9449, Train F1: 0.9620 Val Loss: 0.0702, Val Acc: 0.9333, Val F1: 0.9548\n",
            "Epoch: 21/75, Train Loss: 0.0601, Train Acc: 0.9403, Train F1: 0.9589 Val Loss: 0.0804, Val Acc: 0.9137, Val F1: 0.9414\n",
            "Epoch: 22/75, Train Loss: 0.0614, Train Acc: 0.9403, Train F1: 0.9588 Val Loss: 0.0673, Val Acc: 0.9323, Val F1: 0.9528\n",
            "Epoch: 23/75, Train Loss: 0.0653, Train Acc: 0.9367, Train F1: 0.9561 Val Loss: 0.0879, Val Acc: 0.9110, Val F1: 0.9416\n",
            "Epoch: 24/75, Train Loss: 0.0564, Train Acc: 0.9464, Train F1: 0.9628 Val Loss: 0.0640, Val Acc: 0.9450, Val F1: 0.9628\n",
            "Epoch: 25/75, Train Loss: 0.0571, Train Acc: 0.9465, Train F1: 0.9629 Val Loss: 0.0658, Val Acc: 0.9361, Val F1: 0.9565\n",
            "Epoch: 26/75, Train Loss: 0.0596, Train Acc: 0.9430, Train F1: 0.9606 Val Loss: 0.0608, Val Acc: 0.9457, Val F1: 0.9625\n",
            "Epoch: 27/75, Train Loss: 0.0534, Train Acc: 0.9473, Train F1: 0.9638 Val Loss: 0.0609, Val Acc: 0.9502, Val F1: 0.9661\n",
            "Epoch: 28/75, Train Loss: 0.0536, Train Acc: 0.9493, Train F1: 0.9649 Val Loss: 0.0592, Val Acc: 0.9416, Val F1: 0.9596\n",
            "Epoch: 29/75, Train Loss: 0.0538, Train Acc: 0.9465, Train F1: 0.9630 Val Loss: 0.0616, Val Acc: 0.9447, Val F1: 0.9626\n",
            "Epoch: 30/75, Train Loss: 0.0513, Train Acc: 0.9509, Train F1: 0.9660 Val Loss: 0.0763, Val Acc: 0.9210, Val F1: 0.9476\n",
            "Epoch: 31/75, Train Loss: 0.0536, Train Acc: 0.9468, Train F1: 0.9634 Val Loss: 0.0587, Val Acc: 0.9471, Val F1: 0.9641\n",
            "Epoch: 32/75, Train Loss: 0.0533, Train Acc: 0.9489, Train F1: 0.9645 Val Loss: 0.0586, Val Acc: 0.9402, Val F1: 0.9584\n",
            "Epoch: 33/75, Train Loss: 0.0526, Train Acc: 0.9485, Train F1: 0.9643 Val Loss: 0.0605, Val Acc: 0.9481, Val F1: 0.9641\n",
            "Epoch: 34/75, Train Loss: 0.0498, Train Acc: 0.9549, Train F1: 0.9687 Val Loss: 0.0562, Val Acc: 0.9550, Val F1: 0.9691\n",
            "Epoch: 35/75, Train Loss: 0.0498, Train Acc: 0.9528, Train F1: 0.9674 Val Loss: 0.0577, Val Acc: 0.9436, Val F1: 0.9609\n",
            "Epoch: 36/75, Train Loss: 0.0480, Train Acc: 0.9561, Train F1: 0.9694 Val Loss: 0.0544, Val Acc: 0.9612, Val F1: 0.9733\n",
            "Epoch: 37/75, Train Loss: 0.0459, Train Acc: 0.9601, Train F1: 0.9724 Val Loss: 0.0545, Val Acc: 0.9515, Val F1: 0.9668\n",
            "Epoch: 38/75, Train Loss: 0.0497, Train Acc: 0.9529, Train F1: 0.9674 Val Loss: 0.0603, Val Acc: 0.9481, Val F1: 0.9649\n",
            "Epoch: 39/75, Train Loss: 0.0490, Train Acc: 0.9538, Train F1: 0.9680 Val Loss: 0.0676, Val Acc: 0.9368, Val F1: 0.9578\n",
            "Epoch: 40/75, Train Loss: 0.0476, Train Acc: 0.9548, Train F1: 0.9686 Val Loss: 0.0524, Val Acc: 0.9550, Val F1: 0.9690\n",
            "Epoch: 41/75, Train Loss: 0.0453, Train Acc: 0.9590, Train F1: 0.9715 Val Loss: 0.0533, Val Acc: 0.9595, Val F1: 0.9721\n",
            "Epoch: 42/75, Train Loss: 0.0455, Train Acc: 0.9586, Train F1: 0.9714 Val Loss: 0.0528, Val Acc: 0.9574, Val F1: 0.9707\n",
            "Epoch: 43/75, Train Loss: 0.0450, Train Acc: 0.9597, Train F1: 0.9720 Val Loss: 0.0551, Val Acc: 0.9509, Val F1: 0.9656\n",
            "Epoch: 44/75, Train Loss: 0.0433, Train Acc: 0.9631, Train F1: 0.9744 Val Loss: 0.0540, Val Acc: 0.9474, Val F1: 0.9635\n",
            "Epoch: 45/75, Train Loss: 0.0431, Train Acc: 0.9607, Train F1: 0.9729 Val Loss: 0.0577, Val Acc: 0.9440, Val F1: 0.9604\n",
            "Epoch: 46/75, Train Loss: 0.0468, Train Acc: 0.9548, Train F1: 0.9686 Val Loss: 0.0507, Val Acc: 0.9536, Val F1: 0.9680\n",
            "Epoch: 47/75, Train Loss: 0.0448, Train Acc: 0.9600, Train F1: 0.9721 Val Loss: 0.0597, Val Acc: 0.9433, Val F1: 0.9599\n",
            "Epoch: 48/75, Train Loss: 0.0429, Train Acc: 0.9624, Train F1: 0.9739 Val Loss: 0.0513, Val Acc: 0.9564, Val F1: 0.9702\n",
            "Epoch: 49/75, Train Loss: 0.0423, Train Acc: 0.9629, Train F1: 0.9742 Val Loss: 0.0497, Val Acc: 0.9591, Val F1: 0.9719\n",
            "Epoch: 50/75, Train Loss: 0.0422, Train Acc: 0.9640, Train F1: 0.9752 Val Loss: 0.0531, Val Acc: 0.9536, Val F1: 0.9684\n",
            "Epoch: 51/75, Train Loss: 0.0418, Train Acc: 0.9630, Train F1: 0.9742 Val Loss: 0.0488, Val Acc: 0.9625, Val F1: 0.9742\n",
            "Epoch: 52/75, Train Loss: 0.0417, Train Acc: 0.9633, Train F1: 0.9747 Val Loss: 0.0497, Val Acc: 0.9629, Val F1: 0.9746\n",
            "Epoch: 53/75, Train Loss: 0.0399, Train Acc: 0.9660, Train F1: 0.9764 Val Loss: 0.0498, Val Acc: 0.9619, Val F1: 0.9739\n",
            "Epoch: 54/75, Train Loss: 0.0411, Train Acc: 0.9659, Train F1: 0.9761 Val Loss: 0.0481, Val Acc: 0.9649, Val F1: 0.9759\n",
            "Epoch: 55/75, Train Loss: 0.0388, Train Acc: 0.9685, Train F1: 0.9782 Val Loss: 0.0517, Val Acc: 0.9498, Val F1: 0.9649\n",
            "Epoch: 56/75, Train Loss: 0.0385, Train Acc: 0.9676, Train F1: 0.9775 Val Loss: 0.0483, Val Acc: 0.9605, Val F1: 0.9727\n",
            "Epoch: 57/75, Train Loss: 0.0384, Train Acc: 0.9696, Train F1: 0.9790 Val Loss: 0.0503, Val Acc: 0.9591, Val F1: 0.9722\n",
            "Epoch: 58/75, Train Loss: 0.0383, Train Acc: 0.9685, Train F1: 0.9782 Val Loss: 0.0471, Val Acc: 0.9643, Val F1: 0.9754\n",
            "Epoch: 59/75, Train Loss: 0.0379, Train Acc: 0.9692, Train F1: 0.9786 Val Loss: 0.0469, Val Acc: 0.9656, Val F1: 0.9763\n",
            "Epoch: 60/75, Train Loss: 0.0377, Train Acc: 0.9691, Train F1: 0.9785 Val Loss: 0.0473, Val Acc: 0.9674, Val F1: 0.9774\n",
            "Epoch: 61/75, Train Loss: 0.0376, Train Acc: 0.9695, Train F1: 0.9787 Val Loss: 0.0469, Val Acc: 0.9649, Val F1: 0.9757\n",
            "Epoch: 62/75, Train Loss: 0.0369, Train Acc: 0.9701, Train F1: 0.9793 Val Loss: 0.0468, Val Acc: 0.9660, Val F1: 0.9765\n",
            "Epoch: 63/75, Train Loss: 0.0366, Train Acc: 0.9707, Train F1: 0.9796 Val Loss: 0.0475, Val Acc: 0.9625, Val F1: 0.9741\n",
            "Epoch: 64/75, Train Loss: 0.0365, Train Acc: 0.9704, Train F1: 0.9795 Val Loss: 0.0463, Val Acc: 0.9636, Val F1: 0.9748\n",
            "Epoch: 65/75, Train Loss: 0.0365, Train Acc: 0.9707, Train F1: 0.9797 Val Loss: 0.0459, Val Acc: 0.9646, Val F1: 0.9756\n",
            "Epoch: 66/75, Train Loss: 0.0364, Train Acc: 0.9711, Train F1: 0.9799 Val Loss: 0.0463, Val Acc: 0.9636, Val F1: 0.9750\n",
            "Epoch: 67/75, Train Loss: 0.0359, Train Acc: 0.9711, Train F1: 0.9799 Val Loss: 0.0464, Val Acc: 0.9660, Val F1: 0.9764\n",
            "Epoch: 68/75, Train Loss: 0.0359, Train Acc: 0.9716, Train F1: 0.9803 Val Loss: 0.0460, Val Acc: 0.9643, Val F1: 0.9755\n",
            "Epoch: 69/75, Train Loss: 0.0357, Train Acc: 0.9723, Train F1: 0.9807 Val Loss: 0.0458, Val Acc: 0.9667, Val F1: 0.9771\n",
            "Epoch: 70/75, Train Loss: 0.0357, Train Acc: 0.9724, Train F1: 0.9808 Val Loss: 0.0457, Val Acc: 0.9660, Val F1: 0.9766\n",
            "Epoch: 71/75, Train Loss: 0.0355, Train Acc: 0.9726, Train F1: 0.9811 Val Loss: 0.0458, Val Acc: 0.9653, Val F1: 0.9761\n",
            "Epoch: 72/75, Train Loss: 0.0355, Train Acc: 0.9721, Train F1: 0.9806 Val Loss: 0.0456, Val Acc: 0.9663, Val F1: 0.9768\n",
            "Epoch: 73/75, Train Loss: 0.0354, Train Acc: 0.9727, Train F1: 0.9810 Val Loss: 0.0456, Val Acc: 0.9660, Val F1: 0.9765\n",
            "Epoch: 74/75, Train Loss: 0.0353, Train Acc: 0.9724, Train F1: 0.9808 Val Loss: 0.0456, Val Acc: 0.9656, Val F1: 0.9763\n",
            "Epoch: 75/75, Train Loss: 0.0353, Train Acc: 0.9730, Train F1: 0.9812 Val Loss: 0.0456, Val Acc: 0.9660, Val F1: 0.9765\n",
            "\n",
            " 🔎 search 13 : deep_rescnn --- lr : 0.001981796079782972, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv3.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1511, Train Acc: 0.8244, Train F1: 0.8764 Val Loss: 0.1246, Val Acc: 0.8653, Val F1: 0.9125\n",
            "Epoch: 2/75, Train Loss: 0.1057, Train Acc: 0.8899, Train F1: 0.9240 Val Loss: 0.1106, Val Acc: 0.8777, Val F1: 0.9102\n",
            "Epoch: 3/75, Train Loss: 0.0897, Train Acc: 0.9136, Train F1: 0.9400 Val Loss: 0.0831, Val Acc: 0.9247, Val F1: 0.9476\n",
            "Epoch: 4/75, Train Loss: 0.0758, Train Acc: 0.9277, Train F1: 0.9496 Val Loss: 0.0755, Val Acc: 0.9381, Val F1: 0.9578\n",
            "Epoch: 5/75, Train Loss: 0.0725, Train Acc: 0.9268, Train F1: 0.9491 Val Loss: 0.0754, Val Acc: 0.9247, Val F1: 0.9470\n",
            "Epoch: 6/75, Train Loss: 0.0723, Train Acc: 0.9262, Train F1: 0.9490 Val Loss: 0.0771, Val Acc: 0.9230, Val F1: 0.9465\n",
            "Epoch: 7/75, Train Loss: 0.0624, Train Acc: 0.9405, Train F1: 0.9587 Val Loss: 0.1094, Val Acc: 0.8804, Val F1: 0.9233\n",
            "Epoch: 8/75, Train Loss: 0.0580, Train Acc: 0.9452, Train F1: 0.9619 Val Loss: 0.0579, Val Acc: 0.9519, Val F1: 0.9671\n",
            "Epoch: 9/75, Train Loss: 0.0537, Train Acc: 0.9498, Train F1: 0.9651 Val Loss: 0.0673, Val Acc: 0.9326, Val F1: 0.9522\n",
            "Epoch: 10/75, Train Loss: 0.0567, Train Acc: 0.9433, Train F1: 0.9603 Val Loss: 0.0616, Val Acc: 0.9481, Val F1: 0.9647\n",
            "Epoch: 11/75, Train Loss: 0.0563, Train Acc: 0.9428, Train F1: 0.9602 Val Loss: 0.0578, Val Acc: 0.9474, Val F1: 0.9643\n",
            "Epoch: 12/75, Train Loss: 0.0574, Train Acc: 0.9432, Train F1: 0.9604 Val Loss: 0.0573, Val Acc: 0.9495, Val F1: 0.9650\n",
            "Epoch: 13/75, Train Loss: 0.0487, Train Acc: 0.9548, Train F1: 0.9688 Val Loss: 0.0533, Val Acc: 0.9505, Val F1: 0.9661\n",
            "Epoch: 14/75, Train Loss: 0.0456, Train Acc: 0.9558, Train F1: 0.9691 Val Loss: 0.0474, Val Acc: 0.9564, Val F1: 0.9701\n",
            "Epoch: 15/75, Train Loss: 0.0515, Train Acc: 0.9474, Train F1: 0.9632 Val Loss: 0.0619, Val Acc: 0.9419, Val F1: 0.9608\n",
            "Epoch: 16/75, Train Loss: 0.0463, Train Acc: 0.9582, Train F1: 0.9710 Val Loss: 0.0552, Val Acc: 0.9440, Val F1: 0.9604\n",
            "Epoch: 17/75, Train Loss: 0.0478, Train Acc: 0.9535, Train F1: 0.9678 Val Loss: 0.0534, Val Acc: 0.9546, Val F1: 0.9693\n",
            "Epoch: 18/75, Train Loss: 0.0436, Train Acc: 0.9600, Train F1: 0.9722 Val Loss: 0.0546, Val Acc: 0.9612, Val F1: 0.9736\n",
            "Epoch: 19/75, Train Loss: 0.0413, Train Acc: 0.9645, Train F1: 0.9753 Val Loss: 0.0493, Val Acc: 0.9663, Val F1: 0.9770\n",
            "Epoch: 20/75, Train Loss: 0.0432, Train Acc: 0.9627, Train F1: 0.9739 Val Loss: 0.0470, Val Acc: 0.9588, Val F1: 0.9719\n",
            "Epoch: 21/75, Train Loss: 0.0427, Train Acc: 0.9613, Train F1: 0.9730 Val Loss: 0.0468, Val Acc: 0.9553, Val F1: 0.9694\n",
            "Epoch: 22/75, Train Loss: 0.0431, Train Acc: 0.9599, Train F1: 0.9719 Val Loss: 0.0584, Val Acc: 0.9405, Val F1: 0.9601\n",
            "Epoch: 23/75, Train Loss: 0.0438, Train Acc: 0.9576, Train F1: 0.9703 Val Loss: 0.0547, Val Acc: 0.9536, Val F1: 0.9680\n",
            "Epoch: 24/75, Train Loss: 0.0402, Train Acc: 0.9664, Train F1: 0.9768 Val Loss: 0.0477, Val Acc: 0.9529, Val F1: 0.9674\n",
            "Epoch: 25/75, Train Loss: 0.0397, Train Acc: 0.9666, Train F1: 0.9768 Val Loss: 0.0431, Val Acc: 0.9660, Val F1: 0.9766\n",
            "Epoch: 26/75, Train Loss: 0.0406, Train Acc: 0.9616, Train F1: 0.9734 Val Loss: 0.0514, Val Acc: 0.9546, Val F1: 0.9690\n",
            "Epoch: 27/75, Train Loss: 0.0403, Train Acc: 0.9641, Train F1: 0.9751 Val Loss: 0.0453, Val Acc: 0.9646, Val F1: 0.9753\n",
            "Epoch: 28/75, Train Loss: 0.0409, Train Acc: 0.9637, Train F1: 0.9746 Val Loss: 0.0490, Val Acc: 0.9550, Val F1: 0.9687\n",
            "Epoch: 29/75, Train Loss: 0.0382, Train Acc: 0.9662, Train F1: 0.9764 Val Loss: 0.0475, Val Acc: 0.9515, Val F1: 0.9659\n",
            "Epoch: 30/75, Train Loss: 0.0366, Train Acc: 0.9700, Train F1: 0.9792 Val Loss: 0.0421, Val Acc: 0.9639, Val F1: 0.9752\n",
            "Epoch: 31/75, Train Loss: 0.0344, Train Acc: 0.9724, Train F1: 0.9808 Val Loss: 0.0414, Val Acc: 0.9677, Val F1: 0.9776\n",
            "Epoch: 32/75, Train Loss: 0.0345, Train Acc: 0.9741, Train F1: 0.9821 Val Loss: 0.0417, Val Acc: 0.9632, Val F1: 0.9747\n",
            "Epoch: 33/75, Train Loss: 0.0364, Train Acc: 0.9706, Train F1: 0.9796 Val Loss: 0.0410, Val Acc: 0.9718, Val F1: 0.9807\n",
            "Epoch: 34/75, Train Loss: 0.0351, Train Acc: 0.9692, Train F1: 0.9786 Val Loss: 0.0467, Val Acc: 0.9605, Val F1: 0.9723\n",
            "Epoch: 35/75, Train Loss: 0.0349, Train Acc: 0.9732, Train F1: 0.9813 Val Loss: 0.0396, Val Acc: 0.9670, Val F1: 0.9772\n",
            "Epoch: 36/75, Train Loss: 0.0348, Train Acc: 0.9710, Train F1: 0.9800 Val Loss: 0.0421, Val Acc: 0.9643, Val F1: 0.9751\n",
            "Epoch: 37/75, Train Loss: 0.0358, Train Acc: 0.9711, Train F1: 0.9800 Val Loss: 0.0445, Val Acc: 0.9588, Val F1: 0.9712\n",
            "Epoch: 38/75, Train Loss: 0.0334, Train Acc: 0.9740, Train F1: 0.9819 Val Loss: 0.0439, Val Acc: 0.9691, Val F1: 0.9789\n",
            "Epoch: 39/75, Train Loss: 0.0333, Train Acc: 0.9732, Train F1: 0.9814 Val Loss: 0.0406, Val Acc: 0.9691, Val F1: 0.9788\n",
            "Epoch: 40/75, Train Loss: 0.0325, Train Acc: 0.9742, Train F1: 0.9821 Val Loss: 0.0389, Val Acc: 0.9680, Val F1: 0.9780\n",
            "Epoch: 41/75, Train Loss: 0.0324, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0395, Val Acc: 0.9701, Val F1: 0.9795\n",
            "Epoch: 42/75, Train Loss: 0.0322, Train Acc: 0.9772, Train F1: 0.9842 Val Loss: 0.0387, Val Acc: 0.9704, Val F1: 0.9795\n",
            "Epoch: 43/75, Train Loss: 0.0306, Train Acc: 0.9792, Train F1: 0.9856 Val Loss: 0.0375, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 44/75, Train Loss: 0.0327, Train Acc: 0.9739, Train F1: 0.9819 Val Loss: 0.0656, Val Acc: 0.9285, Val F1: 0.9527\n",
            "Epoch: 45/75, Train Loss: 0.0341, Train Acc: 0.9722, Train F1: 0.9807 Val Loss: 0.0412, Val Acc: 0.9660, Val F1: 0.9762\n",
            "Epoch: 46/75, Train Loss: 0.0326, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0415, Val Acc: 0.9656, Val F1: 0.9766\n",
            "Epoch: 47/75, Train Loss: 0.0333, Train Acc: 0.9734, Train F1: 0.9817 Val Loss: 0.0391, Val Acc: 0.9684, Val F1: 0.9784\n",
            "Epoch: 48/75, Train Loss: 0.0303, Train Acc: 0.9778, Train F1: 0.9847 Val Loss: 0.0374, Val Acc: 0.9711, Val F1: 0.9801\n",
            "Epoch: 49/75, Train Loss: 0.0304, Train Acc: 0.9788, Train F1: 0.9853 Val Loss: 0.0417, Val Acc: 0.9632, Val F1: 0.9742\n",
            "Epoch: 50/75, Train Loss: 0.0307, Train Acc: 0.9763, Train F1: 0.9836 Val Loss: 0.0384, Val Acc: 0.9718, Val F1: 0.9806\n",
            "Epoch: 51/75, Train Loss: 0.0297, Train Acc: 0.9778, Train F1: 0.9845 Val Loss: 0.0372, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 52/75, Train Loss: 0.0295, Train Acc: 0.9779, Train F1: 0.9846 Val Loss: 0.0407, Val Acc: 0.9625, Val F1: 0.9738\n",
            "Epoch: 53/75, Train Loss: 0.0294, Train Acc: 0.9793, Train F1: 0.9856 Val Loss: 0.0374, Val Acc: 0.9694, Val F1: 0.9787\n",
            "Epoch: 54/75, Train Loss: 0.0295, Train Acc: 0.9798, Train F1: 0.9859 Val Loss: 0.0364, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 55/75, Train Loss: 0.0284, Train Acc: 0.9811, Train F1: 0.9869 Val Loss: 0.0365, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 56/75, Train Loss: 0.0291, Train Acc: 0.9794, Train F1: 0.9858 Val Loss: 0.0395, Val Acc: 0.9649, Val F1: 0.9755\n",
            "Epoch: 57/75, Train Loss: 0.0288, Train Acc: 0.9809, Train F1: 0.9867 Val Loss: 0.0370, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 58/75, Train Loss: 0.0282, Train Acc: 0.9811, Train F1: 0.9870 Val Loss: 0.0358, Val Acc: 0.9722, Val F1: 0.9808\n",
            "Epoch: 59/75, Train Loss: 0.0294, Train Acc: 0.9770, Train F1: 0.9841 Val Loss: 0.0368, Val Acc: 0.9687, Val F1: 0.9783\n",
            "Epoch: 60/75, Train Loss: 0.0284, Train Acc: 0.9800, Train F1: 0.9861 Val Loss: 0.0355, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 61/75, Train Loss: 0.0272, Train Acc: 0.9814, Train F1: 0.9872 Val Loss: 0.0350, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 62/75, Train Loss: 0.0276, Train Acc: 0.9819, Train F1: 0.9875 Val Loss: 0.0348, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 63/75, Train Loss: 0.0270, Train Acc: 0.9832, Train F1: 0.9884 Val Loss: 0.0355, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 64/75, Train Loss: 0.0267, Train Acc: 0.9825, Train F1: 0.9878 Val Loss: 0.0353, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 65/75, Train Loss: 0.0274, Train Acc: 0.9822, Train F1: 0.9877 Val Loss: 0.0349, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 66/75, Train Loss: 0.0266, Train Acc: 0.9829, Train F1: 0.9882 Val Loss: 0.0352, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 67/75, Train Loss: 0.0264, Train Acc: 0.9827, Train F1: 0.9880 Val Loss: 0.0346, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 68/75, Train Loss: 0.0263, Train Acc: 0.9829, Train F1: 0.9880 Val Loss: 0.0347, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 69/75, Train Loss: 0.0262, Train Acc: 0.9836, Train F1: 0.9886 Val Loss: 0.0347, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 70/75, Train Loss: 0.0263, Train Acc: 0.9833, Train F1: 0.9885 Val Loss: 0.0346, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 71/75, Train Loss: 0.0261, Train Acc: 0.9830, Train F1: 0.9881 Val Loss: 0.0347, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 72/75, Train Loss: 0.0261, Train Acc: 0.9835, Train F1: 0.9885 Val Loss: 0.0345, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 73/75, Train Loss: 0.0260, Train Acc: 0.9832, Train F1: 0.9883 Val Loss: 0.0345, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 74/75, Train Loss: 0.0259, Train Acc: 0.9836, Train F1: 0.9886 Val Loss: 0.0344, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 75/75, Train Loss: 0.0259, Train Acc: 0.9835, Train F1: 0.9885 Val Loss: 0.0344, Val Acc: 0.9763, Val F1: 0.9837\n",
            "\n",
            " 🔎 search 14 : deep_rescnn --- lr : 0.001981796079782972, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv4.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1328, Train Acc: 0.8558, Train F1: 0.8968 Val Loss: 0.0998, Val Acc: 0.8955, Val F1: 0.9312\n",
            "Epoch: 2/75, Train Loss: 0.0784, Train Acc: 0.9250, Train F1: 0.9479 Val Loss: 0.1058, Val Acc: 0.8722, Val F1: 0.9178\n",
            "Epoch: 3/75, Train Loss: 0.0635, Train Acc: 0.9392, Train F1: 0.9578 Val Loss: 0.0872, Val Acc: 0.9000, Val F1: 0.9274\n",
            "Epoch: 4/75, Train Loss: 0.0653, Train Acc: 0.9362, Train F1: 0.9554 Val Loss: 0.0609, Val Acc: 0.9412, Val F1: 0.9599\n",
            "Epoch: 5/75, Train Loss: 0.0550, Train Acc: 0.9451, Train F1: 0.9619 Val Loss: 0.0540, Val Acc: 0.9519, Val F1: 0.9673\n",
            "Epoch: 6/75, Train Loss: 0.0475, Train Acc: 0.9549, Train F1: 0.9689 Val Loss: 0.0512, Val Acc: 0.9591, Val F1: 0.9714\n",
            "Epoch: 7/75, Train Loss: 0.0429, Train Acc: 0.9616, Train F1: 0.9735 Val Loss: 0.0469, Val Acc: 0.9632, Val F1: 0.9745\n",
            "Epoch: 8/75, Train Loss: 0.0530, Train Acc: 0.9450, Train F1: 0.9618 Val Loss: 0.0452, Val Acc: 0.9643, Val F1: 0.9754\n",
            "Epoch: 9/75, Train Loss: 0.0438, Train Acc: 0.9592, Train F1: 0.9717 Val Loss: 0.0504, Val Acc: 0.9478, Val F1: 0.9633\n",
            "Epoch: 10/75, Train Loss: 0.0385, Train Acc: 0.9660, Train F1: 0.9762 Val Loss: 0.0422, Val Acc: 0.9674, Val F1: 0.9776\n",
            "Epoch: 11/75, Train Loss: 0.0358, Train Acc: 0.9704, Train F1: 0.9797 Val Loss: 0.0661, Val Acc: 0.9316, Val F1: 0.9507\n",
            "Epoch: 12/75, Train Loss: 0.0399, Train Acc: 0.9631, Train F1: 0.9743 Val Loss: 0.0431, Val Acc: 0.9601, Val F1: 0.9723\n",
            "Epoch: 13/75, Train Loss: 0.0389, Train Acc: 0.9656, Train F1: 0.9761 Val Loss: 0.0474, Val Acc: 0.9577, Val F1: 0.9704\n",
            "Epoch: 14/75, Train Loss: 0.0357, Train Acc: 0.9702, Train F1: 0.9791 Val Loss: 0.0431, Val Acc: 0.9629, Val F1: 0.9748\n",
            "Epoch: 15/75, Train Loss: 0.0382, Train Acc: 0.9663, Train F1: 0.9766 Val Loss: 0.0415, Val Acc: 0.9656, Val F1: 0.9765\n",
            "Epoch: 16/75, Train Loss: 0.0399, Train Acc: 0.9633, Train F1: 0.9747 Val Loss: 0.0418, Val Acc: 0.9667, Val F1: 0.9768\n",
            "Epoch: 17/75, Train Loss: 0.0323, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0599, Val Acc: 0.9378, Val F1: 0.9554\n",
            "Epoch: 18/75, Train Loss: 0.0352, Train Acc: 0.9690, Train F1: 0.9784 Val Loss: 0.0397, Val Acc: 0.9663, Val F1: 0.9765\n",
            "Epoch: 19/75, Train Loss: 0.0308, Train Acc: 0.9757, Train F1: 0.9831 Val Loss: 0.0537, Val Acc: 0.9450, Val F1: 0.9609\n",
            "Epoch: 20/75, Train Loss: 0.0328, Train Acc: 0.9706, Train F1: 0.9796 Val Loss: 0.0409, Val Acc: 0.9660, Val F1: 0.9768\n",
            "Epoch: 21/75, Train Loss: 0.0300, Train Acc: 0.9765, Train F1: 0.9837 Val Loss: 0.0554, Val Acc: 0.9454, Val F1: 0.9634\n",
            "Epoch: 22/75, Train Loss: 0.0358, Train Acc: 0.9664, Train F1: 0.9766 Val Loss: 0.0379, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 23/75, Train Loss: 0.0317, Train Acc: 0.9733, Train F1: 0.9814 Val Loss: 0.0385, Val Acc: 0.9667, Val F1: 0.9768\n",
            "Epoch: 24/75, Train Loss: 0.0311, Train Acc: 0.9746, Train F1: 0.9824 Val Loss: 0.0457, Val Acc: 0.9581, Val F1: 0.9704\n",
            "Epoch: 25/75, Train Loss: 0.0282, Train Acc: 0.9781, Train F1: 0.9848 Val Loss: 0.0397, Val Acc: 0.9653, Val F1: 0.9762\n",
            "Epoch: 26/75, Train Loss: 0.0289, Train Acc: 0.9782, Train F1: 0.9849 Val Loss: 0.0521, Val Acc: 0.9467, Val F1: 0.9620\n",
            "Epoch: 27/75, Train Loss: 0.0342, Train Acc: 0.9695, Train F1: 0.9788 Val Loss: 0.0428, Val Acc: 0.9674, Val F1: 0.9772\n",
            "Epoch: 28/75, Train Loss: 0.0304, Train Acc: 0.9746, Train F1: 0.9823 Val Loss: 0.0372, Val Acc: 0.9701, Val F1: 0.9795\n",
            "Epoch: 29/75, Train Loss: 0.0270, Train Acc: 0.9808, Train F1: 0.9866 Val Loss: 0.0381, Val Acc: 0.9680, Val F1: 0.9781\n",
            "Epoch: 30/75, Train Loss: 0.0276, Train Acc: 0.9778, Train F1: 0.9846 Val Loss: 0.0343, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 31/75, Train Loss: 0.0291, Train Acc: 0.9779, Train F1: 0.9847 Val Loss: 0.0347, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 32/75, Train Loss: 0.0266, Train Acc: 0.9802, Train F1: 0.9864 Val Loss: 0.0417, Val Acc: 0.9632, Val F1: 0.9742\n",
            "Epoch: 33/75, Train Loss: 0.0285, Train Acc: 0.9767, Train F1: 0.9838 Val Loss: 0.0376, Val Acc: 0.9670, Val F1: 0.9769\n",
            "Epoch: 34/75, Train Loss: 0.0283, Train Acc: 0.9779, Train F1: 0.9846 Val Loss: 0.0404, Val Acc: 0.9646, Val F1: 0.9759\n",
            "Epoch: 35/75, Train Loss: 0.0297, Train Acc: 0.9742, Train F1: 0.9822 Val Loss: 0.0442, Val Acc: 0.9577, Val F1: 0.9714\n",
            "Epoch: 36/75, Train Loss: 0.0262, Train Acc: 0.9801, Train F1: 0.9862 Val Loss: 0.0339, Val Acc: 0.9718, Val F1: 0.9804\n",
            "Epoch: 37/75, Train Loss: 0.0244, Train Acc: 0.9828, Train F1: 0.9880 Val Loss: 0.0370, Val Acc: 0.9694, Val F1: 0.9791\n",
            "Epoch: 38/75, Train Loss: 0.0244, Train Acc: 0.9816, Train F1: 0.9872 Val Loss: 0.0319, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 39/75, Train Loss: 0.0243, Train Acc: 0.9826, Train F1: 0.9879 Val Loss: 0.0347, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 40/75, Train Loss: 0.0242, Train Acc: 0.9836, Train F1: 0.9885 Val Loss: 0.0337, Val Acc: 0.9725, Val F1: 0.9809\n",
            "Epoch: 41/75, Train Loss: 0.0251, Train Acc: 0.9820, Train F1: 0.9875 Val Loss: 0.0330, Val Acc: 0.9718, Val F1: 0.9806\n",
            "Epoch: 42/75, Train Loss: 0.0237, Train Acc: 0.9827, Train F1: 0.9881 Val Loss: 0.0349, Val Acc: 0.9735, Val F1: 0.9816\n",
            "Epoch: 43/75, Train Loss: 0.0231, Train Acc: 0.9851, Train F1: 0.9897 Val Loss: 0.0343, Val Acc: 0.9732, Val F1: 0.9816\n",
            "Epoch: 44/75, Train Loss: 0.0229, Train Acc: 0.9850, Train F1: 0.9895 Val Loss: 0.0336, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 45/75, Train Loss: 0.0244, Train Acc: 0.9827, Train F1: 0.9880 Val Loss: 0.0389, Val Acc: 0.9632, Val F1: 0.9742\n",
            "Epoch: 46/75, Train Loss: 0.0234, Train Acc: 0.9842, Train F1: 0.9891 Val Loss: 0.0380, Val Acc: 0.9667, Val F1: 0.9773\n",
            "Epoch: 47/75, Train Loss: 0.0238, Train Acc: 0.9833, Train F1: 0.9884 Val Loss: 0.0351, Val Acc: 0.9715, Val F1: 0.9805\n",
            "Epoch: 48/75, Train Loss: 0.0236, Train Acc: 0.9826, Train F1: 0.9879 Val Loss: 0.0323, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 49/75, Train Loss: 0.0237, Train Acc: 0.9821, Train F1: 0.9876 Val Loss: 0.0386, Val Acc: 0.9649, Val F1: 0.9761\n",
            "Epoch: 50/75, Train Loss: 0.0234, Train Acc: 0.9842, Train F1: 0.9890 Val Loss: 0.0313, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 51/75, Train Loss: 0.0213, Train Acc: 0.9868, Train F1: 0.9909 Val Loss: 0.0319, Val Acc: 0.9735, Val F1: 0.9816\n",
            "Epoch: 52/75, Train Loss: 0.0230, Train Acc: 0.9827, Train F1: 0.9880 Val Loss: 0.0304, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 53/75, Train Loss: 0.0228, Train Acc: 0.9848, Train F1: 0.9893 Val Loss: 0.0298, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 54/75, Train Loss: 0.0212, Train Acc: 0.9880, Train F1: 0.9917 Val Loss: 0.0300, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 55/75, Train Loss: 0.0206, Train Acc: 0.9879, Train F1: 0.9915 Val Loss: 0.0341, Val Acc: 0.9725, Val F1: 0.9812\n",
            "Epoch: 56/75, Train Loss: 0.0204, Train Acc: 0.9874, Train F1: 0.9913 Val Loss: 0.0307, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 57/75, Train Loss: 0.0202, Train Acc: 0.9880, Train F1: 0.9916 Val Loss: 0.0314, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 58/75, Train Loss: 0.0199, Train Acc: 0.9890, Train F1: 0.9923 Val Loss: 0.0301, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 59/75, Train Loss: 0.0198, Train Acc: 0.9874, Train F1: 0.9912 Val Loss: 0.0313, Val Acc: 0.9742, Val F1: 0.9821\n",
            "Epoch: 60/75, Train Loss: 0.0198, Train Acc: 0.9884, Train F1: 0.9920 Val Loss: 0.0348, Val Acc: 0.9722, Val F1: 0.9810\n",
            "Epoch: 61/75, Train Loss: 0.0203, Train Acc: 0.9879, Train F1: 0.9916 Val Loss: 0.0312, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 62/75, Train Loss: 0.0194, Train Acc: 0.9892, Train F1: 0.9925 Val Loss: 0.0299, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 63/75, Train Loss: 0.0196, Train Acc: 0.9888, Train F1: 0.9923 Val Loss: 0.0298, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 64/75, Train Loss: 0.0191, Train Acc: 0.9897, Train F1: 0.9928 Val Loss: 0.0296, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 65/75, Train Loss: 0.0191, Train Acc: 0.9891, Train F1: 0.9925 Val Loss: 0.0298, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 66/75, Train Loss: 0.0189, Train Acc: 0.9901, Train F1: 0.9932 Val Loss: 0.0296, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 67/75, Train Loss: 0.0190, Train Acc: 0.9893, Train F1: 0.9925 Val Loss: 0.0298, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 68/75, Train Loss: 0.0190, Train Acc: 0.9907, Train F1: 0.9936 Val Loss: 0.0298, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 69/75, Train Loss: 0.0187, Train Acc: 0.9900, Train F1: 0.9930 Val Loss: 0.0295, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 70/75, Train Loss: 0.0187, Train Acc: 0.9907, Train F1: 0.9936 Val Loss: 0.0297, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 71/75, Train Loss: 0.0187, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0295, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 72/75, Train Loss: 0.0186, Train Acc: 0.9906, Train F1: 0.9934 Val Loss: 0.0294, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 73/75, Train Loss: 0.0186, Train Acc: 0.9905, Train F1: 0.9934 Val Loss: 0.0294, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 74/75, Train Loss: 0.0185, Train Acc: 0.9907, Train F1: 0.9936 Val Loss: 0.0295, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 75/75, Train Loss: 0.0185, Train Acc: 0.9901, Train F1: 0.9932 Val Loss: 0.0295, Val Acc: 0.9749, Val F1: 0.9827\n",
            "\n",
            " 🔎 search 15 : deep_rescnn --- lr : 0.001981796079782972, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv5.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1220, Train Acc: 0.8599, Train F1: 0.9061 Val Loss: 0.0856, Val Acc: 0.9165, Val F1: 0.9421\n",
            "Epoch: 2/75, Train Loss: 0.0756, Train Acc: 0.9229, Train F1: 0.9463 Val Loss: 0.0727, Val Acc: 0.9265, Val F1: 0.9509\n",
            "Epoch: 3/75, Train Loss: 0.0697, Train Acc: 0.9274, Train F1: 0.9498 Val Loss: 0.0612, Val Acc: 0.9467, Val F1: 0.9628\n",
            "Epoch: 4/75, Train Loss: 0.0570, Train Acc: 0.9458, Train F1: 0.9623 Val Loss: 0.0550, Val Acc: 0.9495, Val F1: 0.9647\n",
            "Epoch: 5/75, Train Loss: 0.0495, Train Acc: 0.9551, Train F1: 0.9689 Val Loss: 0.0592, Val Acc: 0.9402, Val F1: 0.9575\n",
            "Epoch: 6/75, Train Loss: 0.0489, Train Acc: 0.9568, Train F1: 0.9699 Val Loss: 0.0531, Val Acc: 0.9560, Val F1: 0.9703\n",
            "Epoch: 7/75, Train Loss: 0.0479, Train Acc: 0.9559, Train F1: 0.9691 Val Loss: 0.0593, Val Acc: 0.9412, Val F1: 0.9582\n",
            "Epoch: 8/75, Train Loss: 0.0409, Train Acc: 0.9648, Train F1: 0.9754 Val Loss: 0.0466, Val Acc: 0.9581, Val F1: 0.9706\n",
            "Epoch: 9/75, Train Loss: 0.0413, Train Acc: 0.9630, Train F1: 0.9741 Val Loss: 0.0501, Val Acc: 0.9574, Val F1: 0.9701\n",
            "Epoch: 10/75, Train Loss: 0.0396, Train Acc: 0.9645, Train F1: 0.9754 Val Loss: 0.0511, Val Acc: 0.9440, Val F1: 0.9601\n",
            "Epoch: 11/75, Train Loss: 0.0481, Train Acc: 0.9519, Train F1: 0.9665 Val Loss: 0.0409, Val Acc: 0.9680, Val F1: 0.9779\n",
            "Epoch: 12/75, Train Loss: 0.0359, Train Acc: 0.9699, Train F1: 0.9790 Val Loss: 0.0459, Val Acc: 0.9550, Val F1: 0.9684\n",
            "Epoch: 13/75, Train Loss: 0.0369, Train Acc: 0.9682, Train F1: 0.9777 Val Loss: 0.0710, Val Acc: 0.9237, Val F1: 0.9497\n",
            "Epoch: 14/75, Train Loss: 0.0348, Train Acc: 0.9715, Train F1: 0.9803 Val Loss: 0.0494, Val Acc: 0.9557, Val F1: 0.9701\n",
            "Epoch: 15/75, Train Loss: 0.0359, Train Acc: 0.9694, Train F1: 0.9789 Val Loss: 0.0530, Val Acc: 0.9540, Val F1: 0.9688\n",
            "Epoch: 16/75, Train Loss: 0.0376, Train Acc: 0.9676, Train F1: 0.9772 Val Loss: 0.0425, Val Acc: 0.9570, Val F1: 0.9698\n",
            "Epoch: 17/75, Train Loss: 0.0351, Train Acc: 0.9707, Train F1: 0.9796 Val Loss: 0.0493, Val Acc: 0.9515, Val F1: 0.9656\n",
            "Epoch: 18/75, Train Loss: 0.0322, Train Acc: 0.9733, Train F1: 0.9814 Val Loss: 0.0363, Val Acc: 0.9708, Val F1: 0.9798\n",
            "Epoch: 19/75, Train Loss: 0.0303, Train Acc: 0.9767, Train F1: 0.9838 Val Loss: 0.0508, Val Acc: 0.9467, Val F1: 0.9620\n",
            "Epoch: 20/75, Train Loss: 0.0328, Train Acc: 0.9704, Train F1: 0.9794 Val Loss: 0.0360, Val Acc: 0.9732, Val F1: 0.9813\n",
            "Epoch: 21/75, Train Loss: 0.0297, Train Acc: 0.9773, Train F1: 0.9842 Val Loss: 0.0415, Val Acc: 0.9632, Val F1: 0.9742\n",
            "Epoch: 22/75, Train Loss: 0.0332, Train Acc: 0.9730, Train F1: 0.9812 Val Loss: 0.0347, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 23/75, Train Loss: 0.0270, Train Acc: 0.9810, Train F1: 0.9868 Val Loss: 0.0351, Val Acc: 0.9711, Val F1: 0.9801\n",
            "Epoch: 24/75, Train Loss: 0.0296, Train Acc: 0.9787, Train F1: 0.9851 Val Loss: 0.0329, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 25/75, Train Loss: 0.0294, Train Acc: 0.9771, Train F1: 0.9840 Val Loss: 0.0380, Val Acc: 0.9732, Val F1: 0.9813\n",
            "Epoch: 26/75, Train Loss: 0.0292, Train Acc: 0.9784, Train F1: 0.9849 Val Loss: 0.0433, Val Acc: 0.9570, Val F1: 0.9696\n",
            "Epoch: 27/75, Train Loss: 0.0302, Train Acc: 0.9758, Train F1: 0.9832 Val Loss: 0.0486, Val Acc: 0.9515, Val F1: 0.9656\n",
            "Epoch: 28/75, Train Loss: 0.0274, Train Acc: 0.9798, Train F1: 0.9860 Val Loss: 0.0336, Val Acc: 0.9698, Val F1: 0.9792\n",
            "Epoch: 29/75, Train Loss: 0.0277, Train Acc: 0.9777, Train F1: 0.9844 Val Loss: 0.0445, Val Acc: 0.9529, Val F1: 0.9667\n",
            "Epoch: 30/75, Train Loss: 0.0339, Train Acc: 0.9729, Train F1: 0.9812 Val Loss: 0.0405, Val Acc: 0.9670, Val F1: 0.9776\n",
            "Epoch: 31/75, Train Loss: 0.0312, Train Acc: 0.9738, Train F1: 0.9819 Val Loss: 0.0409, Val Acc: 0.9639, Val F1: 0.9755\n",
            "Epoch: 32/75, Train Loss: 0.0282, Train Acc: 0.9794, Train F1: 0.9857 Val Loss: 0.0318, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 33/75, Train Loss: 0.0266, Train Acc: 0.9810, Train F1: 0.9867 Val Loss: 0.0357, Val Acc: 0.9708, Val F1: 0.9800\n",
            "Epoch: 34/75, Train Loss: 0.0275, Train Acc: 0.9795, Train F1: 0.9858 Val Loss: 0.0308, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 35/75, Train Loss: 0.0248, Train Acc: 0.9820, Train F1: 0.9876 Val Loss: 0.0363, Val Acc: 0.9701, Val F1: 0.9796\n",
            "Epoch: 36/75, Train Loss: 0.0284, Train Acc: 0.9761, Train F1: 0.9834 Val Loss: 0.0355, Val Acc: 0.9684, Val F1: 0.9780\n",
            "Epoch: 37/75, Train Loss: 0.0291, Train Acc: 0.9769, Train F1: 0.9840 Val Loss: 0.0341, Val Acc: 0.9725, Val F1: 0.9808\n",
            "Epoch: 38/75, Train Loss: 0.0249, Train Acc: 0.9835, Train F1: 0.9884 Val Loss: 0.0303, Val Acc: 0.9749, Val F1: 0.9826\n",
            "Epoch: 39/75, Train Loss: 0.0251, Train Acc: 0.9818, Train F1: 0.9873 Val Loss: 0.0316, Val Acc: 0.9780, Val F1: 0.9849\n",
            "Epoch: 40/75, Train Loss: 0.0251, Train Acc: 0.9819, Train F1: 0.9875 Val Loss: 0.0302, Val Acc: 0.9729, Val F1: 0.9812\n",
            "Epoch: 41/75, Train Loss: 0.0246, Train Acc: 0.9825, Train F1: 0.9878 Val Loss: 0.0315, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 42/75, Train Loss: 0.0251, Train Acc: 0.9809, Train F1: 0.9866 Val Loss: 0.0310, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 43/75, Train Loss: 0.0241, Train Acc: 0.9843, Train F1: 0.9891 Val Loss: 0.0340, Val Acc: 0.9704, Val F1: 0.9792\n",
            "Epoch: 44/75, Train Loss: 0.0248, Train Acc: 0.9804, Train F1: 0.9863 Val Loss: 0.0349, Val Acc: 0.9701, Val F1: 0.9791\n",
            "Epoch: 45/75, Train Loss: 0.0231, Train Acc: 0.9844, Train F1: 0.9892 Val Loss: 0.0304, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 46/75, Train Loss: 0.0230, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0337, Val Acc: 0.9715, Val F1: 0.9800\n",
            "Epoch: 47/75, Train Loss: 0.0238, Train Acc: 0.9842, Train F1: 0.9890 Val Loss: 0.0296, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 48/75, Train Loss: 0.0233, Train Acc: 0.9838, Train F1: 0.9889 Val Loss: 0.0311, Val Acc: 0.9780, Val F1: 0.9849\n",
            "Epoch: 49/75, Train Loss: 0.0221, Train Acc: 0.9857, Train F1: 0.9901 Val Loss: 0.0324, Val Acc: 0.9722, Val F1: 0.9805\n",
            "Epoch: 50/75, Train Loss: 0.0225, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0294, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 51/75, Train Loss: 0.0219, Train Acc: 0.9875, Train F1: 0.9913 Val Loss: 0.0295, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 52/75, Train Loss: 0.0224, Train Acc: 0.9863, Train F1: 0.9904 Val Loss: 0.0375, Val Acc: 0.9670, Val F1: 0.9776\n",
            "Epoch: 53/75, Train Loss: 0.0256, Train Acc: 0.9813, Train F1: 0.9870 Val Loss: 0.0394, Val Acc: 0.9680, Val F1: 0.9783\n",
            "Epoch: 54/75, Train Loss: 0.0216, Train Acc: 0.9863, Train F1: 0.9905 Val Loss: 0.0300, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 55/75, Train Loss: 0.0212, Train Acc: 0.9881, Train F1: 0.9917 Val Loss: 0.0290, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 56/75, Train Loss: 0.0216, Train Acc: 0.9869, Train F1: 0.9909 Val Loss: 0.0322, Val Acc: 0.9729, Val F1: 0.9810\n",
            "Epoch: 57/75, Train Loss: 0.0212, Train Acc: 0.9880, Train F1: 0.9917 Val Loss: 0.0305, Val Acc: 0.9773, Val F1: 0.9842\n",
            "Epoch: 58/75, Train Loss: 0.0207, Train Acc: 0.9884, Train F1: 0.9919 Val Loss: 0.0291, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 59/75, Train Loss: 0.0207, Train Acc: 0.9881, Train F1: 0.9917 Val Loss: 0.0294, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 60/75, Train Loss: 0.0204, Train Acc: 0.9892, Train F1: 0.9926 Val Loss: 0.0302, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 61/75, Train Loss: 0.0203, Train Acc: 0.9884, Train F1: 0.9919 Val Loss: 0.0297, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 62/75, Train Loss: 0.0210, Train Acc: 0.9880, Train F1: 0.9916 Val Loss: 0.0351, Val Acc: 0.9677, Val F1: 0.9773\n",
            "Epoch: 63/75, Train Loss: 0.0208, Train Acc: 0.9882, Train F1: 0.9918 Val Loss: 0.0292, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 64/75, Train Loss: 0.0202, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0287, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 65/75, Train Loss: 0.0202, Train Acc: 0.9884, Train F1: 0.9920 Val Loss: 0.0290, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 66/75, Train Loss: 0.0199, Train Acc: 0.9891, Train F1: 0.9924 Val Loss: 0.0285, Val Acc: 0.9797, Val F1: 0.9859\n",
            "Epoch: 67/75, Train Loss: 0.0197, Train Acc: 0.9895, Train F1: 0.9927 Val Loss: 0.0290, Val Acc: 0.9797, Val F1: 0.9859\n",
            "Epoch: 68/75, Train Loss: 0.0197, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0290, Val Acc: 0.9797, Val F1: 0.9859\n",
            "Epoch: 69/75, Train Loss: 0.0196, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0286, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 70/75, Train Loss: 0.0196, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0286, Val Acc: 0.9787, Val F1: 0.9852\n",
            "Epoch: 71/75, Train Loss: 0.0195, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0285, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 72/75, Train Loss: 0.0194, Train Acc: 0.9901, Train F1: 0.9932 Val Loss: 0.0285, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 73/75, Train Loss: 0.0194, Train Acc: 0.9906, Train F1: 0.9935 Val Loss: 0.0285, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 74/75, Train Loss: 0.0194, Train Acc: 0.9905, Train F1: 0.9933 Val Loss: 0.0285, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 75/75, Train Loss: 0.0194, Train Acc: 0.9905, Train F1: 0.9934 Val Loss: 0.0285, Val Acc: 0.9797, Val F1: 0.9860\n",
            "\n",
            " 🔎 search 16 : deep_rescnn --- lr : 0.0009144615755940233, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv2.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1033, Train Acc: 0.8929, Train F1: 0.9269 Val Loss: 0.0910, Val Acc: 0.9155, Val F1: 0.9437\n",
            "Epoch: 2/75, Train Loss: 0.0820, Train Acc: 0.9233, Train F1: 0.9467 Val Loss: 0.0779, Val Acc: 0.9340, Val F1: 0.9548\n",
            "Epoch: 3/75, Train Loss: 0.0766, Train Acc: 0.9296, Train F1: 0.9511 Val Loss: 0.0827, Val Acc: 0.9210, Val F1: 0.9447\n",
            "Epoch: 4/75, Train Loss: 0.0744, Train Acc: 0.9316, Train F1: 0.9524 Val Loss: 0.0802, Val Acc: 0.9213, Val F1: 0.9445\n",
            "Epoch: 5/75, Train Loss: 0.0732, Train Acc: 0.9324, Train F1: 0.9529 Val Loss: 0.0796, Val Acc: 0.9285, Val F1: 0.9505\n",
            "Epoch: 6/75, Train Loss: 0.0706, Train Acc: 0.9340, Train F1: 0.9537 Val Loss: 0.0788, Val Acc: 0.9351, Val F1: 0.9561\n",
            "Epoch: 7/75, Train Loss: 0.0720, Train Acc: 0.9325, Train F1: 0.9529 Val Loss: 0.0747, Val Acc: 0.9271, Val F1: 0.9504\n",
            "Epoch: 8/75, Train Loss: 0.0660, Train Acc: 0.9395, Train F1: 0.9580 Val Loss: 0.0659, Val Acc: 0.9381, Val F1: 0.9568\n",
            "Epoch: 9/75, Train Loss: 0.0646, Train Acc: 0.9373, Train F1: 0.9566 Val Loss: 0.0666, Val Acc: 0.9364, Val F1: 0.9561\n",
            "Epoch: 10/75, Train Loss: 0.0597, Train Acc: 0.9460, Train F1: 0.9628 Val Loss: 0.0645, Val Acc: 0.9371, Val F1: 0.9563\n",
            "Epoch: 11/75, Train Loss: 0.0602, Train Acc: 0.9438, Train F1: 0.9610 Val Loss: 0.0648, Val Acc: 0.9368, Val F1: 0.9569\n",
            "Epoch: 12/75, Train Loss: 0.0621, Train Acc: 0.9433, Train F1: 0.9605 Val Loss: 0.0658, Val Acc: 0.9368, Val F1: 0.9559\n",
            "Epoch: 13/75, Train Loss: 0.0573, Train Acc: 0.9485, Train F1: 0.9641 Val Loss: 0.0622, Val Acc: 0.9457, Val F1: 0.9630\n",
            "Epoch: 14/75, Train Loss: 0.0583, Train Acc: 0.9455, Train F1: 0.9619 Val Loss: 0.0616, Val Acc: 0.9385, Val F1: 0.9574\n",
            "Epoch: 15/75, Train Loss: 0.0590, Train Acc: 0.9425, Train F1: 0.9598 Val Loss: 0.0630, Val Acc: 0.9426, Val F1: 0.9603\n",
            "Epoch: 16/75, Train Loss: 0.0552, Train Acc: 0.9491, Train F1: 0.9647 Val Loss: 0.0621, Val Acc: 0.9467, Val F1: 0.9640\n",
            "Epoch: 17/75, Train Loss: 0.0516, Train Acc: 0.9517, Train F1: 0.9663 Val Loss: 0.0602, Val Acc: 0.9416, Val F1: 0.9598\n",
            "Epoch: 18/75, Train Loss: 0.0551, Train Acc: 0.9480, Train F1: 0.9638 Val Loss: 0.0720, Val Acc: 0.9364, Val F1: 0.9552\n",
            "Epoch: 19/75, Train Loss: 0.0536, Train Acc: 0.9494, Train F1: 0.9647 Val Loss: 0.0568, Val Acc: 0.9474, Val F1: 0.9637\n",
            "Epoch: 20/75, Train Loss: 0.0544, Train Acc: 0.9491, Train F1: 0.9646 Val Loss: 0.0578, Val Acc: 0.9426, Val F1: 0.9604\n",
            "Epoch: 21/75, Train Loss: 0.0517, Train Acc: 0.9534, Train F1: 0.9675 Val Loss: 0.0642, Val Acc: 0.9412, Val F1: 0.9593\n",
            "Epoch: 22/75, Train Loss: 0.0501, Train Acc: 0.9533, Train F1: 0.9675 Val Loss: 0.0872, Val Acc: 0.9137, Val F1: 0.9435\n",
            "Epoch: 23/75, Train Loss: 0.0538, Train Acc: 0.9483, Train F1: 0.9643 Val Loss: 0.0556, Val Acc: 0.9443, Val F1: 0.9616\n",
            "Epoch: 24/75, Train Loss: 0.0510, Train Acc: 0.9533, Train F1: 0.9675 Val Loss: 0.0559, Val Acc: 0.9454, Val F1: 0.9624\n",
            "Epoch: 25/75, Train Loss: 0.0494, Train Acc: 0.9533, Train F1: 0.9675 Val Loss: 0.0771, Val Acc: 0.9265, Val F1: 0.9513\n",
            "Epoch: 26/75, Train Loss: 0.0516, Train Acc: 0.9519, Train F1: 0.9666 Val Loss: 0.0603, Val Acc: 0.9502, Val F1: 0.9664\n",
            "Epoch: 27/75, Train Loss: 0.0485, Train Acc: 0.9546, Train F1: 0.9686 Val Loss: 0.0573, Val Acc: 0.9509, Val F1: 0.9667\n",
            "Epoch: 28/75, Train Loss: 0.0463, Train Acc: 0.9581, Train F1: 0.9708 Val Loss: 0.0575, Val Acc: 0.9498, Val F1: 0.9661\n",
            "Epoch: 29/75, Train Loss: 0.0464, Train Acc: 0.9582, Train F1: 0.9711 Val Loss: 0.0522, Val Acc: 0.9529, Val F1: 0.9677\n",
            "Epoch: 30/75, Train Loss: 0.0476, Train Acc: 0.9558, Train F1: 0.9695 Val Loss: 0.0516, Val Acc: 0.9529, Val F1: 0.9676\n",
            "Epoch: 31/75, Train Loss: 0.0478, Train Acc: 0.9564, Train F1: 0.9697 Val Loss: 0.0557, Val Acc: 0.9502, Val F1: 0.9651\n",
            "Epoch: 32/75, Train Loss: 0.0467, Train Acc: 0.9575, Train F1: 0.9705 Val Loss: 0.0548, Val Acc: 0.9509, Val F1: 0.9658\n",
            "Epoch: 33/75, Train Loss: 0.0459, Train Acc: 0.9577, Train F1: 0.9704 Val Loss: 0.0514, Val Acc: 0.9526, Val F1: 0.9675\n",
            "Epoch: 34/75, Train Loss: 0.0475, Train Acc: 0.9552, Train F1: 0.9686 Val Loss: 0.0541, Val Acc: 0.9430, Val F1: 0.9603\n",
            "Epoch: 35/75, Train Loss: 0.0458, Train Acc: 0.9562, Train F1: 0.9696 Val Loss: 0.0512, Val Acc: 0.9574, Val F1: 0.9710\n",
            "Epoch: 36/75, Train Loss: 0.0448, Train Acc: 0.9583, Train F1: 0.9710 Val Loss: 0.0503, Val Acc: 0.9543, Val F1: 0.9686\n",
            "Epoch: 37/75, Train Loss: 0.0448, Train Acc: 0.9575, Train F1: 0.9703 Val Loss: 0.0489, Val Acc: 0.9595, Val F1: 0.9720\n",
            "Epoch: 38/75, Train Loss: 0.0437, Train Acc: 0.9605, Train F1: 0.9723 Val Loss: 0.0500, Val Acc: 0.9543, Val F1: 0.9685\n",
            "Epoch: 39/75, Train Loss: 0.0444, Train Acc: 0.9608, Train F1: 0.9726 Val Loss: 0.0495, Val Acc: 0.9601, Val F1: 0.9728\n",
            "Epoch: 40/75, Train Loss: 0.0428, Train Acc: 0.9614, Train F1: 0.9731 Val Loss: 0.0527, Val Acc: 0.9553, Val F1: 0.9690\n",
            "Epoch: 41/75, Train Loss: 0.0459, Train Acc: 0.9573, Train F1: 0.9703 Val Loss: 0.0640, Val Acc: 0.9347, Val F1: 0.9564\n",
            "Epoch: 42/75, Train Loss: 0.0435, Train Acc: 0.9619, Train F1: 0.9733 Val Loss: 0.0516, Val Acc: 0.9529, Val F1: 0.9679\n",
            "Epoch: 43/75, Train Loss: 0.0419, Train Acc: 0.9612, Train F1: 0.9731 Val Loss: 0.0497, Val Acc: 0.9601, Val F1: 0.9728\n",
            "Epoch: 44/75, Train Loss: 0.0409, Train Acc: 0.9627, Train F1: 0.9738 Val Loss: 0.0489, Val Acc: 0.9591, Val F1: 0.9720\n",
            "Epoch: 45/75, Train Loss: 0.0415, Train Acc: 0.9623, Train F1: 0.9739 Val Loss: 0.0488, Val Acc: 0.9570, Val F1: 0.9706\n",
            "Epoch: 46/75, Train Loss: 0.0405, Train Acc: 0.9630, Train F1: 0.9744 Val Loss: 0.0472, Val Acc: 0.9643, Val F1: 0.9756\n",
            "Epoch: 47/75, Train Loss: 0.0390, Train Acc: 0.9656, Train F1: 0.9760 Val Loss: 0.0524, Val Acc: 0.9502, Val F1: 0.9651\n",
            "Epoch: 48/75, Train Loss: 0.0404, Train Acc: 0.9644, Train F1: 0.9751 Val Loss: 0.0479, Val Acc: 0.9605, Val F1: 0.9731\n",
            "Epoch: 49/75, Train Loss: 0.0393, Train Acc: 0.9652, Train F1: 0.9757 Val Loss: 0.0490, Val Acc: 0.9522, Val F1: 0.9669\n",
            "Epoch: 50/75, Train Loss: 0.0408, Train Acc: 0.9617, Train F1: 0.9734 Val Loss: 0.0501, Val Acc: 0.9595, Val F1: 0.9725\n",
            "Epoch: 51/75, Train Loss: 0.0395, Train Acc: 0.9630, Train F1: 0.9744 Val Loss: 0.0484, Val Acc: 0.9512, Val F1: 0.9660\n",
            "Epoch: 52/75, Train Loss: 0.0394, Train Acc: 0.9667, Train F1: 0.9766 Val Loss: 0.0466, Val Acc: 0.9567, Val F1: 0.9704\n",
            "Epoch: 53/75, Train Loss: 0.0382, Train Acc: 0.9671, Train F1: 0.9770 Val Loss: 0.0475, Val Acc: 0.9567, Val F1: 0.9705\n",
            "Epoch: 54/75, Train Loss: 0.0378, Train Acc: 0.9668, Train F1: 0.9770 Val Loss: 0.0450, Val Acc: 0.9622, Val F1: 0.9740\n",
            "Epoch: 55/75, Train Loss: 0.0371, Train Acc: 0.9682, Train F1: 0.9778 Val Loss: 0.0467, Val Acc: 0.9660, Val F1: 0.9767\n",
            "Epoch: 56/75, Train Loss: 0.0365, Train Acc: 0.9698, Train F1: 0.9792 Val Loss: 0.0446, Val Acc: 0.9667, Val F1: 0.9771\n",
            "Epoch: 57/75, Train Loss: 0.0366, Train Acc: 0.9670, Train F1: 0.9771 Val Loss: 0.0453, Val Acc: 0.9588, Val F1: 0.9716\n",
            "Epoch: 58/75, Train Loss: 0.0362, Train Acc: 0.9682, Train F1: 0.9779 Val Loss: 0.0449, Val Acc: 0.9656, Val F1: 0.9765\n",
            "Epoch: 59/75, Train Loss: 0.0359, Train Acc: 0.9699, Train F1: 0.9789 Val Loss: 0.0448, Val Acc: 0.9608, Val F1: 0.9729\n",
            "Epoch: 60/75, Train Loss: 0.0362, Train Acc: 0.9686, Train F1: 0.9781 Val Loss: 0.0447, Val Acc: 0.9643, Val F1: 0.9756\n",
            "Epoch: 61/75, Train Loss: 0.0355, Train Acc: 0.9700, Train F1: 0.9792 Val Loss: 0.0445, Val Acc: 0.9615, Val F1: 0.9736\n",
            "Epoch: 62/75, Train Loss: 0.0354, Train Acc: 0.9702, Train F1: 0.9794 Val Loss: 0.0440, Val Acc: 0.9639, Val F1: 0.9751\n",
            "Epoch: 63/75, Train Loss: 0.0349, Train Acc: 0.9710, Train F1: 0.9799 Val Loss: 0.0437, Val Acc: 0.9639, Val F1: 0.9753\n",
            "Epoch: 64/75, Train Loss: 0.0347, Train Acc: 0.9707, Train F1: 0.9794 Val Loss: 0.0441, Val Acc: 0.9653, Val F1: 0.9763\n",
            "Epoch: 65/75, Train Loss: 0.0345, Train Acc: 0.9712, Train F1: 0.9798 Val Loss: 0.0433, Val Acc: 0.9667, Val F1: 0.9771\n",
            "Epoch: 66/75, Train Loss: 0.0345, Train Acc: 0.9715, Train F1: 0.9800 Val Loss: 0.0453, Val Acc: 0.9632, Val F1: 0.9749\n",
            "Epoch: 67/75, Train Loss: 0.0344, Train Acc: 0.9721, Train F1: 0.9805 Val Loss: 0.0435, Val Acc: 0.9649, Val F1: 0.9758\n",
            "Epoch: 68/75, Train Loss: 0.0340, Train Acc: 0.9718, Train F1: 0.9802 Val Loss: 0.0432, Val Acc: 0.9660, Val F1: 0.9767\n",
            "Epoch: 69/75, Train Loss: 0.0339, Train Acc: 0.9717, Train F1: 0.9802 Val Loss: 0.0442, Val Acc: 0.9653, Val F1: 0.9763\n",
            "Epoch: 70/75, Train Loss: 0.0338, Train Acc: 0.9724, Train F1: 0.9808 Val Loss: 0.0431, Val Acc: 0.9670, Val F1: 0.9774\n",
            "Epoch: 71/75, Train Loss: 0.0337, Train Acc: 0.9718, Train F1: 0.9805 Val Loss: 0.0432, Val Acc: 0.9653, Val F1: 0.9761\n",
            "Epoch: 72/75, Train Loss: 0.0337, Train Acc: 0.9730, Train F1: 0.9812 Val Loss: 0.0431, Val Acc: 0.9660, Val F1: 0.9766\n",
            "Epoch: 73/75, Train Loss: 0.0336, Train Acc: 0.9721, Train F1: 0.9808 Val Loss: 0.0431, Val Acc: 0.9667, Val F1: 0.9771\n",
            "Epoch: 74/75, Train Loss: 0.0335, Train Acc: 0.9725, Train F1: 0.9808 Val Loss: 0.0431, Val Acc: 0.9667, Val F1: 0.9771\n",
            "Epoch: 75/75, Train Loss: 0.0335, Train Acc: 0.9723, Train F1: 0.9807 Val Loss: 0.0431, Val Acc: 0.9667, Val F1: 0.9771\n",
            "\n",
            " 🔎 search 17 : deep_rescnn --- lr : 0.0009144615755940233, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv3.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1106, Train Acc: 0.8779, Train F1: 0.9157 Val Loss: 0.0863, Val Acc: 0.9213, Val F1: 0.9458\n",
            "Epoch: 2/75, Train Loss: 0.0768, Train Acc: 0.9293, Train F1: 0.9508 Val Loss: 0.0732, Val Acc: 0.9378, Val F1: 0.9572\n",
            "Epoch: 3/75, Train Loss: 0.0669, Train Acc: 0.9384, Train F1: 0.9565 Val Loss: 0.0780, Val Acc: 0.9213, Val F1: 0.9476\n",
            "Epoch: 4/75, Train Loss: 0.0649, Train Acc: 0.9362, Train F1: 0.9556 Val Loss: 0.0639, Val Acc: 0.9460, Val F1: 0.9630\n",
            "Epoch: 5/75, Train Loss: 0.0634, Train Acc: 0.9405, Train F1: 0.9583 Val Loss: 0.0653, Val Acc: 0.9392, Val F1: 0.9573\n",
            "Epoch: 6/75, Train Loss: 0.0553, Train Acc: 0.9507, Train F1: 0.9657 Val Loss: 0.0633, Val Acc: 0.9485, Val F1: 0.9651\n",
            "Epoch: 7/75, Train Loss: 0.0536, Train Acc: 0.9487, Train F1: 0.9640 Val Loss: 0.0553, Val Acc: 0.9553, Val F1: 0.9690\n",
            "Epoch: 8/75, Train Loss: 0.0551, Train Acc: 0.9452, Train F1: 0.9619 Val Loss: 0.0701, Val Acc: 0.9237, Val F1: 0.9455\n",
            "Epoch: 9/75, Train Loss: 0.0501, Train Acc: 0.9540, Train F1: 0.9679 Val Loss: 0.0575, Val Acc: 0.9450, Val F1: 0.9615\n",
            "Epoch: 10/75, Train Loss: 0.0446, Train Acc: 0.9625, Train F1: 0.9736 Val Loss: 0.0509, Val Acc: 0.9557, Val F1: 0.9699\n",
            "Epoch: 11/75, Train Loss: 0.0483, Train Acc: 0.9545, Train F1: 0.9681 Val Loss: 0.0606, Val Acc: 0.9447, Val F1: 0.9620\n",
            "Epoch: 12/75, Train Loss: 0.0455, Train Acc: 0.9589, Train F1: 0.9711 Val Loss: 0.0495, Val Acc: 0.9595, Val F1: 0.9720\n",
            "Epoch: 13/75, Train Loss: 0.0477, Train Acc: 0.9536, Train F1: 0.9676 Val Loss: 0.0548, Val Acc: 0.9498, Val F1: 0.9647\n",
            "Epoch: 14/75, Train Loss: 0.0413, Train Acc: 0.9630, Train F1: 0.9741 Val Loss: 0.0632, Val Acc: 0.9443, Val F1: 0.9624\n",
            "Epoch: 15/75, Train Loss: 0.0440, Train Acc: 0.9586, Train F1: 0.9710 Val Loss: 0.0631, Val Acc: 0.9443, Val F1: 0.9623\n",
            "Epoch: 16/75, Train Loss: 0.0442, Train Acc: 0.9611, Train F1: 0.9729 Val Loss: 0.0569, Val Acc: 0.9488, Val F1: 0.9653\n",
            "Epoch: 17/75, Train Loss: 0.0429, Train Acc: 0.9622, Train F1: 0.9735 Val Loss: 0.0585, Val Acc: 0.9498, Val F1: 0.9662\n",
            "Epoch: 18/75, Train Loss: 0.0390, Train Acc: 0.9687, Train F1: 0.9782 Val Loss: 0.0475, Val Acc: 0.9629, Val F1: 0.9744\n",
            "Epoch: 19/75, Train Loss: 0.0408, Train Acc: 0.9645, Train F1: 0.9752 Val Loss: 0.0467, Val Acc: 0.9643, Val F1: 0.9753\n",
            "Epoch: 20/75, Train Loss: 0.0387, Train Acc: 0.9677, Train F1: 0.9776 Val Loss: 0.0427, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 21/75, Train Loss: 0.0367, Train Acc: 0.9712, Train F1: 0.9800 Val Loss: 0.0470, Val Acc: 0.9608, Val F1: 0.9728\n",
            "Epoch: 22/75, Train Loss: 0.0370, Train Acc: 0.9708, Train F1: 0.9798 Val Loss: 0.0475, Val Acc: 0.9646, Val F1: 0.9754\n",
            "Epoch: 23/75, Train Loss: 0.0366, Train Acc: 0.9688, Train F1: 0.9782 Val Loss: 0.0457, Val Acc: 0.9663, Val F1: 0.9767\n",
            "Epoch: 24/75, Train Loss: 0.0378, Train Acc: 0.9675, Train F1: 0.9773 Val Loss: 0.0419, Val Acc: 0.9698, Val F1: 0.9792\n",
            "Epoch: 25/75, Train Loss: 0.0351, Train Acc: 0.9726, Train F1: 0.9811 Val Loss: 0.0422, Val Acc: 0.9632, Val F1: 0.9746\n",
            "Epoch: 26/75, Train Loss: 0.0401, Train Acc: 0.9637, Train F1: 0.9745 Val Loss: 0.0430, Val Acc: 0.9653, Val F1: 0.9760\n",
            "Epoch: 27/75, Train Loss: 0.0342, Train Acc: 0.9734, Train F1: 0.9816 Val Loss: 0.0458, Val Acc: 0.9639, Val F1: 0.9754\n",
            "Epoch: 28/75, Train Loss: 0.0343, Train Acc: 0.9732, Train F1: 0.9814 Val Loss: 0.0432, Val Acc: 0.9660, Val F1: 0.9767\n",
            "Epoch: 29/75, Train Loss: 0.0366, Train Acc: 0.9698, Train F1: 0.9790 Val Loss: 0.0443, Val Acc: 0.9656, Val F1: 0.9761\n",
            "Epoch: 30/75, Train Loss: 0.0364, Train Acc: 0.9725, Train F1: 0.9808 Val Loss: 0.0445, Val Acc: 0.9619, Val F1: 0.9734\n",
            "Epoch: 31/75, Train Loss: 0.0337, Train Acc: 0.9741, Train F1: 0.9819 Val Loss: 0.0459, Val Acc: 0.9605, Val F1: 0.9722\n",
            "Epoch: 32/75, Train Loss: 0.0336, Train Acc: 0.9732, Train F1: 0.9813 Val Loss: 0.0421, Val Acc: 0.9680, Val F1: 0.9781\n",
            "Epoch: 33/75, Train Loss: 0.0322, Train Acc: 0.9766, Train F1: 0.9838 Val Loss: 0.0411, Val Acc: 0.9674, Val F1: 0.9776\n",
            "Epoch: 34/75, Train Loss: 0.0326, Train Acc: 0.9745, Train F1: 0.9821 Val Loss: 0.0380, Val Acc: 0.9711, Val F1: 0.9800\n",
            "Epoch: 35/75, Train Loss: 0.0329, Train Acc: 0.9734, Train F1: 0.9813 Val Loss: 0.0399, Val Acc: 0.9677, Val F1: 0.9776\n",
            "Epoch: 36/75, Train Loss: 0.0318, Train Acc: 0.9766, Train F1: 0.9838 Val Loss: 0.0438, Val Acc: 0.9680, Val F1: 0.9782\n",
            "Epoch: 37/75, Train Loss: 0.0314, Train Acc: 0.9764, Train F1: 0.9834 Val Loss: 0.0392, Val Acc: 0.9698, Val F1: 0.9790\n",
            "Epoch: 38/75, Train Loss: 0.0310, Train Acc: 0.9770, Train F1: 0.9840 Val Loss: 0.0409, Val Acc: 0.9663, Val F1: 0.9765\n",
            "Epoch: 39/75, Train Loss: 0.0314, Train Acc: 0.9762, Train F1: 0.9832 Val Loss: 0.0384, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 40/75, Train Loss: 0.0310, Train Acc: 0.9767, Train F1: 0.9838 Val Loss: 0.0367, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 41/75, Train Loss: 0.0296, Train Acc: 0.9793, Train F1: 0.9855 Val Loss: 0.0387, Val Acc: 0.9670, Val F1: 0.9773\n",
            "Epoch: 42/75, Train Loss: 0.0291, Train Acc: 0.9792, Train F1: 0.9854 Val Loss: 0.0377, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 43/75, Train Loss: 0.0291, Train Acc: 0.9794, Train F1: 0.9855 Val Loss: 0.0396, Val Acc: 0.9639, Val F1: 0.9748\n",
            "Epoch: 44/75, Train Loss: 0.0295, Train Acc: 0.9780, Train F1: 0.9848 Val Loss: 0.0398, Val Acc: 0.9715, Val F1: 0.9805\n",
            "Epoch: 45/75, Train Loss: 0.0303, Train Acc: 0.9762, Train F1: 0.9833 Val Loss: 0.0397, Val Acc: 0.9698, Val F1: 0.9794\n",
            "Epoch: 46/75, Train Loss: 0.0286, Train Acc: 0.9798, Train F1: 0.9857 Val Loss: 0.0391, Val Acc: 0.9677, Val F1: 0.9775\n",
            "Epoch: 47/75, Train Loss: 0.0285, Train Acc: 0.9819, Train F1: 0.9872 Val Loss: 0.0400, Val Acc: 0.9698, Val F1: 0.9794\n",
            "Epoch: 48/75, Train Loss: 0.0275, Train Acc: 0.9821, Train F1: 0.9873 Val Loss: 0.0385, Val Acc: 0.9718, Val F1: 0.9807\n",
            "Epoch: 49/75, Train Loss: 0.0281, Train Acc: 0.9809, Train F1: 0.9866 Val Loss: 0.0353, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 50/75, Train Loss: 0.0276, Train Acc: 0.9812, Train F1: 0.9869 Val Loss: 0.0356, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 51/75, Train Loss: 0.0281, Train Acc: 0.9811, Train F1: 0.9868 Val Loss: 0.0349, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 52/75, Train Loss: 0.0274, Train Acc: 0.9812, Train F1: 0.9868 Val Loss: 0.0348, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 53/75, Train Loss: 0.0268, Train Acc: 0.9822, Train F1: 0.9877 Val Loss: 0.0359, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 54/75, Train Loss: 0.0264, Train Acc: 0.9842, Train F1: 0.9889 Val Loss: 0.0351, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 55/75, Train Loss: 0.0265, Train Acc: 0.9837, Train F1: 0.9886 Val Loss: 0.0360, Val Acc: 0.9739, Val F1: 0.9821\n",
            "Epoch: 56/75, Train Loss: 0.0261, Train Acc: 0.9832, Train F1: 0.9884 Val Loss: 0.0354, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 57/75, Train Loss: 0.0256, Train Acc: 0.9851, Train F1: 0.9896 Val Loss: 0.0342, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 58/75, Train Loss: 0.0255, Train Acc: 0.9845, Train F1: 0.9892 Val Loss: 0.0347, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 59/75, Train Loss: 0.0249, Train Acc: 0.9855, Train F1: 0.9898 Val Loss: 0.0338, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 60/75, Train Loss: 0.0251, Train Acc: 0.9859, Train F1: 0.9901 Val Loss: 0.0348, Val Acc: 0.9732, Val F1: 0.9814\n",
            "Epoch: 61/75, Train Loss: 0.0254, Train Acc: 0.9845, Train F1: 0.9894 Val Loss: 0.0339, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 62/75, Train Loss: 0.0249, Train Acc: 0.9859, Train F1: 0.9902 Val Loss: 0.0341, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 63/75, Train Loss: 0.0246, Train Acc: 0.9853, Train F1: 0.9897 Val Loss: 0.0345, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 64/75, Train Loss: 0.0245, Train Acc: 0.9860, Train F1: 0.9903 Val Loss: 0.0336, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 65/75, Train Loss: 0.0244, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0337, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 66/75, Train Loss: 0.0242, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0338, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 67/75, Train Loss: 0.0241, Train Acc: 0.9858, Train F1: 0.9900 Val Loss: 0.0335, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 68/75, Train Loss: 0.0241, Train Acc: 0.9872, Train F1: 0.9910 Val Loss: 0.0334, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 69/75, Train Loss: 0.0239, Train Acc: 0.9865, Train F1: 0.9905 Val Loss: 0.0335, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 70/75, Train Loss: 0.0238, Train Acc: 0.9868, Train F1: 0.9907 Val Loss: 0.0337, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 71/75, Train Loss: 0.0238, Train Acc: 0.9871, Train F1: 0.9910 Val Loss: 0.0336, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 72/75, Train Loss: 0.0237, Train Acc: 0.9873, Train F1: 0.9911 Val Loss: 0.0336, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 73/75, Train Loss: 0.0236, Train Acc: 0.9868, Train F1: 0.9909 Val Loss: 0.0336, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 74/75, Train Loss: 0.0236, Train Acc: 0.9872, Train F1: 0.9910 Val Loss: 0.0335, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 75/75, Train Loss: 0.0236, Train Acc: 0.9873, Train F1: 0.9910 Val Loss: 0.0335, Val Acc: 0.9763, Val F1: 0.9836\n",
            "\n",
            " 🔎 search 18 : deep_rescnn --- lr : 0.0009144615755940233, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv4.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1320, Train Acc: 0.8424, Train F1: 0.8938 Val Loss: 0.0979, Val Acc: 0.8966, Val F1: 0.9258\n",
            "Epoch: 2/75, Train Loss: 0.0857, Train Acc: 0.9110, Train F1: 0.9381 Val Loss: 0.0742, Val Acc: 0.9361, Val F1: 0.9556\n",
            "Epoch: 3/75, Train Loss: 0.0652, Train Acc: 0.9344, Train F1: 0.9545 Val Loss: 0.0611, Val Acc: 0.9491, Val F1: 0.9652\n",
            "Epoch: 4/75, Train Loss: 0.0572, Train Acc: 0.9447, Train F1: 0.9615 Val Loss: 0.0673, Val Acc: 0.9302, Val F1: 0.9502\n",
            "Epoch: 5/75, Train Loss: 0.0571, Train Acc: 0.9434, Train F1: 0.9606 Val Loss: 0.0608, Val Acc: 0.9419, Val F1: 0.9589\n",
            "Epoch: 6/75, Train Loss: 0.0519, Train Acc: 0.9507, Train F1: 0.9655 Val Loss: 0.0568, Val Acc: 0.9533, Val F1: 0.9684\n",
            "Epoch: 7/75, Train Loss: 0.0496, Train Acc: 0.9529, Train F1: 0.9674 Val Loss: 0.0525, Val Acc: 0.9564, Val F1: 0.9694\n",
            "Epoch: 8/75, Train Loss: 0.0446, Train Acc: 0.9585, Train F1: 0.9711 Val Loss: 0.0510, Val Acc: 0.9526, Val F1: 0.9667\n",
            "Epoch: 9/75, Train Loss: 0.0434, Train Acc: 0.9623, Train F1: 0.9736 Val Loss: 0.0503, Val Acc: 0.9474, Val F1: 0.9628\n",
            "Epoch: 10/75, Train Loss: 0.0402, Train Acc: 0.9647, Train F1: 0.9755 Val Loss: 0.0415, Val Acc: 0.9694, Val F1: 0.9789\n",
            "Epoch: 11/75, Train Loss: 0.0432, Train Acc: 0.9603, Train F1: 0.9722 Val Loss: 0.0446, Val Acc: 0.9674, Val F1: 0.9775\n",
            "Epoch: 12/75, Train Loss: 0.0376, Train Acc: 0.9670, Train F1: 0.9772 Val Loss: 0.0550, Val Acc: 0.9447, Val F1: 0.9630\n",
            "Epoch: 13/75, Train Loss: 0.0390, Train Acc: 0.9659, Train F1: 0.9762 Val Loss: 0.0415, Val Acc: 0.9646, Val F1: 0.9755\n",
            "Epoch: 14/75, Train Loss: 0.0357, Train Acc: 0.9692, Train F1: 0.9786 Val Loss: 0.0398, Val Acc: 0.9722, Val F1: 0.9808\n",
            "Epoch: 15/75, Train Loss: 0.0356, Train Acc: 0.9683, Train F1: 0.9779 Val Loss: 0.0483, Val Acc: 0.9622, Val F1: 0.9743\n",
            "Epoch: 16/75, Train Loss: 0.0377, Train Acc: 0.9691, Train F1: 0.9785 Val Loss: 0.0733, Val Acc: 0.9182, Val F1: 0.9462\n",
            "Epoch: 17/75, Train Loss: 0.0385, Train Acc: 0.9646, Train F1: 0.9754 Val Loss: 0.0406, Val Acc: 0.9670, Val F1: 0.9770\n",
            "Epoch: 18/75, Train Loss: 0.0354, Train Acc: 0.9722, Train F1: 0.9806 Val Loss: 0.0386, Val Acc: 0.9711, Val F1: 0.9802\n",
            "Epoch: 19/75, Train Loss: 0.0364, Train Acc: 0.9679, Train F1: 0.9774 Val Loss: 0.0446, Val Acc: 0.9649, Val F1: 0.9755\n",
            "Epoch: 20/75, Train Loss: 0.0315, Train Acc: 0.9756, Train F1: 0.9829 Val Loss: 0.0392, Val Acc: 0.9646, Val F1: 0.9752\n",
            "Epoch: 21/75, Train Loss: 0.0296, Train Acc: 0.9765, Train F1: 0.9834 Val Loss: 0.0586, Val Acc: 0.9409, Val F1: 0.9606\n",
            "Epoch: 22/75, Train Loss: 0.0314, Train Acc: 0.9746, Train F1: 0.9822 Val Loss: 0.0359, Val Acc: 0.9698, Val F1: 0.9792\n",
            "Epoch: 23/75, Train Loss: 0.0311, Train Acc: 0.9751, Train F1: 0.9827 Val Loss: 0.0452, Val Acc: 0.9564, Val F1: 0.9691\n",
            "Epoch: 24/75, Train Loss: 0.0281, Train Acc: 0.9794, Train F1: 0.9856 Val Loss: 0.0359, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 25/75, Train Loss: 0.0313, Train Acc: 0.9746, Train F1: 0.9822 Val Loss: 0.0372, Val Acc: 0.9711, Val F1: 0.9800\n",
            "Epoch: 26/75, Train Loss: 0.0347, Train Acc: 0.9690, Train F1: 0.9784 Val Loss: 0.0435, Val Acc: 0.9646, Val F1: 0.9759\n",
            "Epoch: 27/75, Train Loss: 0.0328, Train Acc: 0.9718, Train F1: 0.9803 Val Loss: 0.0387, Val Acc: 0.9677, Val F1: 0.9776\n",
            "Epoch: 28/75, Train Loss: 0.0276, Train Acc: 0.9798, Train F1: 0.9858 Val Loss: 0.0359, Val Acc: 0.9722, Val F1: 0.9809\n",
            "Epoch: 29/75, Train Loss: 0.0263, Train Acc: 0.9812, Train F1: 0.9869 Val Loss: 0.0366, Val Acc: 0.9698, Val F1: 0.9790\n",
            "Epoch: 30/75, Train Loss: 0.0287, Train Acc: 0.9770, Train F1: 0.9837 Val Loss: 0.0390, Val Acc: 0.9680, Val F1: 0.9782\n",
            "Epoch: 31/75, Train Loss: 0.0271, Train Acc: 0.9808, Train F1: 0.9864 Val Loss: 0.0385, Val Acc: 0.9667, Val F1: 0.9773\n",
            "Epoch: 32/75, Train Loss: 0.0304, Train Acc: 0.9733, Train F1: 0.9814 Val Loss: 0.0591, Val Acc: 0.9436, Val F1: 0.9624\n",
            "Epoch: 33/75, Train Loss: 0.0282, Train Acc: 0.9793, Train F1: 0.9855 Val Loss: 0.0403, Val Acc: 0.9629, Val F1: 0.9740\n",
            "Epoch: 34/75, Train Loss: 0.0274, Train Acc: 0.9780, Train F1: 0.9846 Val Loss: 0.0403, Val Acc: 0.9670, Val F1: 0.9775\n",
            "Epoch: 35/75, Train Loss: 0.0292, Train Acc: 0.9763, Train F1: 0.9835 Val Loss: 0.0362, Val Acc: 0.9742, Val F1: 0.9821\n",
            "Epoch: 36/75, Train Loss: 0.0254, Train Acc: 0.9813, Train F1: 0.9869 Val Loss: 0.0326, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 37/75, Train Loss: 0.0242, Train Acc: 0.9830, Train F1: 0.9881 Val Loss: 0.0321, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 38/75, Train Loss: 0.0264, Train Acc: 0.9784, Train F1: 0.9847 Val Loss: 0.0331, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 39/75, Train Loss: 0.0241, Train Acc: 0.9838, Train F1: 0.9888 Val Loss: 0.0330, Val Acc: 0.9742, Val F1: 0.9821\n",
            "Epoch: 40/75, Train Loss: 0.0282, Train Acc: 0.9772, Train F1: 0.9840 Val Loss: 0.0402, Val Acc: 0.9680, Val F1: 0.9776\n",
            "Epoch: 41/75, Train Loss: 0.0243, Train Acc: 0.9824, Train F1: 0.9877 Val Loss: 0.0323, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 42/75, Train Loss: 0.0226, Train Acc: 0.9850, Train F1: 0.9895 Val Loss: 0.0329, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 43/75, Train Loss: 0.0258, Train Acc: 0.9794, Train F1: 0.9855 Val Loss: 0.0410, Val Acc: 0.9660, Val F1: 0.9761\n",
            "Epoch: 44/75, Train Loss: 0.0227, Train Acc: 0.9849, Train F1: 0.9894 Val Loss: 0.0312, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 45/75, Train Loss: 0.0244, Train Acc: 0.9835, Train F1: 0.9885 Val Loss: 0.0336, Val Acc: 0.9739, Val F1: 0.9821\n",
            "Epoch: 46/75, Train Loss: 0.0226, Train Acc: 0.9850, Train F1: 0.9895 Val Loss: 0.0321, Val Acc: 0.9749, Val F1: 0.9826\n",
            "Epoch: 47/75, Train Loss: 0.0239, Train Acc: 0.9840, Train F1: 0.9888 Val Loss: 0.0316, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 48/75, Train Loss: 0.0228, Train Acc: 0.9847, Train F1: 0.9893 Val Loss: 0.0304, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 49/75, Train Loss: 0.0217, Train Acc: 0.9872, Train F1: 0.9909 Val Loss: 0.0312, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 50/75, Train Loss: 0.0222, Train Acc: 0.9856, Train F1: 0.9898 Val Loss: 0.0320, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 51/75, Train Loss: 0.0216, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0301, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 52/75, Train Loss: 0.0211, Train Acc: 0.9882, Train F1: 0.9917 Val Loss: 0.0391, Val Acc: 0.9649, Val F1: 0.9753\n",
            "Epoch: 53/75, Train Loss: 0.0214, Train Acc: 0.9876, Train F1: 0.9913 Val Loss: 0.0336, Val Acc: 0.9753, Val F1: 0.9831\n",
            "Epoch: 54/75, Train Loss: 0.0211, Train Acc: 0.9873, Train F1: 0.9912 Val Loss: 0.0300, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 55/75, Train Loss: 0.0206, Train Acc: 0.9877, Train F1: 0.9914 Val Loss: 0.0303, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 56/75, Train Loss: 0.0203, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0295, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 57/75, Train Loss: 0.0198, Train Acc: 0.9885, Train F1: 0.9920 Val Loss: 0.0301, Val Acc: 0.9756, Val F1: 0.9833\n",
            "Epoch: 58/75, Train Loss: 0.0193, Train Acc: 0.9896, Train F1: 0.9927 Val Loss: 0.0294, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 59/75, Train Loss: 0.0196, Train Acc: 0.9887, Train F1: 0.9922 Val Loss: 0.0295, Val Acc: 0.9759, Val F1: 0.9835\n",
            "Epoch: 60/75, Train Loss: 0.0201, Train Acc: 0.9879, Train F1: 0.9915 Val Loss: 0.0296, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 61/75, Train Loss: 0.0195, Train Acc: 0.9892, Train F1: 0.9925 Val Loss: 0.0303, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 62/75, Train Loss: 0.0195, Train Acc: 0.9884, Train F1: 0.9919 Val Loss: 0.0321, Val Acc: 0.9756, Val F1: 0.9833\n",
            "Epoch: 63/75, Train Loss: 0.0194, Train Acc: 0.9883, Train F1: 0.9918 Val Loss: 0.0295, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 64/75, Train Loss: 0.0192, Train Acc: 0.9891, Train F1: 0.9924 Val Loss: 0.0298, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 65/75, Train Loss: 0.0187, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0295, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 66/75, Train Loss: 0.0187, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0292, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 67/75, Train Loss: 0.0187, Train Acc: 0.9904, Train F1: 0.9934 Val Loss: 0.0292, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 68/75, Train Loss: 0.0186, Train Acc: 0.9907, Train F1: 0.9934 Val Loss: 0.0291, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 69/75, Train Loss: 0.0186, Train Acc: 0.9895, Train F1: 0.9927 Val Loss: 0.0293, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 70/75, Train Loss: 0.0184, Train Acc: 0.9907, Train F1: 0.9934 Val Loss: 0.0292, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 71/75, Train Loss: 0.0184, Train Acc: 0.9901, Train F1: 0.9931 Val Loss: 0.0291, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 72/75, Train Loss: 0.0183, Train Acc: 0.9908, Train F1: 0.9936 Val Loss: 0.0292, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 73/75, Train Loss: 0.0182, Train Acc: 0.9907, Train F1: 0.9935 Val Loss: 0.0291, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 74/75, Train Loss: 0.0182, Train Acc: 0.9906, Train F1: 0.9934 Val Loss: 0.0291, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 75/75, Train Loss: 0.0182, Train Acc: 0.9908, Train F1: 0.9936 Val Loss: 0.0291, Val Acc: 0.9770, Val F1: 0.9841\n",
            "\n",
            " 🔎 search 19 : deep_rescnn --- lr : 0.0009144615755940233, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv5.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1295, Train Acc: 0.8498, Train F1: 0.8961 Val Loss: 0.0888, Val Acc: 0.9082, Val F1: 0.9353\n",
            "Epoch: 2/75, Train Loss: 0.0770, Train Acc: 0.9214, Train F1: 0.9455 Val Loss: 0.0653, Val Acc: 0.9416, Val F1: 0.9594\n",
            "Epoch: 3/75, Train Loss: 0.0622, Train Acc: 0.9368, Train F1: 0.9559 Val Loss: 0.0623, Val Acc: 0.9368, Val F1: 0.9574\n",
            "Epoch: 4/75, Train Loss: 0.0547, Train Acc: 0.9475, Train F1: 0.9634 Val Loss: 0.0556, Val Acc: 0.9502, Val F1: 0.9661\n",
            "Epoch: 5/75, Train Loss: 0.0513, Train Acc: 0.9517, Train F1: 0.9663 Val Loss: 0.0540, Val Acc: 0.9509, Val F1: 0.9655\n",
            "Epoch: 6/75, Train Loss: 0.0474, Train Acc: 0.9556, Train F1: 0.9691 Val Loss: 0.0467, Val Acc: 0.9636, Val F1: 0.9747\n",
            "Epoch: 7/75, Train Loss: 0.0444, Train Acc: 0.9603, Train F1: 0.9720 Val Loss: 0.0447, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 8/75, Train Loss: 0.0434, Train Acc: 0.9608, Train F1: 0.9727 Val Loss: 0.0625, Val Acc: 0.9340, Val F1: 0.9561\n",
            "Epoch: 9/75, Train Loss: 0.0384, Train Acc: 0.9687, Train F1: 0.9782 Val Loss: 0.0439, Val Acc: 0.9598, Val F1: 0.9718\n",
            "Epoch: 10/75, Train Loss: 0.0389, Train Acc: 0.9660, Train F1: 0.9762 Val Loss: 0.0405, Val Acc: 0.9663, Val F1: 0.9766\n",
            "Epoch: 11/75, Train Loss: 0.0380, Train Acc: 0.9668, Train F1: 0.9768 Val Loss: 0.0400, Val Acc: 0.9639, Val F1: 0.9748\n",
            "Epoch: 12/75, Train Loss: 0.0378, Train Acc: 0.9671, Train F1: 0.9768 Val Loss: 0.0397, Val Acc: 0.9708, Val F1: 0.9797\n",
            "Epoch: 13/75, Train Loss: 0.0378, Train Acc: 0.9686, Train F1: 0.9781 Val Loss: 0.0585, Val Acc: 0.9378, Val F1: 0.9554\n",
            "Epoch: 14/75, Train Loss: 0.0363, Train Acc: 0.9696, Train F1: 0.9786 Val Loss: 0.0573, Val Acc: 0.9440, Val F1: 0.9626\n",
            "Epoch: 15/75, Train Loss: 0.0333, Train Acc: 0.9733, Train F1: 0.9816 Val Loss: 0.0367, Val Acc: 0.9691, Val F1: 0.9785\n",
            "Epoch: 16/75, Train Loss: 0.0311, Train Acc: 0.9764, Train F1: 0.9834 Val Loss: 0.0369, Val Acc: 0.9735, Val F1: 0.9819\n",
            "Epoch: 17/75, Train Loss: 0.0341, Train Acc: 0.9734, Train F1: 0.9814 Val Loss: 0.0354, Val Acc: 0.9708, Val F1: 0.9798\n",
            "Epoch: 18/75, Train Loss: 0.0310, Train Acc: 0.9765, Train F1: 0.9835 Val Loss: 0.0381, Val Acc: 0.9698, Val F1: 0.9788\n",
            "Epoch: 19/75, Train Loss: 0.0320, Train Acc: 0.9746, Train F1: 0.9822 Val Loss: 0.0524, Val Acc: 0.9481, Val F1: 0.9651\n",
            "Epoch: 20/75, Train Loss: 0.0299, Train Acc: 0.9784, Train F1: 0.9847 Val Loss: 0.0371, Val Acc: 0.9653, Val F1: 0.9760\n",
            "Epoch: 21/75, Train Loss: 0.0311, Train Acc: 0.9763, Train F1: 0.9834 Val Loss: 0.0335, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 22/75, Train Loss: 0.0311, Train Acc: 0.9771, Train F1: 0.9840 Val Loss: 0.0380, Val Acc: 0.9677, Val F1: 0.9774\n",
            "Epoch: 23/75, Train Loss: 0.0296, Train Acc: 0.9781, Train F1: 0.9846 Val Loss: 0.0337, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 24/75, Train Loss: 0.0316, Train Acc: 0.9762, Train F1: 0.9835 Val Loss: 0.0520, Val Acc: 0.9519, Val F1: 0.9658\n",
            "Epoch: 25/75, Train Loss: 0.0282, Train Acc: 0.9805, Train F1: 0.9865 Val Loss: 0.0357, Val Acc: 0.9701, Val F1: 0.9791\n",
            "Epoch: 26/75, Train Loss: 0.0298, Train Acc: 0.9767, Train F1: 0.9839 Val Loss: 0.0359, Val Acc: 0.9711, Val F1: 0.9799\n",
            "Epoch: 27/75, Train Loss: 0.0266, Train Acc: 0.9827, Train F1: 0.9879 Val Loss: 0.0328, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 28/75, Train Loss: 0.0269, Train Acc: 0.9806, Train F1: 0.9864 Val Loss: 0.0327, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 29/75, Train Loss: 0.0280, Train Acc: 0.9787, Train F1: 0.9850 Val Loss: 0.0491, Val Acc: 0.9505, Val F1: 0.9648\n",
            "Epoch: 30/75, Train Loss: 0.0281, Train Acc: 0.9802, Train F1: 0.9860 Val Loss: 0.0360, Val Acc: 0.9687, Val F1: 0.9782\n",
            "Epoch: 31/75, Train Loss: 0.0273, Train Acc: 0.9790, Train F1: 0.9855 Val Loss: 0.0396, Val Acc: 0.9653, Val F1: 0.9764\n",
            "Epoch: 32/75, Train Loss: 0.0269, Train Acc: 0.9804, Train F1: 0.9863 Val Loss: 0.0354, Val Acc: 0.9708, Val F1: 0.9801\n",
            "Epoch: 33/75, Train Loss: 0.0256, Train Acc: 0.9818, Train F1: 0.9872 Val Loss: 0.0394, Val Acc: 0.9694, Val F1: 0.9792\n",
            "Epoch: 34/75, Train Loss: 0.0253, Train Acc: 0.9834, Train F1: 0.9885 Val Loss: 0.0306, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 35/75, Train Loss: 0.0248, Train Acc: 0.9840, Train F1: 0.9888 Val Loss: 0.0329, Val Acc: 0.9780, Val F1: 0.9849\n",
            "Epoch: 36/75, Train Loss: 0.0248, Train Acc: 0.9826, Train F1: 0.9878 Val Loss: 0.0303, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 37/75, Train Loss: 0.0248, Train Acc: 0.9838, Train F1: 0.9888 Val Loss: 0.0303, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 38/75, Train Loss: 0.0244, Train Acc: 0.9847, Train F1: 0.9892 Val Loss: 0.0317, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 39/75, Train Loss: 0.0233, Train Acc: 0.9855, Train F1: 0.9897 Val Loss: 0.0348, Val Acc: 0.9691, Val F1: 0.9784\n",
            "Epoch: 40/75, Train Loss: 0.0255, Train Acc: 0.9820, Train F1: 0.9872 Val Loss: 0.0341, Val Acc: 0.9746, Val F1: 0.9826\n",
            "Epoch: 41/75, Train Loss: 0.0234, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0311, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 42/75, Train Loss: 0.0231, Train Acc: 0.9864, Train F1: 0.9905 Val Loss: 0.0313, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 43/75, Train Loss: 0.0241, Train Acc: 0.9841, Train F1: 0.9889 Val Loss: 0.0298, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 44/75, Train Loss: 0.0238, Train Acc: 0.9844, Train F1: 0.9892 Val Loss: 0.0308, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 45/75, Train Loss: 0.0224, Train Acc: 0.9869, Train F1: 0.9909 Val Loss: 0.0312, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 46/75, Train Loss: 0.0231, Train Acc: 0.9860, Train F1: 0.9902 Val Loss: 0.0298, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 47/75, Train Loss: 0.0230, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0320, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 48/75, Train Loss: 0.0226, Train Acc: 0.9865, Train F1: 0.9907 Val Loss: 0.0319, Val Acc: 0.9739, Val F1: 0.9818\n",
            "Epoch: 49/75, Train Loss: 0.0218, Train Acc: 0.9881, Train F1: 0.9917 Val Loss: 0.0296, Val Acc: 0.9811, Val F1: 0.9870\n",
            "Epoch: 50/75, Train Loss: 0.0225, Train Acc: 0.9877, Train F1: 0.9914 Val Loss: 0.0314, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 51/75, Train Loss: 0.0215, Train Acc: 0.9882, Train F1: 0.9918 Val Loss: 0.0293, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 52/75, Train Loss: 0.0222, Train Acc: 0.9869, Train F1: 0.9909 Val Loss: 0.0312, Val Acc: 0.9749, Val F1: 0.9825\n",
            "Epoch: 53/75, Train Loss: 0.0220, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0304, Val Acc: 0.9763, Val F1: 0.9835\n",
            "Epoch: 54/75, Train Loss: 0.0220, Train Acc: 0.9883, Train F1: 0.9918 Val Loss: 0.0314, Val Acc: 0.9732, Val F1: 0.9814\n",
            "Epoch: 55/75, Train Loss: 0.0212, Train Acc: 0.9888, Train F1: 0.9922 Val Loss: 0.0316, Val Acc: 0.9784, Val F1: 0.9852\n",
            "Epoch: 56/75, Train Loss: 0.0209, Train Acc: 0.9892, Train F1: 0.9926 Val Loss: 0.0303, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 57/75, Train Loss: 0.0206, Train Acc: 0.9896, Train F1: 0.9926 Val Loss: 0.0308, Val Acc: 0.9787, Val F1: 0.9854\n",
            "Epoch: 58/75, Train Loss: 0.0208, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0292, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 59/75, Train Loss: 0.0209, Train Acc: 0.9896, Train F1: 0.9926 Val Loss: 0.0305, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 60/75, Train Loss: 0.0204, Train Acc: 0.9901, Train F1: 0.9931 Val Loss: 0.0299, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 61/75, Train Loss: 0.0202, Train Acc: 0.9896, Train F1: 0.9928 Val Loss: 0.0291, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 62/75, Train Loss: 0.0201, Train Acc: 0.9896, Train F1: 0.9928 Val Loss: 0.0290, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 63/75, Train Loss: 0.0200, Train Acc: 0.9897, Train F1: 0.9928 Val Loss: 0.0289, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 64/75, Train Loss: 0.0199, Train Acc: 0.9906, Train F1: 0.9935 Val Loss: 0.0295, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 65/75, Train Loss: 0.0198, Train Acc: 0.9901, Train F1: 0.9932 Val Loss: 0.0288, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 66/75, Train Loss: 0.0198, Train Acc: 0.9903, Train F1: 0.9932 Val Loss: 0.0288, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 67/75, Train Loss: 0.0198, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0287, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 68/75, Train Loss: 0.0197, Train Acc: 0.9900, Train F1: 0.9931 Val Loss: 0.0288, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 69/75, Train Loss: 0.0196, Train Acc: 0.9911, Train F1: 0.9938 Val Loss: 0.0288, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 70/75, Train Loss: 0.0195, Train Acc: 0.9906, Train F1: 0.9933 Val Loss: 0.0287, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 71/75, Train Loss: 0.0195, Train Acc: 0.9905, Train F1: 0.9933 Val Loss: 0.0286, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 72/75, Train Loss: 0.0194, Train Acc: 0.9905, Train F1: 0.9934 Val Loss: 0.0287, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 73/75, Train Loss: 0.0194, Train Acc: 0.9906, Train F1: 0.9935 Val Loss: 0.0287, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 74/75, Train Loss: 0.0194, Train Acc: 0.9905, Train F1: 0.9934 Val Loss: 0.0287, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 75/75, Train Loss: 0.0193, Train Acc: 0.9905, Train F1: 0.9933 Val Loss: 0.0287, Val Acc: 0.9794, Val F1: 0.9858\n",
            "\n",
            " 🔎 search 20 : deep_rescnn --- lr : 0.0009144615755940233, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv2.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1417, Train Acc: 0.8337, Train F1: 0.8928 Val Loss: 0.1239, Val Acc: 0.8426, Val F1: 0.9002\n",
            "Epoch: 2/75, Train Loss: 0.0922, Train Acc: 0.9079, Train F1: 0.9363 Val Loss: 0.0984, Val Acc: 0.8966, Val F1: 0.9322\n",
            "Epoch: 3/75, Train Loss: 0.0863, Train Acc: 0.9140, Train F1: 0.9404 Val Loss: 0.0862, Val Acc: 0.9206, Val F1: 0.9467\n",
            "Epoch: 4/75, Train Loss: 0.0759, Train Acc: 0.9315, Train F1: 0.9523 Val Loss: 0.0816, Val Acc: 0.9220, Val F1: 0.9456\n",
            "Epoch: 5/75, Train Loss: 0.0721, Train Acc: 0.9316, Train F1: 0.9526 Val Loss: 0.0840, Val Acc: 0.9213, Val F1: 0.9479\n",
            "Epoch: 6/75, Train Loss: 0.0681, Train Acc: 0.9337, Train F1: 0.9539 Val Loss: 0.0827, Val Acc: 0.9144, Val F1: 0.9390\n",
            "Epoch: 7/75, Train Loss: 0.0750, Train Acc: 0.9268, Train F1: 0.9491 Val Loss: 0.0709, Val Acc: 0.9416, Val F1: 0.9597\n",
            "Epoch: 8/75, Train Loss: 0.0659, Train Acc: 0.9383, Train F1: 0.9570 Val Loss: 0.0651, Val Acc: 0.9450, Val F1: 0.9623\n",
            "Epoch: 9/75, Train Loss: 0.0602, Train Acc: 0.9433, Train F1: 0.9608 Val Loss: 0.0654, Val Acc: 0.9388, Val F1: 0.9584\n",
            "Epoch: 10/75, Train Loss: 0.0605, Train Acc: 0.9434, Train F1: 0.9609 Val Loss: 0.0634, Val Acc: 0.9409, Val F1: 0.9594\n",
            "Epoch: 11/75, Train Loss: 0.0636, Train Acc: 0.9360, Train F1: 0.9556 Val Loss: 0.0703, Val Acc: 0.9347, Val F1: 0.9562\n",
            "Epoch: 12/75, Train Loss: 0.0555, Train Acc: 0.9510, Train F1: 0.9662 Val Loss: 0.0692, Val Acc: 0.9337, Val F1: 0.9529\n",
            "Epoch: 13/75, Train Loss: 0.0551, Train Acc: 0.9472, Train F1: 0.9634 Val Loss: 0.0591, Val Acc: 0.9519, Val F1: 0.9673\n",
            "Epoch: 14/75, Train Loss: 0.0568, Train Acc: 0.9451, Train F1: 0.9621 Val Loss: 0.0610, Val Acc: 0.9450, Val F1: 0.9628\n",
            "Epoch: 15/75, Train Loss: 0.0551, Train Acc: 0.9490, Train F1: 0.9646 Val Loss: 0.0671, Val Acc: 0.9354, Val F1: 0.9544\n",
            "Epoch: 16/75, Train Loss: 0.0603, Train Acc: 0.9426, Train F1: 0.9604 Val Loss: 0.0644, Val Acc: 0.9426, Val F1: 0.9614\n",
            "Epoch: 17/75, Train Loss: 0.0511, Train Acc: 0.9558, Train F1: 0.9694 Val Loss: 0.0550, Val Acc: 0.9512, Val F1: 0.9662\n",
            "Epoch: 18/75, Train Loss: 0.0488, Train Acc: 0.9553, Train F1: 0.9692 Val Loss: 0.0554, Val Acc: 0.9584, Val F1: 0.9713\n",
            "Epoch: 19/75, Train Loss: 0.0464, Train Acc: 0.9589, Train F1: 0.9715 Val Loss: 0.0574, Val Acc: 0.9488, Val F1: 0.9653\n",
            "Epoch: 20/75, Train Loss: 0.0475, Train Acc: 0.9574, Train F1: 0.9707 Val Loss: 0.0575, Val Acc: 0.9502, Val F1: 0.9652\n",
            "Epoch: 21/75, Train Loss: 0.0486, Train Acc: 0.9575, Train F1: 0.9704 Val Loss: 0.0516, Val Acc: 0.9605, Val F1: 0.9729\n",
            "Epoch: 22/75, Train Loss: 0.0469, Train Acc: 0.9570, Train F1: 0.9703 Val Loss: 0.0732, Val Acc: 0.9216, Val F1: 0.9435\n",
            "Epoch: 23/75, Train Loss: 0.0449, Train Acc: 0.9604, Train F1: 0.9726 Val Loss: 0.0513, Val Acc: 0.9622, Val F1: 0.9742\n",
            "Epoch: 24/75, Train Loss: 0.0456, Train Acc: 0.9590, Train F1: 0.9717 Val Loss: 0.0542, Val Acc: 0.9519, Val F1: 0.9666\n",
            "Epoch: 25/75, Train Loss: 0.0453, Train Acc: 0.9619, Train F1: 0.9736 Val Loss: 0.0547, Val Acc: 0.9502, Val F1: 0.9650\n",
            "Epoch: 26/75, Train Loss: 0.0430, Train Acc: 0.9616, Train F1: 0.9733 Val Loss: 0.0613, Val Acc: 0.9471, Val F1: 0.9626\n",
            "Epoch: 27/75, Train Loss: 0.0437, Train Acc: 0.9623, Train F1: 0.9739 Val Loss: 0.0537, Val Acc: 0.9519, Val F1: 0.9673\n",
            "Epoch: 28/75, Train Loss: 0.0430, Train Acc: 0.9630, Train F1: 0.9744 Val Loss: 0.0568, Val Acc: 0.9423, Val F1: 0.9592\n",
            "Epoch: 29/75, Train Loss: 0.0438, Train Acc: 0.9595, Train F1: 0.9719 Val Loss: 0.0480, Val Acc: 0.9601, Val F1: 0.9726\n",
            "Epoch: 30/75, Train Loss: 0.0468, Train Acc: 0.9543, Train F1: 0.9684 Val Loss: 0.0556, Val Acc: 0.9471, Val F1: 0.9629\n",
            "Epoch: 31/75, Train Loss: 0.0423, Train Acc: 0.9622, Train F1: 0.9738 Val Loss: 0.0516, Val Acc: 0.9560, Val F1: 0.9702\n",
            "Epoch: 32/75, Train Loss: 0.0474, Train Acc: 0.9552, Train F1: 0.9692 Val Loss: 0.0491, Val Acc: 0.9625, Val F1: 0.9743\n",
            "Epoch: 33/75, Train Loss: 0.0416, Train Acc: 0.9660, Train F1: 0.9764 Val Loss: 0.0525, Val Acc: 0.9529, Val F1: 0.9670\n",
            "Epoch: 34/75, Train Loss: 0.0458, Train Acc: 0.9581, Train F1: 0.9709 Val Loss: 0.0502, Val Acc: 0.9615, Val F1: 0.9736\n",
            "Epoch: 35/75, Train Loss: 0.0407, Train Acc: 0.9669, Train F1: 0.9771 Val Loss: 0.0499, Val Acc: 0.9601, Val F1: 0.9729\n",
            "Epoch: 36/75, Train Loss: 0.0413, Train Acc: 0.9655, Train F1: 0.9762 Val Loss: 0.0581, Val Acc: 0.9440, Val F1: 0.9623\n",
            "Epoch: 37/75, Train Loss: 0.0418, Train Acc: 0.9643, Train F1: 0.9753 Val Loss: 0.0496, Val Acc: 0.9615, Val F1: 0.9734\n",
            "Epoch: 38/75, Train Loss: 0.0416, Train Acc: 0.9636, Train F1: 0.9748 Val Loss: 0.0501, Val Acc: 0.9533, Val F1: 0.9673\n",
            "Epoch: 39/75, Train Loss: 0.0398, Train Acc: 0.9661, Train F1: 0.9765 Val Loss: 0.0534, Val Acc: 0.9546, Val F1: 0.9691\n",
            "Epoch: 40/75, Train Loss: 0.0380, Train Acc: 0.9687, Train F1: 0.9784 Val Loss: 0.0459, Val Acc: 0.9663, Val F1: 0.9768\n",
            "Epoch: 41/75, Train Loss: 0.0382, Train Acc: 0.9676, Train F1: 0.9776 Val Loss: 0.0472, Val Acc: 0.9656, Val F1: 0.9765\n",
            "Epoch: 42/75, Train Loss: 0.0388, Train Acc: 0.9691, Train F1: 0.9786 Val Loss: 0.0483, Val Acc: 0.9646, Val F1: 0.9759\n",
            "Epoch: 43/75, Train Loss: 0.0372, Train Acc: 0.9715, Train F1: 0.9801 Val Loss: 0.0474, Val Acc: 0.9674, Val F1: 0.9775\n",
            "Epoch: 44/75, Train Loss: 0.0371, Train Acc: 0.9710, Train F1: 0.9800 Val Loss: 0.0479, Val Acc: 0.9595, Val F1: 0.9719\n",
            "Epoch: 45/75, Train Loss: 0.0394, Train Acc: 0.9666, Train F1: 0.9769 Val Loss: 0.0525, Val Acc: 0.9553, Val F1: 0.9698\n",
            "Epoch: 46/75, Train Loss: 0.0369, Train Acc: 0.9715, Train F1: 0.9804 Val Loss: 0.0468, Val Acc: 0.9646, Val F1: 0.9756\n",
            "Epoch: 47/75, Train Loss: 0.0367, Train Acc: 0.9707, Train F1: 0.9798 Val Loss: 0.0456, Val Acc: 0.9660, Val F1: 0.9764\n",
            "Epoch: 48/75, Train Loss: 0.0358, Train Acc: 0.9717, Train F1: 0.9804 Val Loss: 0.0452, Val Acc: 0.9663, Val F1: 0.9769\n",
            "Epoch: 49/75, Train Loss: 0.0366, Train Acc: 0.9712, Train F1: 0.9800 Val Loss: 0.0451, Val Acc: 0.9660, Val F1: 0.9766\n",
            "Epoch: 50/75, Train Loss: 0.0358, Train Acc: 0.9725, Train F1: 0.9810 Val Loss: 0.0451, Val Acc: 0.9649, Val F1: 0.9757\n",
            "Epoch: 51/75, Train Loss: 0.0356, Train Acc: 0.9724, Train F1: 0.9808 Val Loss: 0.0442, Val Acc: 0.9680, Val F1: 0.9781\n",
            "Epoch: 52/75, Train Loss: 0.0349, Train Acc: 0.9746, Train F1: 0.9824 Val Loss: 0.0505, Val Acc: 0.9536, Val F1: 0.9674\n",
            "Epoch: 53/75, Train Loss: 0.0349, Train Acc: 0.9745, Train F1: 0.9822 Val Loss: 0.0451, Val Acc: 0.9670, Val F1: 0.9773\n",
            "Epoch: 54/75, Train Loss: 0.0341, Train Acc: 0.9742, Train F1: 0.9821 Val Loss: 0.0446, Val Acc: 0.9643, Val F1: 0.9752\n",
            "Epoch: 55/75, Train Loss: 0.0337, Train Acc: 0.9762, Train F1: 0.9835 Val Loss: 0.0441, Val Acc: 0.9687, Val F1: 0.9786\n",
            "Epoch: 56/75, Train Loss: 0.0340, Train Acc: 0.9757, Train F1: 0.9832 Val Loss: 0.0440, Val Acc: 0.9670, Val F1: 0.9773\n",
            "Epoch: 57/75, Train Loss: 0.0337, Train Acc: 0.9753, Train F1: 0.9827 Val Loss: 0.0434, Val Acc: 0.9701, Val F1: 0.9795\n",
            "Epoch: 58/75, Train Loss: 0.0337, Train Acc: 0.9756, Train F1: 0.9831 Val Loss: 0.0446, Val Acc: 0.9670, Val F1: 0.9771\n",
            "Epoch: 59/75, Train Loss: 0.0334, Train Acc: 0.9753, Train F1: 0.9829 Val Loss: 0.0442, Val Acc: 0.9660, Val F1: 0.9764\n",
            "Epoch: 60/75, Train Loss: 0.0340, Train Acc: 0.9757, Train F1: 0.9833 Val Loss: 0.0437, Val Acc: 0.9677, Val F1: 0.9779\n",
            "Epoch: 61/75, Train Loss: 0.0334, Train Acc: 0.9765, Train F1: 0.9838 Val Loss: 0.0436, Val Acc: 0.9663, Val F1: 0.9768\n",
            "Epoch: 62/75, Train Loss: 0.0329, Train Acc: 0.9764, Train F1: 0.9836 Val Loss: 0.0433, Val Acc: 0.9701, Val F1: 0.9793\n",
            "Epoch: 63/75, Train Loss: 0.0329, Train Acc: 0.9766, Train F1: 0.9838 Val Loss: 0.0429, Val Acc: 0.9677, Val F1: 0.9777\n",
            "Epoch: 64/75, Train Loss: 0.0325, Train Acc: 0.9767, Train F1: 0.9839 Val Loss: 0.0427, Val Acc: 0.9694, Val F1: 0.9789\n",
            "Epoch: 65/75, Train Loss: 0.0333, Train Acc: 0.9759, Train F1: 0.9833 Val Loss: 0.0428, Val Acc: 0.9698, Val F1: 0.9792\n",
            "Epoch: 66/75, Train Loss: 0.0324, Train Acc: 0.9765, Train F1: 0.9835 Val Loss: 0.0432, Val Acc: 0.9704, Val F1: 0.9797\n",
            "Epoch: 67/75, Train Loss: 0.0322, Train Acc: 0.9782, Train F1: 0.9850 Val Loss: 0.0428, Val Acc: 0.9684, Val F1: 0.9783\n",
            "Epoch: 68/75, Train Loss: 0.0321, Train Acc: 0.9773, Train F1: 0.9843 Val Loss: 0.0423, Val Acc: 0.9684, Val F1: 0.9783\n",
            "Epoch: 69/75, Train Loss: 0.0319, Train Acc: 0.9786, Train F1: 0.9852 Val Loss: 0.0423, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 70/75, Train Loss: 0.0320, Train Acc: 0.9773, Train F1: 0.9843 Val Loss: 0.0423, Val Acc: 0.9698, Val F1: 0.9792\n",
            "Epoch: 71/75, Train Loss: 0.0319, Train Acc: 0.9786, Train F1: 0.9852 Val Loss: 0.0423, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 72/75, Train Loss: 0.0318, Train Acc: 0.9781, Train F1: 0.9849 Val Loss: 0.0423, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 73/75, Train Loss: 0.0317, Train Acc: 0.9777, Train F1: 0.9845 Val Loss: 0.0423, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 74/75, Train Loss: 0.0317, Train Acc: 0.9784, Train F1: 0.9850 Val Loss: 0.0423, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 75/75, Train Loss: 0.0317, Train Acc: 0.9781, Train F1: 0.9848 Val Loss: 0.0423, Val Acc: 0.9691, Val F1: 0.9787\n",
            "\n",
            " 🔎 search 21 : deep_rescnn --- lr : 0.0009144615755940233, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv3.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1253, Train Acc: 0.8519, Train F1: 0.8989 Val Loss: 0.0971, Val Acc: 0.8804, Val F1: 0.9218\n",
            "Epoch: 2/75, Train Loss: 0.0804, Train Acc: 0.9194, Train F1: 0.9445 Val Loss: 0.0891, Val Acc: 0.8986, Val F1: 0.9339\n",
            "Epoch: 3/75, Train Loss: 0.0667, Train Acc: 0.9380, Train F1: 0.9571 Val Loss: 0.0638, Val Acc: 0.9450, Val F1: 0.9621\n",
            "Epoch: 4/75, Train Loss: 0.0579, Train Acc: 0.9471, Train F1: 0.9632 Val Loss: 0.0576, Val Acc: 0.9536, Val F1: 0.9679\n",
            "Epoch: 5/75, Train Loss: 0.0520, Train Acc: 0.9533, Train F1: 0.9676 Val Loss: 0.0578, Val Acc: 0.9526, Val F1: 0.9677\n",
            "Epoch: 6/75, Train Loss: 0.0530, Train Acc: 0.9485, Train F1: 0.9643 Val Loss: 0.0576, Val Acc: 0.9533, Val F1: 0.9679\n",
            "Epoch: 7/75, Train Loss: 0.0467, Train Acc: 0.9586, Train F1: 0.9713 Val Loss: 0.0495, Val Acc: 0.9595, Val F1: 0.9720\n",
            "Epoch: 8/75, Train Loss: 0.0462, Train Acc: 0.9583, Train F1: 0.9709 Val Loss: 0.0509, Val Acc: 0.9632, Val F1: 0.9748\n",
            "Epoch: 9/75, Train Loss: 0.0456, Train Acc: 0.9581, Train F1: 0.9707 Val Loss: 0.0525, Val Acc: 0.9553, Val F1: 0.9686\n",
            "Epoch: 10/75, Train Loss: 0.0460, Train Acc: 0.9598, Train F1: 0.9721 Val Loss: 0.0479, Val Acc: 0.9643, Val F1: 0.9755\n",
            "Epoch: 11/75, Train Loss: 0.0429, Train Acc: 0.9625, Train F1: 0.9741 Val Loss: 0.0667, Val Acc: 0.9289, Val F1: 0.9488\n",
            "Epoch: 12/75, Train Loss: 0.0512, Train Acc: 0.9523, Train F1: 0.9673 Val Loss: 0.0487, Val Acc: 0.9619, Val F1: 0.9734\n",
            "Epoch: 13/75, Train Loss: 0.0399, Train Acc: 0.9664, Train F1: 0.9768 Val Loss: 0.0475, Val Acc: 0.9619, Val F1: 0.9738\n",
            "Epoch: 14/75, Train Loss: 0.0377, Train Acc: 0.9701, Train F1: 0.9793 Val Loss: 0.0498, Val Acc: 0.9533, Val F1: 0.9672\n",
            "Epoch: 15/75, Train Loss: 0.0410, Train Acc: 0.9646, Train F1: 0.9753 Val Loss: 0.0466, Val Acc: 0.9598, Val F1: 0.9719\n",
            "Epoch: 16/75, Train Loss: 0.0397, Train Acc: 0.9659, Train F1: 0.9764 Val Loss: 0.0439, Val Acc: 0.9629, Val F1: 0.9745\n",
            "Epoch: 17/75, Train Loss: 0.0368, Train Acc: 0.9695, Train F1: 0.9787 Val Loss: 0.0449, Val Acc: 0.9625, Val F1: 0.9738\n",
            "Epoch: 18/75, Train Loss: 0.0368, Train Acc: 0.9695, Train F1: 0.9788 Val Loss: 0.0440, Val Acc: 0.9667, Val F1: 0.9768\n",
            "Epoch: 19/75, Train Loss: 0.0352, Train Acc: 0.9734, Train F1: 0.9817 Val Loss: 0.0441, Val Acc: 0.9605, Val F1: 0.9725\n",
            "Epoch: 20/75, Train Loss: 0.0351, Train Acc: 0.9704, Train F1: 0.9795 Val Loss: 0.0428, Val Acc: 0.9674, Val F1: 0.9776\n",
            "Epoch: 21/75, Train Loss: 0.0364, Train Acc: 0.9682, Train F1: 0.9780 Val Loss: 0.0432, Val Acc: 0.9601, Val F1: 0.9725\n",
            "Epoch: 22/75, Train Loss: 0.0336, Train Acc: 0.9743, Train F1: 0.9821 Val Loss: 0.0398, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 23/75, Train Loss: 0.0389, Train Acc: 0.9653, Train F1: 0.9759 Val Loss: 0.0535, Val Acc: 0.9409, Val F1: 0.9581\n",
            "Epoch: 24/75, Train Loss: 0.0360, Train Acc: 0.9690, Train F1: 0.9783 Val Loss: 0.0411, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 25/75, Train Loss: 0.0347, Train Acc: 0.9725, Train F1: 0.9808 Val Loss: 0.0469, Val Acc: 0.9595, Val F1: 0.9725\n",
            "Epoch: 26/75, Train Loss: 0.0329, Train Acc: 0.9748, Train F1: 0.9825 Val Loss: 0.0419, Val Acc: 0.9625, Val F1: 0.9738\n",
            "Epoch: 27/75, Train Loss: 0.0325, Train Acc: 0.9730, Train F1: 0.9812 Val Loss: 0.0388, Val Acc: 0.9663, Val F1: 0.9765\n",
            "Epoch: 28/75, Train Loss: 0.0307, Train Acc: 0.9777, Train F1: 0.9845 Val Loss: 0.0375, Val Acc: 0.9684, Val F1: 0.9783\n",
            "Epoch: 29/75, Train Loss: 0.0351, Train Acc: 0.9699, Train F1: 0.9791 Val Loss: 0.0411, Val Acc: 0.9677, Val F1: 0.9780\n",
            "Epoch: 30/75, Train Loss: 0.0311, Train Acc: 0.9755, Train F1: 0.9829 Val Loss: 0.0418, Val Acc: 0.9663, Val F1: 0.9770\n",
            "Epoch: 31/75, Train Loss: 0.0321, Train Acc: 0.9743, Train F1: 0.9823 Val Loss: 0.0389, Val Acc: 0.9687, Val F1: 0.9784\n",
            "Epoch: 32/75, Train Loss: 0.0305, Train Acc: 0.9764, Train F1: 0.9836 Val Loss: 0.0363, Val Acc: 0.9725, Val F1: 0.9811\n",
            "Epoch: 33/75, Train Loss: 0.0280, Train Acc: 0.9811, Train F1: 0.9869 Val Loss: 0.0384, Val Acc: 0.9691, Val F1: 0.9788\n",
            "Epoch: 34/75, Train Loss: 0.0286, Train Acc: 0.9795, Train F1: 0.9857 Val Loss: 0.0424, Val Acc: 0.9625, Val F1: 0.9746\n",
            "Epoch: 35/75, Train Loss: 0.0296, Train Acc: 0.9786, Train F1: 0.9851 Val Loss: 0.0429, Val Acc: 0.9567, Val F1: 0.9696\n",
            "Epoch: 36/75, Train Loss: 0.0311, Train Acc: 0.9746, Train F1: 0.9823 Val Loss: 0.0356, Val Acc: 0.9694, Val F1: 0.9788\n",
            "Epoch: 37/75, Train Loss: 0.0293, Train Acc: 0.9774, Train F1: 0.9844 Val Loss: 0.0358, Val Acc: 0.9694, Val F1: 0.9789\n",
            "Epoch: 38/75, Train Loss: 0.0280, Train Acc: 0.9792, Train F1: 0.9855 Val Loss: 0.0606, Val Acc: 0.9296, Val F1: 0.9491\n",
            "Epoch: 39/75, Train Loss: 0.0282, Train Acc: 0.9798, Train F1: 0.9859 Val Loss: 0.0358, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 40/75, Train Loss: 0.0275, Train Acc: 0.9813, Train F1: 0.9870 Val Loss: 0.0353, Val Acc: 0.9694, Val F1: 0.9789\n",
            "Epoch: 41/75, Train Loss: 0.0295, Train Acc: 0.9775, Train F1: 0.9843 Val Loss: 0.0369, Val Acc: 0.9684, Val F1: 0.9783\n",
            "Epoch: 42/75, Train Loss: 0.0287, Train Acc: 0.9786, Train F1: 0.9851 Val Loss: 0.0530, Val Acc: 0.9433, Val F1: 0.9594\n",
            "Epoch: 43/75, Train Loss: 0.0295, Train Acc: 0.9774, Train F1: 0.9843 Val Loss: 0.0431, Val Acc: 0.9598, Val F1: 0.9717\n",
            "Epoch: 44/75, Train Loss: 0.0271, Train Acc: 0.9811, Train F1: 0.9868 Val Loss: 0.0342, Val Acc: 0.9722, Val F1: 0.9807\n",
            "Epoch: 45/75, Train Loss: 0.0272, Train Acc: 0.9809, Train F1: 0.9868 Val Loss: 0.0341, Val Acc: 0.9698, Val F1: 0.9790\n",
            "Epoch: 46/75, Train Loss: 0.0262, Train Acc: 0.9832, Train F1: 0.9883 Val Loss: 0.0341, Val Acc: 0.9711, Val F1: 0.9800\n",
            "Epoch: 47/75, Train Loss: 0.0268, Train Acc: 0.9806, Train F1: 0.9866 Val Loss: 0.0333, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 48/75, Train Loss: 0.0261, Train Acc: 0.9828, Train F1: 0.9881 Val Loss: 0.0349, Val Acc: 0.9718, Val F1: 0.9807\n",
            "Epoch: 49/75, Train Loss: 0.0256, Train Acc: 0.9836, Train F1: 0.9887 Val Loss: 0.0375, Val Acc: 0.9694, Val F1: 0.9791\n",
            "Epoch: 50/75, Train Loss: 0.0261, Train Acc: 0.9817, Train F1: 0.9872 Val Loss: 0.0324, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 51/75, Train Loss: 0.0259, Train Acc: 0.9812, Train F1: 0.9869 Val Loss: 0.0322, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 52/75, Train Loss: 0.0253, Train Acc: 0.9832, Train F1: 0.9883 Val Loss: 0.0344, Val Acc: 0.9684, Val F1: 0.9780\n",
            "Epoch: 53/75, Train Loss: 0.0249, Train Acc: 0.9842, Train F1: 0.9889 Val Loss: 0.0317, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 54/75, Train Loss: 0.0247, Train Acc: 0.9842, Train F1: 0.9890 Val Loss: 0.0320, Val Acc: 0.9732, Val F1: 0.9814\n",
            "Epoch: 55/75, Train Loss: 0.0243, Train Acc: 0.9845, Train F1: 0.9893 Val Loss: 0.0330, Val Acc: 0.9708, Val F1: 0.9797\n",
            "Epoch: 56/75, Train Loss: 0.0243, Train Acc: 0.9841, Train F1: 0.9889 Val Loss: 0.0322, Val Acc: 0.9729, Val F1: 0.9812\n",
            "Epoch: 57/75, Train Loss: 0.0238, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0364, Val Acc: 0.9701, Val F1: 0.9796\n",
            "Epoch: 58/75, Train Loss: 0.0245, Train Acc: 0.9836, Train F1: 0.9887 Val Loss: 0.0314, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 59/75, Train Loss: 0.0236, Train Acc: 0.9856, Train F1: 0.9900 Val Loss: 0.0316, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 60/75, Train Loss: 0.0244, Train Acc: 0.9845, Train F1: 0.9893 Val Loss: 0.0336, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 61/75, Train Loss: 0.0236, Train Acc: 0.9850, Train F1: 0.9897 Val Loss: 0.0320, Val Acc: 0.9732, Val F1: 0.9814\n",
            "Epoch: 62/75, Train Loss: 0.0233, Train Acc: 0.9866, Train F1: 0.9907 Val Loss: 0.0315, Val Acc: 0.9749, Val F1: 0.9826\n",
            "Epoch: 63/75, Train Loss: 0.0231, Train Acc: 0.9869, Train F1: 0.9909 Val Loss: 0.0314, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 64/75, Train Loss: 0.0233, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0323, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 65/75, Train Loss: 0.0230, Train Acc: 0.9868, Train F1: 0.9909 Val Loss: 0.0317, Val Acc: 0.9732, Val F1: 0.9814\n",
            "Epoch: 66/75, Train Loss: 0.0230, Train Acc: 0.9855, Train F1: 0.9899 Val Loss: 0.0312, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 67/75, Train Loss: 0.0226, Train Acc: 0.9872, Train F1: 0.9911 Val Loss: 0.0316, Val Acc: 0.9732, Val F1: 0.9814\n",
            "Epoch: 68/75, Train Loss: 0.0228, Train Acc: 0.9866, Train F1: 0.9907 Val Loss: 0.0311, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 69/75, Train Loss: 0.0226, Train Acc: 0.9866, Train F1: 0.9907 Val Loss: 0.0310, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 70/75, Train Loss: 0.0225, Train Acc: 0.9872, Train F1: 0.9911 Val Loss: 0.0321, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 71/75, Train Loss: 0.0226, Train Acc: 0.9872, Train F1: 0.9911 Val Loss: 0.0311, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 72/75, Train Loss: 0.0224, Train Acc: 0.9872, Train F1: 0.9910 Val Loss: 0.0310, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 73/75, Train Loss: 0.0224, Train Acc: 0.9872, Train F1: 0.9910 Val Loss: 0.0311, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 74/75, Train Loss: 0.0224, Train Acc: 0.9871, Train F1: 0.9910 Val Loss: 0.0310, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 75/75, Train Loss: 0.0223, Train Acc: 0.9871, Train F1: 0.9909 Val Loss: 0.0310, Val Acc: 0.9746, Val F1: 0.9825\n",
            "\n",
            " 🔎 search 22 : deep_rescnn --- lr : 0.0009144615755940233, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv4.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1296, Train Acc: 0.8498, Train F1: 0.8994 Val Loss: 0.0989, Val Acc: 0.8955, Val F1: 0.9300\n",
            "Epoch: 2/75, Train Loss: 0.0828, Train Acc: 0.9176, Train F1: 0.9427 Val Loss: 0.0750, Val Acc: 0.9368, Val F1: 0.9568\n",
            "Epoch: 3/75, Train Loss: 0.0692, Train Acc: 0.9326, Train F1: 0.9535 Val Loss: 0.0717, Val Acc: 0.9316, Val F1: 0.9516\n",
            "Epoch: 4/75, Train Loss: 0.0596, Train Acc: 0.9438, Train F1: 0.9610 Val Loss: 0.0698, Val Acc: 0.9337, Val F1: 0.9558\n",
            "Epoch: 5/75, Train Loss: 0.0549, Train Acc: 0.9490, Train F1: 0.9648 Val Loss: 0.0556, Val Acc: 0.9546, Val F1: 0.9689\n",
            "Epoch: 6/75, Train Loss: 0.0484, Train Acc: 0.9584, Train F1: 0.9711 Val Loss: 0.0532, Val Acc: 0.9553, Val F1: 0.9689\n",
            "Epoch: 7/75, Train Loss: 0.0464, Train Acc: 0.9561, Train F1: 0.9693 Val Loss: 0.0494, Val Acc: 0.9612, Val F1: 0.9734\n",
            "Epoch: 8/75, Train Loss: 0.0430, Train Acc: 0.9615, Train F1: 0.9732 Val Loss: 0.0469, Val Acc: 0.9625, Val F1: 0.9742\n",
            "Epoch: 9/75, Train Loss: 0.0413, Train Acc: 0.9629, Train F1: 0.9743 Val Loss: 0.0492, Val Acc: 0.9632, Val F1: 0.9749\n",
            "Epoch: 10/75, Train Loss: 0.0383, Train Acc: 0.9684, Train F1: 0.9781 Val Loss: 0.0650, Val Acc: 0.9375, Val F1: 0.9583\n",
            "Epoch: 11/75, Train Loss: 0.0475, Train Acc: 0.9584, Train F1: 0.9711 Val Loss: 0.0505, Val Acc: 0.9540, Val F1: 0.9676\n",
            "Epoch: 12/75, Train Loss: 0.0356, Train Acc: 0.9741, Train F1: 0.9821 Val Loss: 0.0460, Val Acc: 0.9605, Val F1: 0.9730\n",
            "Epoch: 13/75, Train Loss: 0.0364, Train Acc: 0.9704, Train F1: 0.9795 Val Loss: 0.0446, Val Acc: 0.9677, Val F1: 0.9778\n",
            "Epoch: 14/75, Train Loss: 0.0371, Train Acc: 0.9670, Train F1: 0.9771 Val Loss: 0.0491, Val Acc: 0.9564, Val F1: 0.9704\n",
            "Epoch: 15/75, Train Loss: 0.0333, Train Acc: 0.9751, Train F1: 0.9828 Val Loss: 0.0416, Val Acc: 0.9646, Val F1: 0.9755\n",
            "Epoch: 16/75, Train Loss: 0.0347, Train Acc: 0.9725, Train F1: 0.9810 Val Loss: 0.0509, Val Acc: 0.9570, Val F1: 0.9709\n",
            "Epoch: 17/75, Train Loss: 0.0316, Train Acc: 0.9777, Train F1: 0.9845 Val Loss: 0.0489, Val Acc: 0.9553, Val F1: 0.9685\n",
            "Epoch: 18/75, Train Loss: 0.0317, Train Acc: 0.9775, Train F1: 0.9844 Val Loss: 0.0402, Val Acc: 0.9701, Val F1: 0.9792\n",
            "Epoch: 19/75, Train Loss: 0.0319, Train Acc: 0.9767, Train F1: 0.9838 Val Loss: 0.0456, Val Acc: 0.9584, Val F1: 0.9707\n",
            "Epoch: 20/75, Train Loss: 0.0340, Train Acc: 0.9721, Train F1: 0.9805 Val Loss: 0.0425, Val Acc: 0.9636, Val F1: 0.9745\n",
            "Epoch: 21/75, Train Loss: 0.0293, Train Acc: 0.9800, Train F1: 0.9861 Val Loss: 0.0388, Val Acc: 0.9698, Val F1: 0.9792\n",
            "Epoch: 22/75, Train Loss: 0.0307, Train Acc: 0.9762, Train F1: 0.9836 Val Loss: 0.0388, Val Acc: 0.9667, Val F1: 0.9769\n",
            "Epoch: 23/75, Train Loss: 0.0338, Train Acc: 0.9721, Train F1: 0.9805 Val Loss: 0.0459, Val Acc: 0.9567, Val F1: 0.9694\n",
            "Epoch: 24/75, Train Loss: 0.0297, Train Acc: 0.9784, Train F1: 0.9849 Val Loss: 0.0385, Val Acc: 0.9711, Val F1: 0.9801\n",
            "Epoch: 25/75, Train Loss: 0.0290, Train Acc: 0.9796, Train F1: 0.9857 Val Loss: 0.0395, Val Acc: 0.9687, Val F1: 0.9784\n",
            "Epoch: 26/75, Train Loss: 0.0287, Train Acc: 0.9800, Train F1: 0.9859 Val Loss: 0.0399, Val Acc: 0.9701, Val F1: 0.9795\n",
            "Epoch: 27/75, Train Loss: 0.0293, Train Acc: 0.9780, Train F1: 0.9847 Val Loss: 0.0435, Val Acc: 0.9646, Val F1: 0.9759\n",
            "Epoch: 28/75, Train Loss: 0.0299, Train Acc: 0.9772, Train F1: 0.9842 Val Loss: 0.0394, Val Acc: 0.9663, Val F1: 0.9764\n",
            "Epoch: 29/75, Train Loss: 0.0271, Train Acc: 0.9820, Train F1: 0.9875 Val Loss: 0.0359, Val Acc: 0.9715, Val F1: 0.9804\n",
            "Epoch: 30/75, Train Loss: 0.0273, Train Acc: 0.9809, Train F1: 0.9867 Val Loss: 0.0362, Val Acc: 0.9708, Val F1: 0.9797\n",
            "Epoch: 31/75, Train Loss: 0.0285, Train Acc: 0.9793, Train F1: 0.9855 Val Loss: 0.0739, Val Acc: 0.9213, Val F1: 0.9482\n",
            "Epoch: 32/75, Train Loss: 0.0290, Train Acc: 0.9787, Train F1: 0.9852 Val Loss: 0.0369, Val Acc: 0.9701, Val F1: 0.9793\n",
            "Epoch: 33/75, Train Loss: 0.0262, Train Acc: 0.9824, Train F1: 0.9877 Val Loss: 0.0408, Val Acc: 0.9674, Val F1: 0.9778\n",
            "Epoch: 34/75, Train Loss: 0.0275, Train Acc: 0.9820, Train F1: 0.9876 Val Loss: 0.0352, Val Acc: 0.9701, Val F1: 0.9792\n",
            "Epoch: 35/75, Train Loss: 0.0259, Train Acc: 0.9830, Train F1: 0.9883 Val Loss: 0.0499, Val Acc: 0.9608, Val F1: 0.9735\n",
            "Epoch: 36/75, Train Loss: 0.0273, Train Acc: 0.9797, Train F1: 0.9860 Val Loss: 0.0352, Val Acc: 0.9715, Val F1: 0.9804\n",
            "Epoch: 37/75, Train Loss: 0.0251, Train Acc: 0.9821, Train F1: 0.9876 Val Loss: 0.0341, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 38/75, Train Loss: 0.0242, Train Acc: 0.9859, Train F1: 0.9902 Val Loss: 0.0347, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 39/75, Train Loss: 0.0238, Train Acc: 0.9836, Train F1: 0.9886 Val Loss: 0.0356, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 40/75, Train Loss: 0.0255, Train Acc: 0.9830, Train F1: 0.9883 Val Loss: 0.0380, Val Acc: 0.9694, Val F1: 0.9786\n",
            "Epoch: 41/75, Train Loss: 0.0243, Train Acc: 0.9849, Train F1: 0.9895 Val Loss: 0.0346, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 42/75, Train Loss: 0.0241, Train Acc: 0.9860, Train F1: 0.9903 Val Loss: 0.0342, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 43/75, Train Loss: 0.0236, Train Acc: 0.9863, Train F1: 0.9905 Val Loss: 0.0346, Val Acc: 0.9718, Val F1: 0.9806\n",
            "Epoch: 44/75, Train Loss: 0.0246, Train Acc: 0.9840, Train F1: 0.9888 Val Loss: 0.0407, Val Acc: 0.9680, Val F1: 0.9782\n",
            "Epoch: 45/75, Train Loss: 0.0232, Train Acc: 0.9837, Train F1: 0.9886 Val Loss: 0.0366, Val Acc: 0.9722, Val F1: 0.9809\n",
            "Epoch: 46/75, Train Loss: 0.0236, Train Acc: 0.9838, Train F1: 0.9888 Val Loss: 0.0343, Val Acc: 0.9725, Val F1: 0.9809\n",
            "Epoch: 47/75, Train Loss: 0.0228, Train Acc: 0.9861, Train F1: 0.9903 Val Loss: 0.0349, Val Acc: 0.9698, Val F1: 0.9789\n",
            "Epoch: 48/75, Train Loss: 0.0244, Train Acc: 0.9844, Train F1: 0.9891 Val Loss: 0.0348, Val Acc: 0.9708, Val F1: 0.9797\n",
            "Epoch: 49/75, Train Loss: 0.0237, Train Acc: 0.9847, Train F1: 0.9894 Val Loss: 0.0323, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 50/75, Train Loss: 0.0225, Train Acc: 0.9858, Train F1: 0.9901 Val Loss: 0.0351, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 51/75, Train Loss: 0.0241, Train Acc: 0.9841, Train F1: 0.9889 Val Loss: 0.0339, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 52/75, Train Loss: 0.0229, Train Acc: 0.9850, Train F1: 0.9895 Val Loss: 0.0345, Val Acc: 0.9715, Val F1: 0.9804\n",
            "Epoch: 53/75, Train Loss: 0.0218, Train Acc: 0.9871, Train F1: 0.9910 Val Loss: 0.0328, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 54/75, Train Loss: 0.0221, Train Acc: 0.9866, Train F1: 0.9907 Val Loss: 0.0356, Val Acc: 0.9711, Val F1: 0.9798\n",
            "Epoch: 55/75, Train Loss: 0.0221, Train Acc: 0.9865, Train F1: 0.9906 Val Loss: 0.0329, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 56/75, Train Loss: 0.0220, Train Acc: 0.9868, Train F1: 0.9908 Val Loss: 0.0320, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 57/75, Train Loss: 0.0216, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0325, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 58/75, Train Loss: 0.0211, Train Acc: 0.9882, Train F1: 0.9918 Val Loss: 0.0322, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 59/75, Train Loss: 0.0213, Train Acc: 0.9866, Train F1: 0.9906 Val Loss: 0.0323, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 60/75, Train Loss: 0.0212, Train Acc: 0.9876, Train F1: 0.9914 Val Loss: 0.0321, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 61/75, Train Loss: 0.0212, Train Acc: 0.9877, Train F1: 0.9915 Val Loss: 0.0319, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 62/75, Train Loss: 0.0214, Train Acc: 0.9866, Train F1: 0.9906 Val Loss: 0.0321, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 63/75, Train Loss: 0.0207, Train Acc: 0.9874, Train F1: 0.9913 Val Loss: 0.0318, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 64/75, Train Loss: 0.0209, Train Acc: 0.9884, Train F1: 0.9920 Val Loss: 0.0316, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 65/75, Train Loss: 0.0207, Train Acc: 0.9880, Train F1: 0.9916 Val Loss: 0.0323, Val Acc: 0.9742, Val F1: 0.9821\n",
            "Epoch: 66/75, Train Loss: 0.0204, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0326, Val Acc: 0.9732, Val F1: 0.9816\n",
            "Epoch: 67/75, Train Loss: 0.0203, Train Acc: 0.9875, Train F1: 0.9913 Val Loss: 0.0320, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 68/75, Train Loss: 0.0204, Train Acc: 0.9882, Train F1: 0.9918 Val Loss: 0.0317, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 69/75, Train Loss: 0.0203, Train Acc: 0.9879, Train F1: 0.9916 Val Loss: 0.0317, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 70/75, Train Loss: 0.0203, Train Acc: 0.9877, Train F1: 0.9915 Val Loss: 0.0317, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 71/75, Train Loss: 0.0202, Train Acc: 0.9882, Train F1: 0.9917 Val Loss: 0.0318, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 72/75, Train Loss: 0.0202, Train Acc: 0.9880, Train F1: 0.9916 Val Loss: 0.0317, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 73/75, Train Loss: 0.0202, Train Acc: 0.9884, Train F1: 0.9919 Val Loss: 0.0317, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 74/75, Train Loss: 0.0202, Train Acc: 0.9881, Train F1: 0.9917 Val Loss: 0.0317, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 75/75, Train Loss: 0.0201, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0317, Val Acc: 0.9739, Val F1: 0.9820\n",
            "\n",
            " 🔎 search 23 : deep_rescnn --- lr : 0.0009144615755940233, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv5.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1676, Train Acc: 0.7908, Train F1: 0.8563 Val Loss: 0.1053, Val Acc: 0.8828, Val F1: 0.9196\n",
            "Epoch: 2/75, Train Loss: 0.0907, Train Acc: 0.9009, Train F1: 0.9314 Val Loss: 0.0935, Val Acc: 0.8893, Val F1: 0.9281\n",
            "Epoch: 3/75, Train Loss: 0.0755, Train Acc: 0.9237, Train F1: 0.9472 Val Loss: 0.0698, Val Acc: 0.9423, Val F1: 0.9598\n",
            "Epoch: 4/75, Train Loss: 0.0612, Train Acc: 0.9424, Train F1: 0.9599 Val Loss: 0.0637, Val Acc: 0.9426, Val F1: 0.9597\n",
            "Epoch: 5/75, Train Loss: 0.0561, Train Acc: 0.9468, Train F1: 0.9631 Val Loss: 0.0564, Val Acc: 0.9557, Val F1: 0.9696\n",
            "Epoch: 6/75, Train Loss: 0.0508, Train Acc: 0.9519, Train F1: 0.9666 Val Loss: 0.0559, Val Acc: 0.9509, Val F1: 0.9655\n",
            "Epoch: 7/75, Train Loss: 0.0474, Train Acc: 0.9569, Train F1: 0.9700 Val Loss: 0.0510, Val Acc: 0.9522, Val F1: 0.9666\n",
            "Epoch: 8/75, Train Loss: 0.0474, Train Acc: 0.9561, Train F1: 0.9694 Val Loss: 0.0474, Val Acc: 0.9622, Val F1: 0.9738\n",
            "Epoch: 9/75, Train Loss: 0.0425, Train Acc: 0.9631, Train F1: 0.9744 Val Loss: 0.0468, Val Acc: 0.9605, Val F1: 0.9726\n",
            "Epoch: 10/75, Train Loss: 0.0420, Train Acc: 0.9633, Train F1: 0.9745 Val Loss: 0.0485, Val Acc: 0.9533, Val F1: 0.9671\n",
            "Epoch: 11/75, Train Loss: 0.0393, Train Acc: 0.9674, Train F1: 0.9774 Val Loss: 0.0469, Val Acc: 0.9632, Val F1: 0.9743\n",
            "Epoch: 12/75, Train Loss: 0.0367, Train Acc: 0.9709, Train F1: 0.9798 Val Loss: 0.0472, Val Acc: 0.9564, Val F1: 0.9694\n",
            "Epoch: 13/75, Train Loss: 0.0362, Train Acc: 0.9710, Train F1: 0.9798 Val Loss: 0.0441, Val Acc: 0.9636, Val F1: 0.9748\n",
            "Epoch: 14/75, Train Loss: 0.0340, Train Acc: 0.9750, Train F1: 0.9827 Val Loss: 0.0441, Val Acc: 0.9612, Val F1: 0.9735\n",
            "Epoch: 15/75, Train Loss: 0.0341, Train Acc: 0.9743, Train F1: 0.9822 Val Loss: 0.0403, Val Acc: 0.9691, Val F1: 0.9789\n",
            "Epoch: 16/75, Train Loss: 0.0333, Train Acc: 0.9727, Train F1: 0.9812 Val Loss: 0.0386, Val Acc: 0.9684, Val F1: 0.9782\n",
            "Epoch: 17/75, Train Loss: 0.0341, Train Acc: 0.9737, Train F1: 0.9818 Val Loss: 0.0386, Val Acc: 0.9711, Val F1: 0.9802\n",
            "Epoch: 18/75, Train Loss: 0.0342, Train Acc: 0.9717, Train F1: 0.9803 Val Loss: 0.0388, Val Acc: 0.9698, Val F1: 0.9792\n",
            "Epoch: 19/75, Train Loss: 0.0333, Train Acc: 0.9741, Train F1: 0.9821 Val Loss: 0.0392, Val Acc: 0.9687, Val F1: 0.9782\n",
            "Epoch: 20/75, Train Loss: 0.0322, Train Acc: 0.9785, Train F1: 0.9850 Val Loss: 0.0367, Val Acc: 0.9711, Val F1: 0.9802\n",
            "Epoch: 21/75, Train Loss: 0.0298, Train Acc: 0.9786, Train F1: 0.9850 Val Loss: 0.0587, Val Acc: 0.9399, Val F1: 0.9599\n",
            "Epoch: 22/75, Train Loss: 0.0307, Train Acc: 0.9755, Train F1: 0.9831 Val Loss: 0.0451, Val Acc: 0.9588, Val F1: 0.9721\n",
            "Epoch: 23/75, Train Loss: 0.0305, Train Acc: 0.9770, Train F1: 0.9840 Val Loss: 0.0354, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 24/75, Train Loss: 0.0294, Train Acc: 0.9784, Train F1: 0.9849 Val Loss: 0.0403, Val Acc: 0.9649, Val F1: 0.9762\n",
            "Epoch: 25/75, Train Loss: 0.0279, Train Acc: 0.9795, Train F1: 0.9857 Val Loss: 0.0456, Val Acc: 0.9612, Val F1: 0.9737\n",
            "Epoch: 26/75, Train Loss: 0.0281, Train Acc: 0.9800, Train F1: 0.9861 Val Loss: 0.0379, Val Acc: 0.9667, Val F1: 0.9767\n",
            "Epoch: 27/75, Train Loss: 0.0297, Train Acc: 0.9778, Train F1: 0.9845 Val Loss: 0.0368, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 28/75, Train Loss: 0.0274, Train Acc: 0.9805, Train F1: 0.9864 Val Loss: 0.0388, Val Acc: 0.9677, Val F1: 0.9773\n",
            "Epoch: 29/75, Train Loss: 0.0283, Train Acc: 0.9790, Train F1: 0.9854 Val Loss: 0.0416, Val Acc: 0.9608, Val F1: 0.9723\n",
            "Epoch: 30/75, Train Loss: 0.0328, Train Acc: 0.9740, Train F1: 0.9819 Val Loss: 0.0347, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 31/75, Train Loss: 0.0268, Train Acc: 0.9810, Train F1: 0.9867 Val Loss: 0.0334, Val Acc: 0.9749, Val F1: 0.9828\n",
            "Epoch: 32/75, Train Loss: 0.0253, Train Acc: 0.9843, Train F1: 0.9890 Val Loss: 0.0338, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 33/75, Train Loss: 0.0261, Train Acc: 0.9816, Train F1: 0.9872 Val Loss: 0.0360, Val Acc: 0.9715, Val F1: 0.9805\n",
            "Epoch: 34/75, Train Loss: 0.0271, Train Acc: 0.9810, Train F1: 0.9868 Val Loss: 0.0388, Val Acc: 0.9649, Val F1: 0.9753\n",
            "Epoch: 35/75, Train Loss: 0.0279, Train Acc: 0.9786, Train F1: 0.9852 Val Loss: 0.0384, Val Acc: 0.9625, Val F1: 0.9736\n",
            "Epoch: 36/75, Train Loss: 0.0267, Train Acc: 0.9810, Train F1: 0.9868 Val Loss: 0.0331, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 37/75, Train Loss: 0.0250, Train Acc: 0.9858, Train F1: 0.9901 Val Loss: 0.0330, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 38/75, Train Loss: 0.0238, Train Acc: 0.9865, Train F1: 0.9907 Val Loss: 0.0344, Val Acc: 0.9749, Val F1: 0.9828\n",
            "Epoch: 39/75, Train Loss: 0.0261, Train Acc: 0.9814, Train F1: 0.9871 Val Loss: 0.0397, Val Acc: 0.9684, Val F1: 0.9785\n",
            "Epoch: 40/75, Train Loss: 0.0244, Train Acc: 0.9858, Train F1: 0.9901 Val Loss: 0.0347, Val Acc: 0.9742, Val F1: 0.9824\n",
            "Epoch: 41/75, Train Loss: 0.0254, Train Acc: 0.9841, Train F1: 0.9889 Val Loss: 0.0315, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 42/75, Train Loss: 0.0237, Train Acc: 0.9861, Train F1: 0.9903 Val Loss: 0.0327, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 43/75, Train Loss: 0.0233, Train Acc: 0.9865, Train F1: 0.9906 Val Loss: 0.0317, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 44/75, Train Loss: 0.0242, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0318, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 45/75, Train Loss: 0.0233, Train Acc: 0.9872, Train F1: 0.9910 Val Loss: 0.0312, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 46/75, Train Loss: 0.0228, Train Acc: 0.9872, Train F1: 0.9912 Val Loss: 0.0307, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 47/75, Train Loss: 0.0228, Train Acc: 0.9863, Train F1: 0.9904 Val Loss: 0.0323, Val Acc: 0.9763, Val F1: 0.9835\n",
            "Epoch: 48/75, Train Loss: 0.0228, Train Acc: 0.9874, Train F1: 0.9912 Val Loss: 0.0324, Val Acc: 0.9746, Val F1: 0.9823\n",
            "Epoch: 49/75, Train Loss: 0.0226, Train Acc: 0.9866, Train F1: 0.9906 Val Loss: 0.0348, Val Acc: 0.9739, Val F1: 0.9822\n",
            "Epoch: 50/75, Train Loss: 0.0222, Train Acc: 0.9892, Train F1: 0.9925 Val Loss: 0.0317, Val Acc: 0.9749, Val F1: 0.9825\n",
            "Epoch: 51/75, Train Loss: 0.0221, Train Acc: 0.9876, Train F1: 0.9914 Val Loss: 0.0309, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 52/75, Train Loss: 0.0225, Train Acc: 0.9875, Train F1: 0.9913 Val Loss: 0.0302, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 53/75, Train Loss: 0.0224, Train Acc: 0.9882, Train F1: 0.9917 Val Loss: 0.0303, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 54/75, Train Loss: 0.0218, Train Acc: 0.9879, Train F1: 0.9915 Val Loss: 0.0307, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 55/75, Train Loss: 0.0216, Train Acc: 0.9877, Train F1: 0.9915 Val Loss: 0.0300, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 56/75, Train Loss: 0.0210, Train Acc: 0.9898, Train F1: 0.9930 Val Loss: 0.0306, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 57/75, Train Loss: 0.0211, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0308, Val Acc: 0.9780, Val F1: 0.9849\n",
            "Epoch: 58/75, Train Loss: 0.0212, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0315, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 59/75, Train Loss: 0.0211, Train Acc: 0.9890, Train F1: 0.9924 Val Loss: 0.0298, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 60/75, Train Loss: 0.0214, Train Acc: 0.9887, Train F1: 0.9921 Val Loss: 0.0302, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 61/75, Train Loss: 0.0208, Train Acc: 0.9896, Train F1: 0.9927 Val Loss: 0.0303, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 62/75, Train Loss: 0.0205, Train Acc: 0.9898, Train F1: 0.9930 Val Loss: 0.0298, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 63/75, Train Loss: 0.0206, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0298, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 64/75, Train Loss: 0.0204, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0299, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 65/75, Train Loss: 0.0204, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0297, Val Acc: 0.9811, Val F1: 0.9870\n",
            "Epoch: 66/75, Train Loss: 0.0203, Train Acc: 0.9900, Train F1: 0.9931 Val Loss: 0.0301, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 67/75, Train Loss: 0.0203, Train Acc: 0.9905, Train F1: 0.9934 Val Loss: 0.0297, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 68/75, Train Loss: 0.0202, Train Acc: 0.9900, Train F1: 0.9930 Val Loss: 0.0297, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 69/75, Train Loss: 0.0202, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0299, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 70/75, Train Loss: 0.0201, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0298, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 71/75, Train Loss: 0.0200, Train Acc: 0.9896, Train F1: 0.9927 Val Loss: 0.0297, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 72/75, Train Loss: 0.0200, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0296, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 73/75, Train Loss: 0.0200, Train Acc: 0.9900, Train F1: 0.9931 Val Loss: 0.0296, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 74/75, Train Loss: 0.0200, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0297, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 75/75, Train Loss: 0.0200, Train Acc: 0.9901, Train F1: 0.9932 Val Loss: 0.0297, Val Acc: 0.9804, Val F1: 0.9865\n",
            "\n",
            " 🔎 search 24 : deep_rescnn --- lr : 0.0012742905281420724, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv2.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1157, Train Acc: 0.8761, Train F1: 0.9157 Val Loss: 0.1024, Val Acc: 0.9065, Val F1: 0.9357\n",
            "Epoch: 2/75, Train Loss: 0.0945, Train Acc: 0.9093, Train F1: 0.9372 Val Loss: 0.0956, Val Acc: 0.9086, Val F1: 0.9354\n",
            "Epoch: 3/75, Train Loss: 0.0905, Train Acc: 0.9100, Train F1: 0.9373 Val Loss: 0.0956, Val Acc: 0.9172, Val F1: 0.9426\n",
            "Epoch: 4/75, Train Loss: 0.0844, Train Acc: 0.9196, Train F1: 0.9439 Val Loss: 0.1047, Val Acc: 0.8942, Val F1: 0.9294\n",
            "Epoch: 5/75, Train Loss: 0.0837, Train Acc: 0.9187, Train F1: 0.9436 Val Loss: 0.0790, Val Acc: 0.9306, Val F1: 0.9525\n",
            "Epoch: 6/75, Train Loss: 0.0798, Train Acc: 0.9257, Train F1: 0.9484 Val Loss: 0.0886, Val Acc: 0.9065, Val F1: 0.9374\n",
            "Epoch: 7/75, Train Loss: 0.0728, Train Acc: 0.9317, Train F1: 0.9525 Val Loss: 0.0828, Val Acc: 0.9162, Val F1: 0.9446\n",
            "Epoch: 8/75, Train Loss: 0.0721, Train Acc: 0.9279, Train F1: 0.9497 Val Loss: 0.0780, Val Acc: 0.9186, Val F1: 0.9423\n",
            "Epoch: 9/75, Train Loss: 0.0739, Train Acc: 0.9237, Train F1: 0.9467 Val Loss: 0.0742, Val Acc: 0.9337, Val F1: 0.9535\n",
            "Epoch: 10/75, Train Loss: 0.0715, Train Acc: 0.9304, Train F1: 0.9514 Val Loss: 0.0891, Val Acc: 0.9172, Val F1: 0.9408\n",
            "Epoch: 11/75, Train Loss: 0.0656, Train Acc: 0.9360, Train F1: 0.9553 Val Loss: 0.0799, Val Acc: 0.9148, Val F1: 0.9389\n",
            "Epoch: 12/75, Train Loss: 0.0657, Train Acc: 0.9365, Train F1: 0.9559 Val Loss: 0.0674, Val Acc: 0.9313, Val F1: 0.9519\n",
            "Epoch: 13/75, Train Loss: 0.0618, Train Acc: 0.9385, Train F1: 0.9571 Val Loss: 0.0778, Val Acc: 0.9199, Val F1: 0.9452\n",
            "Epoch: 14/75, Train Loss: 0.0643, Train Acc: 0.9360, Train F1: 0.9554 Val Loss: 0.0631, Val Acc: 0.9488, Val F1: 0.9652\n",
            "Epoch: 15/75, Train Loss: 0.0573, Train Acc: 0.9450, Train F1: 0.9618 Val Loss: 0.0677, Val Acc: 0.9254, Val F1: 0.9471\n",
            "Epoch: 16/75, Train Loss: 0.0583, Train Acc: 0.9433, Train F1: 0.9607 Val Loss: 0.0593, Val Acc: 0.9460, Val F1: 0.9631\n",
            "Epoch: 17/75, Train Loss: 0.0552, Train Acc: 0.9468, Train F1: 0.9631 Val Loss: 0.0561, Val Acc: 0.9529, Val F1: 0.9677\n",
            "Epoch: 18/75, Train Loss: 0.0568, Train Acc: 0.9435, Train F1: 0.9606 Val Loss: 0.0554, Val Acc: 0.9543, Val F1: 0.9685\n",
            "Epoch: 19/75, Train Loss: 0.0557, Train Acc: 0.9471, Train F1: 0.9633 Val Loss: 0.0660, Val Acc: 0.9326, Val F1: 0.9522\n",
            "Epoch: 20/75, Train Loss: 0.0538, Train Acc: 0.9470, Train F1: 0.9632 Val Loss: 0.0654, Val Acc: 0.9433, Val F1: 0.9607\n",
            "Epoch: 21/75, Train Loss: 0.0519, Train Acc: 0.9526, Train F1: 0.9669 Val Loss: 0.0571, Val Acc: 0.9464, Val F1: 0.9631\n",
            "Epoch: 22/75, Train Loss: 0.0534, Train Acc: 0.9506, Train F1: 0.9657 Val Loss: 0.0751, Val Acc: 0.9192, Val F1: 0.9417\n",
            "Epoch: 23/75, Train Loss: 0.0523, Train Acc: 0.9506, Train F1: 0.9653 Val Loss: 0.0586, Val Acc: 0.9430, Val F1: 0.9599\n",
            "Epoch: 24/75, Train Loss: 0.0474, Train Acc: 0.9575, Train F1: 0.9703 Val Loss: 0.0586, Val Acc: 0.9454, Val F1: 0.9618\n",
            "Epoch: 25/75, Train Loss: 0.0533, Train Acc: 0.9496, Train F1: 0.9647 Val Loss: 0.0578, Val Acc: 0.9488, Val F1: 0.9651\n",
            "Epoch: 26/75, Train Loss: 0.0459, Train Acc: 0.9584, Train F1: 0.9710 Val Loss: 0.0526, Val Acc: 0.9526, Val F1: 0.9676\n",
            "Epoch: 27/75, Train Loss: 0.0494, Train Acc: 0.9543, Train F1: 0.9682 Val Loss: 0.0549, Val Acc: 0.9588, Val F1: 0.9719\n",
            "Epoch: 28/75, Train Loss: 0.0499, Train Acc: 0.9550, Train F1: 0.9687 Val Loss: 0.0556, Val Acc: 0.9557, Val F1: 0.9699\n",
            "Epoch: 29/75, Train Loss: 0.0499, Train Acc: 0.9553, Train F1: 0.9687 Val Loss: 0.0533, Val Acc: 0.9515, Val F1: 0.9663\n",
            "Epoch: 30/75, Train Loss: 0.0461, Train Acc: 0.9590, Train F1: 0.9712 Val Loss: 0.0531, Val Acc: 0.9588, Val F1: 0.9718\n",
            "Epoch: 31/75, Train Loss: 0.0446, Train Acc: 0.9613, Train F1: 0.9729 Val Loss: 0.0507, Val Acc: 0.9622, Val F1: 0.9740\n",
            "Epoch: 32/75, Train Loss: 0.0444, Train Acc: 0.9601, Train F1: 0.9723 Val Loss: 0.0513, Val Acc: 0.9495, Val F1: 0.9649\n",
            "Epoch: 33/75, Train Loss: 0.0475, Train Acc: 0.9577, Train F1: 0.9705 Val Loss: 0.0664, Val Acc: 0.9333, Val F1: 0.9556\n",
            "Epoch: 34/75, Train Loss: 0.0461, Train Acc: 0.9596, Train F1: 0.9718 Val Loss: 0.0774, Val Acc: 0.9210, Val F1: 0.9478\n",
            "Epoch: 35/75, Train Loss: 0.0470, Train Acc: 0.9586, Train F1: 0.9712 Val Loss: 0.0514, Val Acc: 0.9615, Val F1: 0.9732\n",
            "Epoch: 36/75, Train Loss: 0.0429, Train Acc: 0.9614, Train F1: 0.9730 Val Loss: 0.0485, Val Acc: 0.9660, Val F1: 0.9766\n",
            "Epoch: 37/75, Train Loss: 0.0427, Train Acc: 0.9638, Train F1: 0.9747 Val Loss: 0.0572, Val Acc: 0.9460, Val F1: 0.9619\n",
            "Epoch: 38/75, Train Loss: 0.0435, Train Acc: 0.9629, Train F1: 0.9740 Val Loss: 0.0593, Val Acc: 0.9436, Val F1: 0.9607\n",
            "Epoch: 39/75, Train Loss: 0.0421, Train Acc: 0.9639, Train F1: 0.9748 Val Loss: 0.0503, Val Acc: 0.9536, Val F1: 0.9677\n",
            "Epoch: 40/75, Train Loss: 0.0424, Train Acc: 0.9625, Train F1: 0.9740 Val Loss: 0.0472, Val Acc: 0.9629, Val F1: 0.9742\n",
            "Epoch: 41/75, Train Loss: 0.0399, Train Acc: 0.9660, Train F1: 0.9760 Val Loss: 0.0492, Val Acc: 0.9564, Val F1: 0.9695\n",
            "Epoch: 42/75, Train Loss: 0.0411, Train Acc: 0.9643, Train F1: 0.9751 Val Loss: 0.0510, Val Acc: 0.9533, Val F1: 0.9673\n",
            "Epoch: 43/75, Train Loss: 0.0390, Train Acc: 0.9683, Train F1: 0.9778 Val Loss: 0.0490, Val Acc: 0.9595, Val F1: 0.9718\n",
            "Epoch: 44/75, Train Loss: 0.0389, Train Acc: 0.9667, Train F1: 0.9768 Val Loss: 0.0463, Val Acc: 0.9632, Val F1: 0.9747\n",
            "Epoch: 45/75, Train Loss: 0.0391, Train Acc: 0.9676, Train F1: 0.9772 Val Loss: 0.0485, Val Acc: 0.9619, Val F1: 0.9736\n",
            "Epoch: 46/75, Train Loss: 0.0381, Train Acc: 0.9698, Train F1: 0.9790 Val Loss: 0.0500, Val Acc: 0.9567, Val F1: 0.9699\n",
            "Epoch: 47/75, Train Loss: 0.0377, Train Acc: 0.9696, Train F1: 0.9789 Val Loss: 0.0491, Val Acc: 0.9581, Val F1: 0.9709\n",
            "Epoch: 48/75, Train Loss: 0.0388, Train Acc: 0.9667, Train F1: 0.9767 Val Loss: 0.0491, Val Acc: 0.9629, Val F1: 0.9746\n",
            "Epoch: 49/75, Train Loss: 0.0374, Train Acc: 0.9684, Train F1: 0.9780 Val Loss: 0.0455, Val Acc: 0.9649, Val F1: 0.9758\n",
            "Epoch: 50/75, Train Loss: 0.0380, Train Acc: 0.9709, Train F1: 0.9796 Val Loss: 0.0513, Val Acc: 0.9553, Val F1: 0.9696\n",
            "Epoch: 51/75, Train Loss: 0.0378, Train Acc: 0.9686, Train F1: 0.9781 Val Loss: 0.0517, Val Acc: 0.9625, Val F1: 0.9746\n",
            "Epoch: 52/75, Train Loss: 0.0366, Train Acc: 0.9706, Train F1: 0.9793 Val Loss: 0.0463, Val Acc: 0.9605, Val F1: 0.9725\n",
            "Epoch: 53/75, Train Loss: 0.0368, Train Acc: 0.9708, Train F1: 0.9795 Val Loss: 0.0449, Val Acc: 0.9656, Val F1: 0.9762\n",
            "Epoch: 54/75, Train Loss: 0.0350, Train Acc: 0.9732, Train F1: 0.9812 Val Loss: 0.0440, Val Acc: 0.9670, Val F1: 0.9772\n",
            "Epoch: 55/75, Train Loss: 0.0352, Train Acc: 0.9739, Train F1: 0.9817 Val Loss: 0.0446, Val Acc: 0.9680, Val F1: 0.9780\n",
            "Epoch: 56/75, Train Loss: 0.0357, Train Acc: 0.9723, Train F1: 0.9807 Val Loss: 0.0442, Val Acc: 0.9632, Val F1: 0.9746\n",
            "Epoch: 57/75, Train Loss: 0.0347, Train Acc: 0.9734, Train F1: 0.9815 Val Loss: 0.0453, Val Acc: 0.9646, Val F1: 0.9754\n",
            "Epoch: 58/75, Train Loss: 0.0352, Train Acc: 0.9715, Train F1: 0.9800 Val Loss: 0.0445, Val Acc: 0.9643, Val F1: 0.9752\n",
            "Epoch: 59/75, Train Loss: 0.0341, Train Acc: 0.9735, Train F1: 0.9816 Val Loss: 0.0438, Val Acc: 0.9670, Val F1: 0.9772\n",
            "Epoch: 60/75, Train Loss: 0.0337, Train Acc: 0.9748, Train F1: 0.9825 Val Loss: 0.0432, Val Acc: 0.9670, Val F1: 0.9772\n",
            "Epoch: 61/75, Train Loss: 0.0336, Train Acc: 0.9750, Train F1: 0.9827 Val Loss: 0.0440, Val Acc: 0.9670, Val F1: 0.9774\n",
            "Epoch: 62/75, Train Loss: 0.0334, Train Acc: 0.9748, Train F1: 0.9825 Val Loss: 0.0430, Val Acc: 0.9670, Val F1: 0.9772\n",
            "Epoch: 63/75, Train Loss: 0.0334, Train Acc: 0.9758, Train F1: 0.9832 Val Loss: 0.0445, Val Acc: 0.9622, Val F1: 0.9737\n",
            "Epoch: 64/75, Train Loss: 0.0329, Train Acc: 0.9746, Train F1: 0.9820 Val Loss: 0.0438, Val Acc: 0.9660, Val F1: 0.9764\n",
            "Epoch: 65/75, Train Loss: 0.0325, Train Acc: 0.9750, Train F1: 0.9826 Val Loss: 0.0432, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 66/75, Train Loss: 0.0323, Train Acc: 0.9749, Train F1: 0.9826 Val Loss: 0.0436, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 67/75, Train Loss: 0.0322, Train Acc: 0.9758, Train F1: 0.9830 Val Loss: 0.0428, Val Acc: 0.9694, Val F1: 0.9789\n",
            "Epoch: 68/75, Train Loss: 0.0322, Train Acc: 0.9774, Train F1: 0.9842 Val Loss: 0.0433, Val Acc: 0.9677, Val F1: 0.9778\n",
            "Epoch: 69/75, Train Loss: 0.0320, Train Acc: 0.9763, Train F1: 0.9834 Val Loss: 0.0427, Val Acc: 0.9694, Val F1: 0.9789\n",
            "Epoch: 70/75, Train Loss: 0.0318, Train Acc: 0.9763, Train F1: 0.9834 Val Loss: 0.0432, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 71/75, Train Loss: 0.0318, Train Acc: 0.9765, Train F1: 0.9836 Val Loss: 0.0431, Val Acc: 0.9680, Val F1: 0.9780\n",
            "Epoch: 72/75, Train Loss: 0.0317, Train Acc: 0.9765, Train F1: 0.9837 Val Loss: 0.0427, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 73/75, Train Loss: 0.0316, Train Acc: 0.9765, Train F1: 0.9836 Val Loss: 0.0426, Val Acc: 0.9684, Val F1: 0.9782\n",
            "Epoch: 74/75, Train Loss: 0.0316, Train Acc: 0.9767, Train F1: 0.9838 Val Loss: 0.0426, Val Acc: 0.9701, Val F1: 0.9794\n",
            "Epoch: 75/75, Train Loss: 0.0315, Train Acc: 0.9770, Train F1: 0.9840 Val Loss: 0.0426, Val Acc: 0.9701, Val F1: 0.9794\n",
            "\n",
            " 🔎 search 25 : deep_rescnn --- lr : 0.0012742905281420724, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv3.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1098, Train Acc: 0.8848, Train F1: 0.9193 Val Loss: 0.0836, Val Acc: 0.9223, Val F1: 0.9458\n",
            "Epoch: 2/75, Train Loss: 0.0783, Train Acc: 0.9249, Train F1: 0.9476 Val Loss: 0.0761, Val Acc: 0.9375, Val F1: 0.9575\n",
            "Epoch: 3/75, Train Loss: 0.0663, Train Acc: 0.9363, Train F1: 0.9558 Val Loss: 0.0682, Val Acc: 0.9409, Val F1: 0.9593\n",
            "Epoch: 4/75, Train Loss: 0.0701, Train Acc: 0.9315, Train F1: 0.9523 Val Loss: 0.0732, Val Acc: 0.9196, Val F1: 0.9427\n",
            "Epoch: 5/75, Train Loss: 0.0617, Train Acc: 0.9394, Train F1: 0.9579 Val Loss: 0.0693, Val Acc: 0.9299, Val F1: 0.9516\n",
            "Epoch: 6/75, Train Loss: 0.0570, Train Acc: 0.9459, Train F1: 0.9623 Val Loss: 0.0693, Val Acc: 0.9333, Val F1: 0.9547\n",
            "Epoch: 7/75, Train Loss: 0.0544, Train Acc: 0.9435, Train F1: 0.9606 Val Loss: 0.0606, Val Acc: 0.9433, Val F1: 0.9598\n",
            "Epoch: 8/75, Train Loss: 0.0529, Train Acc: 0.9491, Train F1: 0.9645 Val Loss: 0.0594, Val Acc: 0.9402, Val F1: 0.9579\n",
            "Epoch: 9/75, Train Loss: 0.0509, Train Acc: 0.9512, Train F1: 0.9657 Val Loss: 0.0784, Val Acc: 0.9100, Val F1: 0.9344\n",
            "Epoch: 10/75, Train Loss: 0.0482, Train Acc: 0.9552, Train F1: 0.9687 Val Loss: 0.0561, Val Acc: 0.9533, Val F1: 0.9677\n",
            "Epoch: 11/75, Train Loss: 0.0482, Train Acc: 0.9550, Train F1: 0.9685 Val Loss: 0.0544, Val Acc: 0.9512, Val F1: 0.9656\n",
            "Epoch: 12/75, Train Loss: 0.0458, Train Acc: 0.9577, Train F1: 0.9706 Val Loss: 0.0523, Val Acc: 0.9567, Val F1: 0.9697\n",
            "Epoch: 13/75, Train Loss: 0.0495, Train Acc: 0.9504, Train F1: 0.9654 Val Loss: 0.0568, Val Acc: 0.9533, Val F1: 0.9685\n",
            "Epoch: 14/75, Train Loss: 0.0493, Train Acc: 0.9543, Train F1: 0.9676 Val Loss: 0.0498, Val Acc: 0.9522, Val F1: 0.9667\n",
            "Epoch: 15/75, Train Loss: 0.0453, Train Acc: 0.9580, Train F1: 0.9708 Val Loss: 0.0492, Val Acc: 0.9560, Val F1: 0.9697\n",
            "Epoch: 16/75, Train Loss: 0.0427, Train Acc: 0.9606, Train F1: 0.9726 Val Loss: 0.0459, Val Acc: 0.9636, Val F1: 0.9750\n",
            "Epoch: 17/75, Train Loss: 0.0401, Train Acc: 0.9648, Train F1: 0.9753 Val Loss: 0.0522, Val Acc: 0.9471, Val F1: 0.9626\n",
            "Epoch: 18/75, Train Loss: 0.0403, Train Acc: 0.9633, Train F1: 0.9746 Val Loss: 0.0446, Val Acc: 0.9680, Val F1: 0.9781\n",
            "Epoch: 19/75, Train Loss: 0.0413, Train Acc: 0.9624, Train F1: 0.9737 Val Loss: 0.0455, Val Acc: 0.9674, Val F1: 0.9777\n",
            "Epoch: 20/75, Train Loss: 0.0479, Train Acc: 0.9527, Train F1: 0.9667 Val Loss: 0.0585, Val Acc: 0.9364, Val F1: 0.9548\n",
            "Epoch: 21/75, Train Loss: 0.0387, Train Acc: 0.9661, Train F1: 0.9764 Val Loss: 0.0426, Val Acc: 0.9615, Val F1: 0.9736\n",
            "Epoch: 22/75, Train Loss: 0.0369, Train Acc: 0.9688, Train F1: 0.9785 Val Loss: 0.0475, Val Acc: 0.9639, Val F1: 0.9755\n",
            "Epoch: 23/75, Train Loss: 0.0387, Train Acc: 0.9643, Train F1: 0.9751 Val Loss: 0.0422, Val Acc: 0.9605, Val F1: 0.9725\n",
            "Epoch: 24/75, Train Loss: 0.0400, Train Acc: 0.9655, Train F1: 0.9759 Val Loss: 0.0422, Val Acc: 0.9646, Val F1: 0.9757\n",
            "Epoch: 25/75, Train Loss: 0.0363, Train Acc: 0.9685, Train F1: 0.9779 Val Loss: 0.0392, Val Acc: 0.9680, Val F1: 0.9780\n",
            "Epoch: 26/75, Train Loss: 0.0392, Train Acc: 0.9653, Train F1: 0.9756 Val Loss: 0.0517, Val Acc: 0.9467, Val F1: 0.9622\n",
            "Epoch: 27/75, Train Loss: 0.0376, Train Acc: 0.9664, Train F1: 0.9764 Val Loss: 0.0395, Val Acc: 0.9663, Val F1: 0.9767\n",
            "Epoch: 28/75, Train Loss: 0.0367, Train Acc: 0.9688, Train F1: 0.9781 Val Loss: 0.0400, Val Acc: 0.9653, Val F1: 0.9759\n",
            "Epoch: 29/75, Train Loss: 0.0342, Train Acc: 0.9733, Train F1: 0.9813 Val Loss: 0.0456, Val Acc: 0.9526, Val F1: 0.9666\n",
            "Epoch: 30/75, Train Loss: 0.0365, Train Acc: 0.9668, Train F1: 0.9769 Val Loss: 0.0422, Val Acc: 0.9629, Val F1: 0.9743\n",
            "Epoch: 31/75, Train Loss: 0.0330, Train Acc: 0.9727, Train F1: 0.9810 Val Loss: 0.0365, Val Acc: 0.9694, Val F1: 0.9788\n",
            "Epoch: 32/75, Train Loss: 0.0358, Train Acc: 0.9698, Train F1: 0.9788 Val Loss: 0.0395, Val Acc: 0.9667, Val F1: 0.9771\n",
            "Epoch: 33/75, Train Loss: 0.0323, Train Acc: 0.9749, Train F1: 0.9824 Val Loss: 0.0371, Val Acc: 0.9711, Val F1: 0.9801\n",
            "Epoch: 34/75, Train Loss: 0.0330, Train Acc: 0.9731, Train F1: 0.9810 Val Loss: 0.0446, Val Acc: 0.9574, Val F1: 0.9701\n",
            "Epoch: 35/75, Train Loss: 0.0330, Train Acc: 0.9730, Train F1: 0.9811 Val Loss: 0.0419, Val Acc: 0.9663, Val F1: 0.9768\n",
            "Epoch: 36/75, Train Loss: 0.0321, Train Acc: 0.9751, Train F1: 0.9825 Val Loss: 0.0393, Val Acc: 0.9619, Val F1: 0.9738\n",
            "Epoch: 37/75, Train Loss: 0.0332, Train Acc: 0.9740, Train F1: 0.9817 Val Loss: 0.0407, Val Acc: 0.9704, Val F1: 0.9798\n",
            "Epoch: 38/75, Train Loss: 0.0325, Train Acc: 0.9737, Train F1: 0.9816 Val Loss: 0.0384, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 39/75, Train Loss: 0.0300, Train Acc: 0.9785, Train F1: 0.9850 Val Loss: 0.0359, Val Acc: 0.9698, Val F1: 0.9792\n",
            "Epoch: 40/75, Train Loss: 0.0337, Train Acc: 0.9704, Train F1: 0.9792 Val Loss: 0.0427, Val Acc: 0.9670, Val F1: 0.9775\n",
            "Epoch: 41/75, Train Loss: 0.0311, Train Acc: 0.9761, Train F1: 0.9833 Val Loss: 0.0365, Val Acc: 0.9680, Val F1: 0.9778\n",
            "Epoch: 42/75, Train Loss: 0.0304, Train Acc: 0.9764, Train F1: 0.9835 Val Loss: 0.0365, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 43/75, Train Loss: 0.0312, Train Acc: 0.9753, Train F1: 0.9827 Val Loss: 0.0356, Val Acc: 0.9711, Val F1: 0.9801\n",
            "Epoch: 44/75, Train Loss: 0.0290, Train Acc: 0.9777, Train F1: 0.9845 Val Loss: 0.0347, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 45/75, Train Loss: 0.0278, Train Acc: 0.9800, Train F1: 0.9859 Val Loss: 0.0402, Val Acc: 0.9584, Val F1: 0.9708\n",
            "Epoch: 46/75, Train Loss: 0.0281, Train Acc: 0.9798, Train F1: 0.9859 Val Loss: 0.0346, Val Acc: 0.9722, Val F1: 0.9809\n",
            "Epoch: 47/75, Train Loss: 0.0301, Train Acc: 0.9756, Train F1: 0.9828 Val Loss: 0.0450, Val Acc: 0.9636, Val F1: 0.9753\n",
            "Epoch: 48/75, Train Loss: 0.0289, Train Acc: 0.9771, Train F1: 0.9838 Val Loss: 0.0348, Val Acc: 0.9715, Val F1: 0.9802\n",
            "Epoch: 49/75, Train Loss: 0.0270, Train Acc: 0.9825, Train F1: 0.9878 Val Loss: 0.0340, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 50/75, Train Loss: 0.0271, Train Acc: 0.9805, Train F1: 0.9863 Val Loss: 0.0387, Val Acc: 0.9718, Val F1: 0.9807\n",
            "Epoch: 51/75, Train Loss: 0.0269, Train Acc: 0.9809, Train F1: 0.9865 Val Loss: 0.0361, Val Acc: 0.9660, Val F1: 0.9763\n",
            "Epoch: 52/75, Train Loss: 0.0269, Train Acc: 0.9804, Train F1: 0.9863 Val Loss: 0.0354, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 53/75, Train Loss: 0.0264, Train Acc: 0.9808, Train F1: 0.9866 Val Loss: 0.0339, Val Acc: 0.9694, Val F1: 0.9788\n",
            "Epoch: 54/75, Train Loss: 0.0267, Train Acc: 0.9782, Train F1: 0.9848 Val Loss: 0.0367, Val Acc: 0.9701, Val F1: 0.9796\n",
            "Epoch: 55/75, Train Loss: 0.0259, Train Acc: 0.9812, Train F1: 0.9869 Val Loss: 0.0348, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 56/75, Train Loss: 0.0254, Train Acc: 0.9821, Train F1: 0.9875 Val Loss: 0.0339, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 57/75, Train Loss: 0.0252, Train Acc: 0.9830, Train F1: 0.9883 Val Loss: 0.0341, Val Acc: 0.9729, Val F1: 0.9814\n",
            "Epoch: 58/75, Train Loss: 0.0250, Train Acc: 0.9829, Train F1: 0.9879 Val Loss: 0.0327, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 59/75, Train Loss: 0.0253, Train Acc: 0.9817, Train F1: 0.9873 Val Loss: 0.0328, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 60/75, Train Loss: 0.0244, Train Acc: 0.9849, Train F1: 0.9893 Val Loss: 0.0332, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 61/75, Train Loss: 0.0242, Train Acc: 0.9836, Train F1: 0.9885 Val Loss: 0.0324, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 62/75, Train Loss: 0.0238, Train Acc: 0.9838, Train F1: 0.9887 Val Loss: 0.0336, Val Acc: 0.9718, Val F1: 0.9804\n",
            "Epoch: 63/75, Train Loss: 0.0244, Train Acc: 0.9845, Train F1: 0.9892 Val Loss: 0.0324, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 64/75, Train Loss: 0.0241, Train Acc: 0.9842, Train F1: 0.9890 Val Loss: 0.0322, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 65/75, Train Loss: 0.0238, Train Acc: 0.9850, Train F1: 0.9896 Val Loss: 0.0320, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 66/75, Train Loss: 0.0236, Train Acc: 0.9851, Train F1: 0.9897 Val Loss: 0.0319, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 67/75, Train Loss: 0.0234, Train Acc: 0.9855, Train F1: 0.9897 Val Loss: 0.0327, Val Acc: 0.9735, Val F1: 0.9818\n",
            "Epoch: 68/75, Train Loss: 0.0234, Train Acc: 0.9850, Train F1: 0.9894 Val Loss: 0.0318, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 69/75, Train Loss: 0.0231, Train Acc: 0.9855, Train F1: 0.9898 Val Loss: 0.0321, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 70/75, Train Loss: 0.0230, Train Acc: 0.9856, Train F1: 0.9899 Val Loss: 0.0317, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 71/75, Train Loss: 0.0230, Train Acc: 0.9850, Train F1: 0.9895 Val Loss: 0.0318, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 72/75, Train Loss: 0.0229, Train Acc: 0.9856, Train F1: 0.9899 Val Loss: 0.0318, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 73/75, Train Loss: 0.0228, Train Acc: 0.9861, Train F1: 0.9902 Val Loss: 0.0318, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 74/75, Train Loss: 0.0228, Train Acc: 0.9857, Train F1: 0.9900 Val Loss: 0.0318, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 75/75, Train Loss: 0.0228, Train Acc: 0.9860, Train F1: 0.9902 Val Loss: 0.0318, Val Acc: 0.9759, Val F1: 0.9834\n",
            "\n",
            " 🔎 search 26 : deep_rescnn --- lr : 0.0012742905281420724, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv4.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1045, Train Acc: 0.8874, Train F1: 0.9226 Val Loss: 0.0827, Val Acc: 0.9275, Val F1: 0.9497\n",
            "Epoch: 2/75, Train Loss: 0.0758, Train Acc: 0.9227, Train F1: 0.9463 Val Loss: 0.0685, Val Acc: 0.9364, Val F1: 0.9565\n",
            "Epoch: 3/75, Train Loss: 0.0611, Train Acc: 0.9415, Train F1: 0.9592 Val Loss: 0.0614, Val Acc: 0.9457, Val F1: 0.9629\n",
            "Epoch: 4/75, Train Loss: 0.0563, Train Acc: 0.9452, Train F1: 0.9620 Val Loss: 0.0703, Val Acc: 0.9296, Val F1: 0.9531\n",
            "Epoch: 5/75, Train Loss: 0.0485, Train Acc: 0.9537, Train F1: 0.9675 Val Loss: 0.0564, Val Acc: 0.9443, Val F1: 0.9607\n",
            "Epoch: 6/75, Train Loss: 0.0440, Train Acc: 0.9600, Train F1: 0.9723 Val Loss: 0.0503, Val Acc: 0.9595, Val F1: 0.9721\n",
            "Epoch: 7/75, Train Loss: 0.0445, Train Acc: 0.9596, Train F1: 0.9721 Val Loss: 0.0466, Val Acc: 0.9639, Val F1: 0.9753\n",
            "Epoch: 8/75, Train Loss: 0.0475, Train Acc: 0.9522, Train F1: 0.9662 Val Loss: 0.1043, Val Acc: 0.8883, Val F1: 0.9282\n",
            "Epoch: 9/75, Train Loss: 0.0440, Train Acc: 0.9623, Train F1: 0.9737 Val Loss: 0.0447, Val Acc: 0.9622, Val F1: 0.9738\n",
            "Epoch: 10/75, Train Loss: 0.0361, Train Acc: 0.9701, Train F1: 0.9790 Val Loss: 0.0437, Val Acc: 0.9677, Val F1: 0.9778\n",
            "Epoch: 11/75, Train Loss: 0.0397, Train Acc: 0.9630, Train F1: 0.9740 Val Loss: 0.0420, Val Acc: 0.9687, Val F1: 0.9784\n",
            "Epoch: 12/75, Train Loss: 0.0372, Train Acc: 0.9670, Train F1: 0.9769 Val Loss: 0.0443, Val Acc: 0.9660, Val F1: 0.9764\n",
            "Epoch: 13/75, Train Loss: 0.0388, Train Acc: 0.9666, Train F1: 0.9766 Val Loss: 0.0529, Val Acc: 0.9550, Val F1: 0.9695\n",
            "Epoch: 14/75, Train Loss: 0.0342, Train Acc: 0.9741, Train F1: 0.9819 Val Loss: 0.0651, Val Acc: 0.9265, Val F1: 0.9468\n",
            "Epoch: 15/75, Train Loss: 0.0336, Train Acc: 0.9733, Train F1: 0.9813 Val Loss: 0.0534, Val Acc: 0.9515, Val F1: 0.9673\n",
            "Epoch: 16/75, Train Loss: 0.0328, Train Acc: 0.9743, Train F1: 0.9821 Val Loss: 0.0394, Val Acc: 0.9708, Val F1: 0.9798\n",
            "Epoch: 17/75, Train Loss: 0.0349, Train Acc: 0.9691, Train F1: 0.9784 Val Loss: 0.0378, Val Acc: 0.9694, Val F1: 0.9788\n",
            "Epoch: 18/75, Train Loss: 0.0325, Train Acc: 0.9731, Train F1: 0.9810 Val Loss: 0.0389, Val Acc: 0.9677, Val F1: 0.9779\n",
            "Epoch: 19/75, Train Loss: 0.0317, Train Acc: 0.9749, Train F1: 0.9825 Val Loss: 0.0581, Val Acc: 0.9426, Val F1: 0.9591\n",
            "Epoch: 20/75, Train Loss: 0.0341, Train Acc: 0.9714, Train F1: 0.9799 Val Loss: 0.0422, Val Acc: 0.9701, Val F1: 0.9794\n",
            "Epoch: 21/75, Train Loss: 0.0353, Train Acc: 0.9688, Train F1: 0.9782 Val Loss: 0.0375, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 22/75, Train Loss: 0.0313, Train Acc: 0.9750, Train F1: 0.9825 Val Loss: 0.0426, Val Acc: 0.9643, Val F1: 0.9756\n",
            "Epoch: 23/75, Train Loss: 0.0290, Train Acc: 0.9784, Train F1: 0.9848 Val Loss: 0.0372, Val Acc: 0.9656, Val F1: 0.9763\n",
            "Epoch: 24/75, Train Loss: 0.0298, Train Acc: 0.9759, Train F1: 0.9831 Val Loss: 0.0386, Val Acc: 0.9663, Val F1: 0.9768\n",
            "Epoch: 25/75, Train Loss: 0.0312, Train Acc: 0.9761, Train F1: 0.9831 Val Loss: 0.0371, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 26/75, Train Loss: 0.0307, Train Acc: 0.9750, Train F1: 0.9825 Val Loss: 0.0391, Val Acc: 0.9708, Val F1: 0.9800\n",
            "Epoch: 27/75, Train Loss: 0.0284, Train Acc: 0.9782, Train F1: 0.9848 Val Loss: 0.0360, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 28/75, Train Loss: 0.0289, Train Acc: 0.9765, Train F1: 0.9837 Val Loss: 0.0355, Val Acc: 0.9722, Val F1: 0.9808\n",
            "Epoch: 29/75, Train Loss: 0.0318, Train Acc: 0.9724, Train F1: 0.9809 Val Loss: 0.0459, Val Acc: 0.9660, Val F1: 0.9769\n",
            "Epoch: 30/75, Train Loss: 0.0273, Train Acc: 0.9800, Train F1: 0.9859 Val Loss: 0.0437, Val Acc: 0.9622, Val F1: 0.9744\n",
            "Epoch: 31/75, Train Loss: 0.0287, Train Acc: 0.9784, Train F1: 0.9849 Val Loss: 0.0393, Val Acc: 0.9687, Val F1: 0.9783\n",
            "Epoch: 32/75, Train Loss: 0.0270, Train Acc: 0.9810, Train F1: 0.9869 Val Loss: 0.0373, Val Acc: 0.9718, Val F1: 0.9806\n",
            "Epoch: 33/75, Train Loss: 0.0291, Train Acc: 0.9761, Train F1: 0.9832 Val Loss: 0.0421, Val Acc: 0.9687, Val F1: 0.9786\n",
            "Epoch: 34/75, Train Loss: 0.0283, Train Acc: 0.9786, Train F1: 0.9850 Val Loss: 0.0530, Val Acc: 0.9509, Val F1: 0.9669\n",
            "Epoch: 35/75, Train Loss: 0.0274, Train Acc: 0.9787, Train F1: 0.9851 Val Loss: 0.0360, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 36/75, Train Loss: 0.0290, Train Acc: 0.9780, Train F1: 0.9847 Val Loss: 0.0356, Val Acc: 0.9708, Val F1: 0.9797\n",
            "Epoch: 37/75, Train Loss: 0.0264, Train Acc: 0.9804, Train F1: 0.9861 Val Loss: 0.0362, Val Acc: 0.9711, Val F1: 0.9799\n",
            "Epoch: 38/75, Train Loss: 0.0265, Train Acc: 0.9798, Train F1: 0.9860 Val Loss: 0.0378, Val Acc: 0.9694, Val F1: 0.9790\n",
            "Epoch: 39/75, Train Loss: 0.0248, Train Acc: 0.9837, Train F1: 0.9886 Val Loss: 0.0385, Val Acc: 0.9632, Val F1: 0.9742\n",
            "Epoch: 40/75, Train Loss: 0.0259, Train Acc: 0.9797, Train F1: 0.9859 Val Loss: 0.0339, Val Acc: 0.9725, Val F1: 0.9809\n",
            "Epoch: 41/75, Train Loss: 0.0239, Train Acc: 0.9838, Train F1: 0.9888 Val Loss: 0.0390, Val Acc: 0.9649, Val F1: 0.9755\n",
            "Epoch: 42/75, Train Loss: 0.0252, Train Acc: 0.9816, Train F1: 0.9870 Val Loss: 0.0339, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 43/75, Train Loss: 0.0252, Train Acc: 0.9825, Train F1: 0.9878 Val Loss: 0.0351, Val Acc: 0.9694, Val F1: 0.9787\n",
            "Epoch: 44/75, Train Loss: 0.0235, Train Acc: 0.9843, Train F1: 0.9890 Val Loss: 0.0374, Val Acc: 0.9660, Val F1: 0.9762\n",
            "Epoch: 45/75, Train Loss: 0.0244, Train Acc: 0.9829, Train F1: 0.9881 Val Loss: 0.0330, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 46/75, Train Loss: 0.0235, Train Acc: 0.9842, Train F1: 0.9890 Val Loss: 0.0333, Val Acc: 0.9749, Val F1: 0.9828\n",
            "Epoch: 47/75, Train Loss: 0.0234, Train Acc: 0.9834, Train F1: 0.9884 Val Loss: 0.0327, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 48/75, Train Loss: 0.0234, Train Acc: 0.9845, Train F1: 0.9892 Val Loss: 0.0323, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 49/75, Train Loss: 0.0227, Train Acc: 0.9838, Train F1: 0.9888 Val Loss: 0.0314, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 50/75, Train Loss: 0.0217, Train Acc: 0.9864, Train F1: 0.9906 Val Loss: 0.0330, Val Acc: 0.9735, Val F1: 0.9816\n",
            "Epoch: 51/75, Train Loss: 0.0221, Train Acc: 0.9855, Train F1: 0.9898 Val Loss: 0.0338, Val Acc: 0.9711, Val F1: 0.9799\n",
            "Epoch: 52/75, Train Loss: 0.0216, Train Acc: 0.9869, Train F1: 0.9908 Val Loss: 0.0325, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 53/75, Train Loss: 0.0215, Train Acc: 0.9874, Train F1: 0.9912 Val Loss: 0.0331, Val Acc: 0.9742, Val F1: 0.9821\n",
            "Epoch: 54/75, Train Loss: 0.0212, Train Acc: 0.9873, Train F1: 0.9912 Val Loss: 0.0310, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 55/75, Train Loss: 0.0209, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0320, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 56/75, Train Loss: 0.0213, Train Acc: 0.9880, Train F1: 0.9915 Val Loss: 0.0329, Val Acc: 0.9742, Val F1: 0.9821\n",
            "Epoch: 57/75, Train Loss: 0.0210, Train Acc: 0.9873, Train F1: 0.9912 Val Loss: 0.0312, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 58/75, Train Loss: 0.0209, Train Acc: 0.9866, Train F1: 0.9907 Val Loss: 0.0319, Val Acc: 0.9729, Val F1: 0.9812\n",
            "Epoch: 59/75, Train Loss: 0.0206, Train Acc: 0.9872, Train F1: 0.9910 Val Loss: 0.0323, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 60/75, Train Loss: 0.0202, Train Acc: 0.9884, Train F1: 0.9919 Val Loss: 0.0323, Val Acc: 0.9735, Val F1: 0.9816\n",
            "Epoch: 61/75, Train Loss: 0.0200, Train Acc: 0.9879, Train F1: 0.9915 Val Loss: 0.0314, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 62/75, Train Loss: 0.0201, Train Acc: 0.9882, Train F1: 0.9918 Val Loss: 0.0312, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 63/75, Train Loss: 0.0203, Train Acc: 0.9877, Train F1: 0.9914 Val Loss: 0.0311, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 64/75, Train Loss: 0.0197, Train Acc: 0.9893, Train F1: 0.9925 Val Loss: 0.0308, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 65/75, Train Loss: 0.0194, Train Acc: 0.9888, Train F1: 0.9922 Val Loss: 0.0308, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 66/75, Train Loss: 0.0195, Train Acc: 0.9896, Train F1: 0.9927 Val Loss: 0.0310, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 67/75, Train Loss: 0.0192, Train Acc: 0.9891, Train F1: 0.9923 Val Loss: 0.0307, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 68/75, Train Loss: 0.0193, Train Acc: 0.9891, Train F1: 0.9924 Val Loss: 0.0312, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 69/75, Train Loss: 0.0192, Train Acc: 0.9897, Train F1: 0.9927 Val Loss: 0.0312, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 70/75, Train Loss: 0.0191, Train Acc: 0.9899, Train F1: 0.9929 Val Loss: 0.0308, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 71/75, Train Loss: 0.0190, Train Acc: 0.9896, Train F1: 0.9928 Val Loss: 0.0307, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 72/75, Train Loss: 0.0189, Train Acc: 0.9895, Train F1: 0.9925 Val Loss: 0.0307, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 73/75, Train Loss: 0.0188, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0307, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 74/75, Train Loss: 0.0188, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0307, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 75/75, Train Loss: 0.0188, Train Acc: 0.9896, Train F1: 0.9928 Val Loss: 0.0307, Val Acc: 0.9770, Val F1: 0.9841\n",
            "\n",
            " 🔎 search 27 : deep_rescnn --- lr : 0.0012742905281420724, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv5.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1188, Train Acc: 0.8607, Train F1: 0.9051 Val Loss: 0.0833, Val Acc: 0.9072, Val F1: 0.9369\n",
            "Epoch: 2/75, Train Loss: 0.0730, Train Acc: 0.9245, Train F1: 0.9470 Val Loss: 0.0707, Val Acc: 0.9275, Val F1: 0.9512\n",
            "Epoch: 3/75, Train Loss: 0.0590, Train Acc: 0.9409, Train F1: 0.9588 Val Loss: 0.0837, Val Acc: 0.9069, Val F1: 0.9391\n",
            "Epoch: 4/75, Train Loss: 0.0530, Train Acc: 0.9505, Train F1: 0.9654 Val Loss: 0.0509, Val Acc: 0.9515, Val F1: 0.9662\n",
            "Epoch: 5/75, Train Loss: 0.0452, Train Acc: 0.9580, Train F1: 0.9708 Val Loss: 0.0486, Val Acc: 0.9608, Val F1: 0.9731\n",
            "Epoch: 6/75, Train Loss: 0.0460, Train Acc: 0.9572, Train F1: 0.9700 Val Loss: 0.0477, Val Acc: 0.9625, Val F1: 0.9740\n",
            "Epoch: 7/75, Train Loss: 0.0431, Train Acc: 0.9578, Train F1: 0.9704 Val Loss: 0.0418, Val Acc: 0.9656, Val F1: 0.9762\n",
            "Epoch: 8/75, Train Loss: 0.0433, Train Acc: 0.9613, Train F1: 0.9728 Val Loss: 0.0409, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 9/75, Train Loss: 0.0376, Train Acc: 0.9669, Train F1: 0.9768 Val Loss: 0.0440, Val Acc: 0.9643, Val F1: 0.9757\n",
            "Epoch: 10/75, Train Loss: 0.0356, Train Acc: 0.9699, Train F1: 0.9788 Val Loss: 0.0527, Val Acc: 0.9509, Val F1: 0.9670\n",
            "Epoch: 11/75, Train Loss: 0.0418, Train Acc: 0.9619, Train F1: 0.9731 Val Loss: 0.0532, Val Acc: 0.9488, Val F1: 0.9657\n",
            "Epoch: 12/75, Train Loss: 0.0342, Train Acc: 0.9731, Train F1: 0.9812 Val Loss: 0.0430, Val Acc: 0.9639, Val F1: 0.9754\n",
            "Epoch: 13/75, Train Loss: 0.0347, Train Acc: 0.9704, Train F1: 0.9792 Val Loss: 0.0429, Val Acc: 0.9663, Val F1: 0.9771\n",
            "Epoch: 14/75, Train Loss: 0.0315, Train Acc: 0.9742, Train F1: 0.9819 Val Loss: 0.0391, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 15/75, Train Loss: 0.0349, Train Acc: 0.9707, Train F1: 0.9796 Val Loss: 0.0374, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 16/75, Train Loss: 0.0326, Train Acc: 0.9751, Train F1: 0.9827 Val Loss: 0.0369, Val Acc: 0.9722, Val F1: 0.9809\n",
            "Epoch: 17/75, Train Loss: 0.0344, Train Acc: 0.9710, Train F1: 0.9798 Val Loss: 0.0400, Val Acc: 0.9636, Val F1: 0.9744\n",
            "Epoch: 18/75, Train Loss: 0.0328, Train Acc: 0.9741, Train F1: 0.9818 Val Loss: 0.0439, Val Acc: 0.9567, Val F1: 0.9694\n",
            "Epoch: 19/75, Train Loss: 0.0324, Train Acc: 0.9755, Train F1: 0.9830 Val Loss: 0.0370, Val Acc: 0.9701, Val F1: 0.9795\n",
            "Epoch: 20/75, Train Loss: 0.0298, Train Acc: 0.9777, Train F1: 0.9843 Val Loss: 0.0363, Val Acc: 0.9660, Val F1: 0.9762\n",
            "Epoch: 21/75, Train Loss: 0.0303, Train Acc: 0.9763, Train F1: 0.9834 Val Loss: 0.0570, Val Acc: 0.9388, Val F1: 0.9562\n",
            "Epoch: 22/75, Train Loss: 0.0288, Train Acc: 0.9786, Train F1: 0.9850 Val Loss: 0.0408, Val Acc: 0.9701, Val F1: 0.9796\n",
            "Epoch: 23/75, Train Loss: 0.0309, Train Acc: 0.9747, Train F1: 0.9823 Val Loss: 0.0342, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 24/75, Train Loss: 0.0289, Train Acc: 0.9793, Train F1: 0.9856 Val Loss: 0.0342, Val Acc: 0.9739, Val F1: 0.9821\n",
            "Epoch: 25/75, Train Loss: 0.0285, Train Acc: 0.9789, Train F1: 0.9854 Val Loss: 0.0440, Val Acc: 0.9570, Val F1: 0.9696\n",
            "Epoch: 26/75, Train Loss: 0.0286, Train Acc: 0.9787, Train F1: 0.9852 Val Loss: 0.0364, Val Acc: 0.9749, Val F1: 0.9828\n",
            "Epoch: 27/75, Train Loss: 0.0284, Train Acc: 0.9773, Train F1: 0.9841 Val Loss: 0.0505, Val Acc: 0.9457, Val F1: 0.9613\n",
            "Epoch: 28/75, Train Loss: 0.0306, Train Acc: 0.9743, Train F1: 0.9823 Val Loss: 0.0401, Val Acc: 0.9698, Val F1: 0.9794\n",
            "Epoch: 29/75, Train Loss: 0.0293, Train Acc: 0.9777, Train F1: 0.9844 Val Loss: 0.0332, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 30/75, Train Loss: 0.0280, Train Acc: 0.9805, Train F1: 0.9863 Val Loss: 0.0365, Val Acc: 0.9711, Val F1: 0.9802\n",
            "Epoch: 31/75, Train Loss: 0.0269, Train Acc: 0.9801, Train F1: 0.9860 Val Loss: 0.0331, Val Acc: 0.9729, Val F1: 0.9812\n",
            "Epoch: 32/75, Train Loss: 0.0266, Train Acc: 0.9817, Train F1: 0.9873 Val Loss: 0.0321, Val Acc: 0.9749, Val F1: 0.9826\n",
            "Epoch: 33/75, Train Loss: 0.0250, Train Acc: 0.9818, Train F1: 0.9873 Val Loss: 0.0446, Val Acc: 0.9533, Val F1: 0.9669\n",
            "Epoch: 34/75, Train Loss: 0.0297, Train Acc: 0.9741, Train F1: 0.9818 Val Loss: 0.0353, Val Acc: 0.9715, Val F1: 0.9801\n",
            "Epoch: 35/75, Train Loss: 0.0262, Train Acc: 0.9795, Train F1: 0.9858 Val Loss: 0.0313, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 36/75, Train Loss: 0.0282, Train Acc: 0.9780, Train F1: 0.9848 Val Loss: 0.0491, Val Acc: 0.9564, Val F1: 0.9706\n",
            "Epoch: 37/75, Train Loss: 0.0254, Train Acc: 0.9827, Train F1: 0.9879 Val Loss: 0.0458, Val Acc: 0.9591, Val F1: 0.9724\n",
            "Epoch: 38/75, Train Loss: 0.0243, Train Acc: 0.9827, Train F1: 0.9879 Val Loss: 0.0312, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 39/75, Train Loss: 0.0250, Train Acc: 0.9826, Train F1: 0.9878 Val Loss: 0.0343, Val Acc: 0.9749, Val F1: 0.9828\n",
            "Epoch: 40/75, Train Loss: 0.0252, Train Acc: 0.9821, Train F1: 0.9875 Val Loss: 0.0366, Val Acc: 0.9725, Val F1: 0.9812\n",
            "Epoch: 41/75, Train Loss: 0.0245, Train Acc: 0.9834, Train F1: 0.9885 Val Loss: 0.0313, Val Acc: 0.9766, Val F1: 0.9840\n",
            "Epoch: 42/75, Train Loss: 0.0233, Train Acc: 0.9843, Train F1: 0.9889 Val Loss: 0.0341, Val Acc: 0.9704, Val F1: 0.9794\n",
            "Epoch: 43/75, Train Loss: 0.0232, Train Acc: 0.9853, Train F1: 0.9897 Val Loss: 0.0418, Val Acc: 0.9601, Val F1: 0.9719\n",
            "Epoch: 44/75, Train Loss: 0.0235, Train Acc: 0.9844, Train F1: 0.9891 Val Loss: 0.0297, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 45/75, Train Loss: 0.0228, Train Acc: 0.9845, Train F1: 0.9892 Val Loss: 0.0297, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 46/75, Train Loss: 0.0231, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0307, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 47/75, Train Loss: 0.0222, Train Acc: 0.9865, Train F1: 0.9905 Val Loss: 0.0305, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 48/75, Train Loss: 0.0233, Train Acc: 0.9837, Train F1: 0.9886 Val Loss: 0.0389, Val Acc: 0.9601, Val F1: 0.9719\n",
            "Epoch: 49/75, Train Loss: 0.0225, Train Acc: 0.9856, Train F1: 0.9899 Val Loss: 0.0305, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 50/75, Train Loss: 0.0226, Train Acc: 0.9859, Train F1: 0.9901 Val Loss: 0.0304, Val Acc: 0.9763, Val F1: 0.9835\n",
            "Epoch: 51/75, Train Loss: 0.0216, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0311, Val Acc: 0.9746, Val F1: 0.9823\n",
            "Epoch: 52/75, Train Loss: 0.0219, Train Acc: 0.9868, Train F1: 0.9908 Val Loss: 0.0293, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 53/75, Train Loss: 0.0217, Train Acc: 0.9873, Train F1: 0.9910 Val Loss: 0.0298, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 54/75, Train Loss: 0.0211, Train Acc: 0.9889, Train F1: 0.9922 Val Loss: 0.0305, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 55/75, Train Loss: 0.0206, Train Acc: 0.9882, Train F1: 0.9917 Val Loss: 0.0307, Val Acc: 0.9804, Val F1: 0.9866\n",
            "Epoch: 56/75, Train Loss: 0.0204, Train Acc: 0.9895, Train F1: 0.9927 Val Loss: 0.0293, Val Acc: 0.9770, Val F1: 0.9840\n",
            "Epoch: 57/75, Train Loss: 0.0203, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0292, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 58/75, Train Loss: 0.0205, Train Acc: 0.9881, Train F1: 0.9917 Val Loss: 0.0300, Val Acc: 0.9780, Val F1: 0.9849\n",
            "Epoch: 59/75, Train Loss: 0.0201, Train Acc: 0.9892, Train F1: 0.9925 Val Loss: 0.0292, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 60/75, Train Loss: 0.0200, Train Acc: 0.9884, Train F1: 0.9918 Val Loss: 0.0316, Val Acc: 0.9780, Val F1: 0.9849\n",
            "Epoch: 61/75, Train Loss: 0.0201, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0288, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 62/75, Train Loss: 0.0199, Train Acc: 0.9888, Train F1: 0.9921 Val Loss: 0.0288, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 63/75, Train Loss: 0.0196, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0287, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 64/75, Train Loss: 0.0196, Train Acc: 0.9896, Train F1: 0.9928 Val Loss: 0.0289, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 65/75, Train Loss: 0.0195, Train Acc: 0.9899, Train F1: 0.9931 Val Loss: 0.0288, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 66/75, Train Loss: 0.0193, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0287, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 67/75, Train Loss: 0.0191, Train Acc: 0.9908, Train F1: 0.9936 Val Loss: 0.0286, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 68/75, Train Loss: 0.0191, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0296, Val Acc: 0.9753, Val F1: 0.9828\n",
            "Epoch: 69/75, Train Loss: 0.0191, Train Acc: 0.9908, Train F1: 0.9936 Val Loss: 0.0291, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 70/75, Train Loss: 0.0190, Train Acc: 0.9907, Train F1: 0.9935 Val Loss: 0.0286, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 71/75, Train Loss: 0.0189, Train Acc: 0.9907, Train F1: 0.9934 Val Loss: 0.0286, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 72/75, Train Loss: 0.0188, Train Acc: 0.9914, Train F1: 0.9939 Val Loss: 0.0286, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 73/75, Train Loss: 0.0188, Train Acc: 0.9918, Train F1: 0.9943 Val Loss: 0.0286, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 74/75, Train Loss: 0.0187, Train Acc: 0.9915, Train F1: 0.9940 Val Loss: 0.0286, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 75/75, Train Loss: 0.0187, Train Acc: 0.9916, Train F1: 0.9942 Val Loss: 0.0286, Val Acc: 0.9787, Val F1: 0.9853\n",
            "\n",
            " 🔎 search 28 : deep_rescnn --- lr : 0.0012742905281420724, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv2.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1382, Train Acc: 0.8480, Train F1: 0.8985 Val Loss: 0.1235, Val Acc: 0.8656, Val F1: 0.9002\n",
            "Epoch: 2/75, Train Loss: 0.1011, Train Acc: 0.9019, Train F1: 0.9316 Val Loss: 0.0935, Val Acc: 0.9175, Val F1: 0.9422\n",
            "Epoch: 3/75, Train Loss: 0.0873, Train Acc: 0.9192, Train F1: 0.9440 Val Loss: 0.0884, Val Acc: 0.9141, Val F1: 0.9394\n",
            "Epoch: 4/75, Train Loss: 0.0798, Train Acc: 0.9268, Train F1: 0.9493 Val Loss: 0.0803, Val Acc: 0.9333, Val F1: 0.9545\n",
            "Epoch: 5/75, Train Loss: 0.0808, Train Acc: 0.9254, Train F1: 0.9486 Val Loss: 0.0818, Val Acc: 0.9275, Val F1: 0.9495\n",
            "Epoch: 6/75, Train Loss: 0.0780, Train Acc: 0.9286, Train F1: 0.9505 Val Loss: 0.0772, Val Acc: 0.9419, Val F1: 0.9605\n",
            "Epoch: 7/75, Train Loss: 0.0722, Train Acc: 0.9354, Train F1: 0.9552 Val Loss: 0.0735, Val Acc: 0.9388, Val F1: 0.9574\n",
            "Epoch: 8/75, Train Loss: 0.0700, Train Acc: 0.9365, Train F1: 0.9561 Val Loss: 0.0722, Val Acc: 0.9381, Val F1: 0.9572\n",
            "Epoch: 9/75, Train Loss: 0.0670, Train Acc: 0.9393, Train F1: 0.9578 Val Loss: 0.0806, Val Acc: 0.9230, Val F1: 0.9485\n",
            "Epoch: 10/75, Train Loss: 0.0701, Train Acc: 0.9362, Train F1: 0.9557 Val Loss: 0.0735, Val Acc: 0.9375, Val F1: 0.9577\n",
            "Epoch: 11/75, Train Loss: 0.0633, Train Acc: 0.9425, Train F1: 0.9602 Val Loss: 0.0809, Val Acc: 0.9158, Val F1: 0.9400\n",
            "Epoch: 12/75, Train Loss: 0.0642, Train Acc: 0.9418, Train F1: 0.9596 Val Loss: 0.0914, Val Acc: 0.8973, Val F1: 0.9332\n",
            "Epoch: 13/75, Train Loss: 0.0619, Train Acc: 0.9441, Train F1: 0.9614 Val Loss: 0.0660, Val Acc: 0.9378, Val F1: 0.9566\n",
            "Epoch: 14/75, Train Loss: 0.0642, Train Acc: 0.9402, Train F1: 0.9585 Val Loss: 0.0759, Val Acc: 0.9302, Val F1: 0.9519\n",
            "Epoch: 15/75, Train Loss: 0.0599, Train Acc: 0.9467, Train F1: 0.9631 Val Loss: 0.0666, Val Acc: 0.9368, Val F1: 0.9560\n",
            "Epoch: 16/75, Train Loss: 0.0588, Train Acc: 0.9471, Train F1: 0.9633 Val Loss: 0.0696, Val Acc: 0.9292, Val F1: 0.9525\n",
            "Epoch: 17/75, Train Loss: 0.0582, Train Acc: 0.9473, Train F1: 0.9633 Val Loss: 0.0605, Val Acc: 0.9471, Val F1: 0.9635\n",
            "Epoch: 18/75, Train Loss: 0.0569, Train Acc: 0.9487, Train F1: 0.9645 Val Loss: 0.0599, Val Acc: 0.9464, Val F1: 0.9631\n",
            "Epoch: 19/75, Train Loss: 0.0588, Train Acc: 0.9436, Train F1: 0.9609 Val Loss: 0.0596, Val Acc: 0.9467, Val F1: 0.9633\n",
            "Epoch: 20/75, Train Loss: 0.0560, Train Acc: 0.9494, Train F1: 0.9649 Val Loss: 0.0681, Val Acc: 0.9330, Val F1: 0.9529\n",
            "Epoch: 21/75, Train Loss: 0.0516, Train Acc: 0.9504, Train F1: 0.9657 Val Loss: 0.0569, Val Acc: 0.9467, Val F1: 0.9630\n",
            "Epoch: 22/75, Train Loss: 0.0524, Train Acc: 0.9487, Train F1: 0.9643 Val Loss: 0.0661, Val Acc: 0.9357, Val F1: 0.9549\n",
            "Epoch: 23/75, Train Loss: 0.0547, Train Acc: 0.9485, Train F1: 0.9644 Val Loss: 0.0664, Val Acc: 0.9392, Val F1: 0.9593\n",
            "Epoch: 24/75, Train Loss: 0.0535, Train Acc: 0.9532, Train F1: 0.9675 Val Loss: 0.0588, Val Acc: 0.9471, Val F1: 0.9639\n",
            "Epoch: 25/75, Train Loss: 0.0505, Train Acc: 0.9545, Train F1: 0.9685 Val Loss: 0.0560, Val Acc: 0.9509, Val F1: 0.9662\n",
            "Epoch: 26/75, Train Loss: 0.0478, Train Acc: 0.9592, Train F1: 0.9718 Val Loss: 0.0546, Val Acc: 0.9546, Val F1: 0.9685\n",
            "Epoch: 27/75, Train Loss: 0.0512, Train Acc: 0.9513, Train F1: 0.9664 Val Loss: 0.0525, Val Acc: 0.9495, Val F1: 0.9651\n",
            "Epoch: 28/75, Train Loss: 0.0471, Train Acc: 0.9573, Train F1: 0.9704 Val Loss: 0.0642, Val Acc: 0.9357, Val F1: 0.9548\n",
            "Epoch: 29/75, Train Loss: 0.0470, Train Acc: 0.9569, Train F1: 0.9703 Val Loss: 0.0647, Val Acc: 0.9385, Val F1: 0.9566\n",
            "Epoch: 30/75, Train Loss: 0.0508, Train Acc: 0.9518, Train F1: 0.9665 Val Loss: 0.0608, Val Acc: 0.9443, Val F1: 0.9623\n",
            "Epoch: 31/75, Train Loss: 0.0460, Train Acc: 0.9572, Train F1: 0.9702 Val Loss: 0.0549, Val Acc: 0.9457, Val F1: 0.9621\n",
            "Epoch: 32/75, Train Loss: 0.0510, Train Acc: 0.9537, Train F1: 0.9679 Val Loss: 0.0549, Val Acc: 0.9502, Val F1: 0.9660\n",
            "Epoch: 33/75, Train Loss: 0.0444, Train Acc: 0.9604, Train F1: 0.9726 Val Loss: 0.0497, Val Acc: 0.9553, Val F1: 0.9692\n",
            "Epoch: 34/75, Train Loss: 0.0433, Train Acc: 0.9629, Train F1: 0.9743 Val Loss: 0.0577, Val Acc: 0.9481, Val F1: 0.9637\n",
            "Epoch: 35/75, Train Loss: 0.0438, Train Acc: 0.9619, Train F1: 0.9737 Val Loss: 0.0522, Val Acc: 0.9574, Val F1: 0.9705\n",
            "Epoch: 36/75, Train Loss: 0.0467, Train Acc: 0.9570, Train F1: 0.9702 Val Loss: 0.0498, Val Acc: 0.9646, Val F1: 0.9755\n",
            "Epoch: 37/75, Train Loss: 0.0482, Train Acc: 0.9562, Train F1: 0.9696 Val Loss: 0.0507, Val Acc: 0.9584, Val F1: 0.9717\n",
            "Epoch: 38/75, Train Loss: 0.0434, Train Acc: 0.9592, Train F1: 0.9717 Val Loss: 0.0485, Val Acc: 0.9591, Val F1: 0.9717\n",
            "Epoch: 39/75, Train Loss: 0.0440, Train Acc: 0.9597, Train F1: 0.9722 Val Loss: 0.0494, Val Acc: 0.9533, Val F1: 0.9679\n",
            "Epoch: 40/75, Train Loss: 0.0415, Train Acc: 0.9635, Train F1: 0.9745 Val Loss: 0.0482, Val Acc: 0.9581, Val F1: 0.9711\n",
            "Epoch: 41/75, Train Loss: 0.0412, Train Acc: 0.9645, Train F1: 0.9754 Val Loss: 0.0485, Val Acc: 0.9557, Val F1: 0.9693\n",
            "Epoch: 42/75, Train Loss: 0.0441, Train Acc: 0.9599, Train F1: 0.9722 Val Loss: 0.0496, Val Acc: 0.9567, Val F1: 0.9705\n",
            "Epoch: 43/75, Train Loss: 0.0421, Train Acc: 0.9617, Train F1: 0.9734 Val Loss: 0.0505, Val Acc: 0.9577, Val F1: 0.9713\n",
            "Epoch: 44/75, Train Loss: 0.0406, Train Acc: 0.9641, Train F1: 0.9752 Val Loss: 0.0506, Val Acc: 0.9601, Val F1: 0.9721\n",
            "Epoch: 45/75, Train Loss: 0.0405, Train Acc: 0.9643, Train F1: 0.9752 Val Loss: 0.0484, Val Acc: 0.9629, Val F1: 0.9746\n",
            "Epoch: 46/75, Train Loss: 0.0430, Train Acc: 0.9625, Train F1: 0.9740 Val Loss: 0.0474, Val Acc: 0.9653, Val F1: 0.9763\n",
            "Epoch: 47/75, Train Loss: 0.0390, Train Acc: 0.9680, Train F1: 0.9779 Val Loss: 0.0474, Val Acc: 0.9636, Val F1: 0.9752\n",
            "Epoch: 48/75, Train Loss: 0.0395, Train Acc: 0.9643, Train F1: 0.9753 Val Loss: 0.0545, Val Acc: 0.9515, Val F1: 0.9659\n",
            "Epoch: 49/75, Train Loss: 0.0404, Train Acc: 0.9655, Train F1: 0.9761 Val Loss: 0.0570, Val Acc: 0.9488, Val F1: 0.9656\n",
            "Epoch: 50/75, Train Loss: 0.0384, Train Acc: 0.9691, Train F1: 0.9786 Val Loss: 0.0489, Val Acc: 0.9595, Val F1: 0.9724\n",
            "Epoch: 51/75, Train Loss: 0.0390, Train Acc: 0.9669, Train F1: 0.9771 Val Loss: 0.0459, Val Acc: 0.9636, Val F1: 0.9748\n",
            "Epoch: 52/75, Train Loss: 0.0369, Train Acc: 0.9690, Train F1: 0.9786 Val Loss: 0.0458, Val Acc: 0.9653, Val F1: 0.9762\n",
            "Epoch: 53/75, Train Loss: 0.0369, Train Acc: 0.9703, Train F1: 0.9794 Val Loss: 0.0464, Val Acc: 0.9646, Val F1: 0.9758\n",
            "Epoch: 54/75, Train Loss: 0.0372, Train Acc: 0.9690, Train F1: 0.9785 Val Loss: 0.0477, Val Acc: 0.9591, Val F1: 0.9722\n",
            "Epoch: 55/75, Train Loss: 0.0361, Train Acc: 0.9704, Train F1: 0.9793 Val Loss: 0.0447, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 56/75, Train Loss: 0.0365, Train Acc: 0.9696, Train F1: 0.9791 Val Loss: 0.0438, Val Acc: 0.9632, Val F1: 0.9746\n",
            "Epoch: 57/75, Train Loss: 0.0356, Train Acc: 0.9706, Train F1: 0.9795 Val Loss: 0.0445, Val Acc: 0.9667, Val F1: 0.9769\n",
            "Epoch: 58/75, Train Loss: 0.0360, Train Acc: 0.9699, Train F1: 0.9791 Val Loss: 0.0442, Val Acc: 0.9646, Val F1: 0.9757\n",
            "Epoch: 59/75, Train Loss: 0.0351, Train Acc: 0.9717, Train F1: 0.9805 Val Loss: 0.0443, Val Acc: 0.9639, Val F1: 0.9754\n",
            "Epoch: 60/75, Train Loss: 0.0348, Train Acc: 0.9718, Train F1: 0.9805 Val Loss: 0.0433, Val Acc: 0.9656, Val F1: 0.9762\n",
            "Epoch: 61/75, Train Loss: 0.0352, Train Acc: 0.9716, Train F1: 0.9803 Val Loss: 0.0443, Val Acc: 0.9649, Val F1: 0.9760\n",
            "Epoch: 62/75, Train Loss: 0.0343, Train Acc: 0.9734, Train F1: 0.9816 Val Loss: 0.0432, Val Acc: 0.9656, Val F1: 0.9764\n",
            "Epoch: 63/75, Train Loss: 0.0342, Train Acc: 0.9742, Train F1: 0.9821 Val Loss: 0.0448, Val Acc: 0.9639, Val F1: 0.9754\n",
            "Epoch: 64/75, Train Loss: 0.0341, Train Acc: 0.9731, Train F1: 0.9814 Val Loss: 0.0436, Val Acc: 0.9649, Val F1: 0.9760\n",
            "Epoch: 65/75, Train Loss: 0.0347, Train Acc: 0.9722, Train F1: 0.9806 Val Loss: 0.0425, Val Acc: 0.9680, Val F1: 0.9780\n",
            "Epoch: 66/75, Train Loss: 0.0336, Train Acc: 0.9748, Train F1: 0.9824 Val Loss: 0.0432, Val Acc: 0.9674, Val F1: 0.9774\n",
            "Epoch: 67/75, Train Loss: 0.0335, Train Acc: 0.9749, Train F1: 0.9826 Val Loss: 0.0432, Val Acc: 0.9663, Val F1: 0.9769\n",
            "Epoch: 68/75, Train Loss: 0.0334, Train Acc: 0.9746, Train F1: 0.9824 Val Loss: 0.0426, Val Acc: 0.9691, Val F1: 0.9788\n",
            "Epoch: 69/75, Train Loss: 0.0330, Train Acc: 0.9757, Train F1: 0.9831 Val Loss: 0.0425, Val Acc: 0.9670, Val F1: 0.9773\n",
            "Epoch: 70/75, Train Loss: 0.0329, Train Acc: 0.9762, Train F1: 0.9835 Val Loss: 0.0428, Val Acc: 0.9670, Val F1: 0.9774\n",
            "Epoch: 71/75, Train Loss: 0.0329, Train Acc: 0.9753, Train F1: 0.9828 Val Loss: 0.0429, Val Acc: 0.9687, Val F1: 0.9786\n",
            "Epoch: 72/75, Train Loss: 0.0330, Train Acc: 0.9749, Train F1: 0.9825 Val Loss: 0.0424, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 73/75, Train Loss: 0.0328, Train Acc: 0.9761, Train F1: 0.9835 Val Loss: 0.0423, Val Acc: 0.9680, Val F1: 0.9780\n",
            "Epoch: 74/75, Train Loss: 0.0327, Train Acc: 0.9757, Train F1: 0.9831 Val Loss: 0.0423, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 75/75, Train Loss: 0.0327, Train Acc: 0.9758, Train F1: 0.9833 Val Loss: 0.0423, Val Acc: 0.9684, Val F1: 0.9783\n",
            "\n",
            " 🔎 search 29 : deep_rescnn --- lr : 0.0012742905281420724, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv3.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1234, Train Acc: 0.8581, Train F1: 0.9035 Val Loss: 0.1042, Val Acc: 0.8821, Val F1: 0.9232\n",
            "Epoch: 2/75, Train Loss: 0.0849, Train Acc: 0.9144, Train F1: 0.9403 Val Loss: 0.1120, Val Acc: 0.8670, Val F1: 0.9149\n",
            "Epoch: 3/75, Train Loss: 0.0689, Train Acc: 0.9379, Train F1: 0.9565 Val Loss: 0.0707, Val Acc: 0.9368, Val F1: 0.9571\n",
            "Epoch: 4/75, Train Loss: 0.0607, Train Acc: 0.9416, Train F1: 0.9596 Val Loss: 0.0711, Val Acc: 0.9323, Val F1: 0.9549\n",
            "Epoch: 5/75, Train Loss: 0.0561, Train Acc: 0.9450, Train F1: 0.9620 Val Loss: 0.0572, Val Acc: 0.9498, Val F1: 0.9652\n",
            "Epoch: 6/75, Train Loss: 0.0507, Train Acc: 0.9544, Train F1: 0.9683 Val Loss: 0.0614, Val Acc: 0.9409, Val F1: 0.9587\n",
            "Epoch: 7/75, Train Loss: 0.0522, Train Acc: 0.9519, Train F1: 0.9667 Val Loss: 0.0595, Val Acc: 0.9412, Val F1: 0.9590\n",
            "Epoch: 8/75, Train Loss: 0.0487, Train Acc: 0.9538, Train F1: 0.9679 Val Loss: 0.0580, Val Acc: 0.9478, Val F1: 0.9648\n",
            "Epoch: 9/75, Train Loss: 0.0453, Train Acc: 0.9612, Train F1: 0.9730 Val Loss: 0.0520, Val Acc: 0.9543, Val F1: 0.9689\n",
            "Epoch: 10/75, Train Loss: 0.0468, Train Acc: 0.9561, Train F1: 0.9696 Val Loss: 0.0648, Val Acc: 0.9405, Val F1: 0.9602\n",
            "Epoch: 11/75, Train Loss: 0.0527, Train Acc: 0.9494, Train F1: 0.9649 Val Loss: 0.0501, Val Acc: 0.9560, Val F1: 0.9698\n",
            "Epoch: 12/75, Train Loss: 0.0448, Train Acc: 0.9600, Train F1: 0.9723 Val Loss: 0.0510, Val Acc: 0.9567, Val F1: 0.9697\n",
            "Epoch: 13/75, Train Loss: 0.0441, Train Acc: 0.9591, Train F1: 0.9716 Val Loss: 0.0514, Val Acc: 0.9567, Val F1: 0.9702\n",
            "Epoch: 14/75, Train Loss: 0.0408, Train Acc: 0.9648, Train F1: 0.9756 Val Loss: 0.0460, Val Acc: 0.9636, Val F1: 0.9747\n",
            "Epoch: 15/75, Train Loss: 0.0409, Train Acc: 0.9636, Train F1: 0.9746 Val Loss: 0.0465, Val Acc: 0.9612, Val F1: 0.9734\n",
            "Epoch: 16/75, Train Loss: 0.0422, Train Acc: 0.9625, Train F1: 0.9741 Val Loss: 0.0478, Val Acc: 0.9629, Val F1: 0.9741\n",
            "Epoch: 17/75, Train Loss: 0.0386, Train Acc: 0.9676, Train F1: 0.9775 Val Loss: 0.0418, Val Acc: 0.9667, Val F1: 0.9769\n",
            "Epoch: 18/75, Train Loss: 0.0383, Train Acc: 0.9677, Train F1: 0.9776 Val Loss: 0.0458, Val Acc: 0.9605, Val F1: 0.9725\n",
            "Epoch: 19/75, Train Loss: 0.0442, Train Acc: 0.9575, Train F1: 0.9706 Val Loss: 0.0441, Val Acc: 0.9649, Val F1: 0.9758\n",
            "Epoch: 20/75, Train Loss: 0.0362, Train Acc: 0.9710, Train F1: 0.9798 Val Loss: 0.0424, Val Acc: 0.9643, Val F1: 0.9752\n",
            "Epoch: 21/75, Train Loss: 0.0383, Train Acc: 0.9674, Train F1: 0.9774 Val Loss: 0.0458, Val Acc: 0.9591, Val F1: 0.9714\n",
            "Epoch: 22/75, Train Loss: 0.0357, Train Acc: 0.9710, Train F1: 0.9799 Val Loss: 0.0441, Val Acc: 0.9625, Val F1: 0.9738\n",
            "Epoch: 23/75, Train Loss: 0.0357, Train Acc: 0.9709, Train F1: 0.9797 Val Loss: 0.0416, Val Acc: 0.9663, Val F1: 0.9766\n",
            "Epoch: 24/75, Train Loss: 0.0384, Train Acc: 0.9664, Train F1: 0.9765 Val Loss: 0.0551, Val Acc: 0.9540, Val F1: 0.9690\n",
            "Epoch: 25/75, Train Loss: 0.0352, Train Acc: 0.9712, Train F1: 0.9800 Val Loss: 0.0400, Val Acc: 0.9684, Val F1: 0.9782\n",
            "Epoch: 26/75, Train Loss: 0.0374, Train Acc: 0.9672, Train F1: 0.9771 Val Loss: 0.0611, Val Acc: 0.9357, Val F1: 0.9540\n",
            "Epoch: 27/75, Train Loss: 0.0360, Train Acc: 0.9723, Train F1: 0.9808 Val Loss: 0.0405, Val Acc: 0.9629, Val F1: 0.9744\n",
            "Epoch: 28/75, Train Loss: 0.0345, Train Acc: 0.9715, Train F1: 0.9801 Val Loss: 0.0555, Val Acc: 0.9423, Val F1: 0.9588\n",
            "Epoch: 29/75, Train Loss: 0.0341, Train Acc: 0.9714, Train F1: 0.9801 Val Loss: 0.0431, Val Acc: 0.9608, Val F1: 0.9727\n",
            "Epoch: 30/75, Train Loss: 0.0340, Train Acc: 0.9730, Train F1: 0.9813 Val Loss: 0.0390, Val Acc: 0.9660, Val F1: 0.9765\n",
            "Epoch: 31/75, Train Loss: 0.0317, Train Acc: 0.9762, Train F1: 0.9834 Val Loss: 0.0479, Val Acc: 0.9601, Val F1: 0.9726\n",
            "Epoch: 32/75, Train Loss: 0.0360, Train Acc: 0.9710, Train F1: 0.9799 Val Loss: 0.0395, Val Acc: 0.9656, Val F1: 0.9764\n",
            "Epoch: 33/75, Train Loss: 0.0321, Train Acc: 0.9735, Train F1: 0.9816 Val Loss: 0.0377, Val Acc: 0.9677, Val F1: 0.9776\n",
            "Epoch: 34/75, Train Loss: 0.0337, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0407, Val Acc: 0.9698, Val F1: 0.9793\n",
            "Epoch: 35/75, Train Loss: 0.0297, Train Acc: 0.9781, Train F1: 0.9848 Val Loss: 0.0368, Val Acc: 0.9715, Val F1: 0.9804\n",
            "Epoch: 36/75, Train Loss: 0.0305, Train Acc: 0.9773, Train F1: 0.9843 Val Loss: 0.0366, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 37/75, Train Loss: 0.0325, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0361, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 38/75, Train Loss: 0.0305, Train Acc: 0.9772, Train F1: 0.9842 Val Loss: 0.0452, Val Acc: 0.9560, Val F1: 0.9689\n",
            "Epoch: 39/75, Train Loss: 0.0308, Train Acc: 0.9767, Train F1: 0.9838 Val Loss: 0.0386, Val Acc: 0.9698, Val F1: 0.9793\n",
            "Epoch: 40/75, Train Loss: 0.0282, Train Acc: 0.9806, Train F1: 0.9865 Val Loss: 0.0349, Val Acc: 0.9704, Val F1: 0.9795\n",
            "Epoch: 41/75, Train Loss: 0.0299, Train Acc: 0.9771, Train F1: 0.9841 Val Loss: 0.0445, Val Acc: 0.9680, Val F1: 0.9783\n",
            "Epoch: 42/75, Train Loss: 0.0303, Train Acc: 0.9772, Train F1: 0.9842 Val Loss: 0.0360, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 43/75, Train Loss: 0.0283, Train Acc: 0.9792, Train F1: 0.9856 Val Loss: 0.0359, Val Acc: 0.9729, Val F1: 0.9814\n",
            "Epoch: 44/75, Train Loss: 0.0273, Train Acc: 0.9821, Train F1: 0.9876 Val Loss: 0.0405, Val Acc: 0.9649, Val F1: 0.9761\n",
            "Epoch: 45/75, Train Loss: 0.0283, Train Acc: 0.9797, Train F1: 0.9859 Val Loss: 0.0348, Val Acc: 0.9729, Val F1: 0.9812\n",
            "Epoch: 46/75, Train Loss: 0.0285, Train Acc: 0.9778, Train F1: 0.9845 Val Loss: 0.0368, Val Acc: 0.9694, Val F1: 0.9790\n",
            "Epoch: 47/75, Train Loss: 0.0273, Train Acc: 0.9805, Train F1: 0.9865 Val Loss: 0.0378, Val Acc: 0.9725, Val F1: 0.9812\n",
            "Epoch: 48/75, Train Loss: 0.0272, Train Acc: 0.9809, Train F1: 0.9867 Val Loss: 0.0348, Val Acc: 0.9687, Val F1: 0.9783\n",
            "Epoch: 49/75, Train Loss: 0.0273, Train Acc: 0.9822, Train F1: 0.9876 Val Loss: 0.0354, Val Acc: 0.9725, Val F1: 0.9812\n",
            "Epoch: 50/75, Train Loss: 0.0312, Train Acc: 0.9739, Train F1: 0.9819 Val Loss: 0.0352, Val Acc: 0.9715, Val F1: 0.9802\n",
            "Epoch: 51/75, Train Loss: 0.0273, Train Acc: 0.9805, Train F1: 0.9865 Val Loss: 0.0335, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 52/75, Train Loss: 0.0267, Train Acc: 0.9830, Train F1: 0.9883 Val Loss: 0.0341, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 53/75, Train Loss: 0.0256, Train Acc: 0.9836, Train F1: 0.9886 Val Loss: 0.0365, Val Acc: 0.9735, Val F1: 0.9819\n",
            "Epoch: 54/75, Train Loss: 0.0253, Train Acc: 0.9832, Train F1: 0.9882 Val Loss: 0.0336, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 55/75, Train Loss: 0.0251, Train Acc: 0.9835, Train F1: 0.9885 Val Loss: 0.0333, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 56/75, Train Loss: 0.0247, Train Acc: 0.9840, Train F1: 0.9888 Val Loss: 0.0327, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 57/75, Train Loss: 0.0253, Train Acc: 0.9832, Train F1: 0.9882 Val Loss: 0.0332, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 58/75, Train Loss: 0.0243, Train Acc: 0.9849, Train F1: 0.9895 Val Loss: 0.0327, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 59/75, Train Loss: 0.0246, Train Acc: 0.9849, Train F1: 0.9896 Val Loss: 0.0323, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 60/75, Train Loss: 0.0245, Train Acc: 0.9843, Train F1: 0.9891 Val Loss: 0.0328, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 61/75, Train Loss: 0.0239, Train Acc: 0.9848, Train F1: 0.9894 Val Loss: 0.0324, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 62/75, Train Loss: 0.0240, Train Acc: 0.9861, Train F1: 0.9904 Val Loss: 0.0328, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 63/75, Train Loss: 0.0240, Train Acc: 0.9851, Train F1: 0.9896 Val Loss: 0.0323, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 64/75, Train Loss: 0.0237, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0323, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 65/75, Train Loss: 0.0234, Train Acc: 0.9866, Train F1: 0.9907 Val Loss: 0.0327, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 66/75, Train Loss: 0.0234, Train Acc: 0.9864, Train F1: 0.9905 Val Loss: 0.0321, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 67/75, Train Loss: 0.0234, Train Acc: 0.9863, Train F1: 0.9906 Val Loss: 0.0327, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 68/75, Train Loss: 0.0234, Train Acc: 0.9859, Train F1: 0.9902 Val Loss: 0.0325, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 69/75, Train Loss: 0.0232, Train Acc: 0.9865, Train F1: 0.9906 Val Loss: 0.0320, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 70/75, Train Loss: 0.0231, Train Acc: 0.9867, Train F1: 0.9907 Val Loss: 0.0319, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 71/75, Train Loss: 0.0231, Train Acc: 0.9866, Train F1: 0.9907 Val Loss: 0.0319, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 72/75, Train Loss: 0.0230, Train Acc: 0.9873, Train F1: 0.9912 Val Loss: 0.0319, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 73/75, Train Loss: 0.0229, Train Acc: 0.9867, Train F1: 0.9908 Val Loss: 0.0319, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 74/75, Train Loss: 0.0229, Train Acc: 0.9871, Train F1: 0.9910 Val Loss: 0.0319, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 75/75, Train Loss: 0.0229, Train Acc: 0.9868, Train F1: 0.9908 Val Loss: 0.0319, Val Acc: 0.9739, Val F1: 0.9820\n",
            "\n",
            " 🔎 search 30 : deep_rescnn --- lr : 0.0012742905281420724, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv4.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1215, Train Acc: 0.8614, Train F1: 0.9047 Val Loss: 0.0856, Val Acc: 0.9210, Val F1: 0.9455\n",
            "Epoch: 2/75, Train Loss: 0.0764, Train Acc: 0.9220, Train F1: 0.9462 Val Loss: 0.0751, Val Acc: 0.9282, Val F1: 0.9491\n",
            "Epoch: 3/75, Train Loss: 0.0628, Train Acc: 0.9381, Train F1: 0.9570 Val Loss: 0.0621, Val Acc: 0.9392, Val F1: 0.9589\n",
            "Epoch: 4/75, Train Loss: 0.0553, Train Acc: 0.9457, Train F1: 0.9625 Val Loss: 0.0517, Val Acc: 0.9612, Val F1: 0.9734\n",
            "Epoch: 5/75, Train Loss: 0.0477, Train Acc: 0.9566, Train F1: 0.9700 Val Loss: 0.0594, Val Acc: 0.9395, Val F1: 0.9571\n",
            "Epoch: 6/75, Train Loss: 0.0474, Train Acc: 0.9553, Train F1: 0.9690 Val Loss: 0.0562, Val Acc: 0.9443, Val F1: 0.9626\n",
            "Epoch: 7/75, Train Loss: 0.0420, Train Acc: 0.9638, Train F1: 0.9748 Val Loss: 0.0510, Val Acc: 0.9570, Val F1: 0.9709\n",
            "Epoch: 8/75, Train Loss: 0.0402, Train Acc: 0.9645, Train F1: 0.9755 Val Loss: 0.0469, Val Acc: 0.9639, Val F1: 0.9752\n",
            "Epoch: 9/75, Train Loss: 0.0383, Train Acc: 0.9679, Train F1: 0.9778 Val Loss: 0.0674, Val Acc: 0.9268, Val F1: 0.9472\n",
            "Epoch: 10/75, Train Loss: 0.0353, Train Acc: 0.9723, Train F1: 0.9806 Val Loss: 0.0402, Val Acc: 0.9687, Val F1: 0.9783\n",
            "Epoch: 11/75, Train Loss: 0.0368, Train Acc: 0.9682, Train F1: 0.9779 Val Loss: 0.0515, Val Acc: 0.9502, Val F1: 0.9647\n",
            "Epoch: 12/75, Train Loss: 0.0427, Train Acc: 0.9600, Train F1: 0.9724 Val Loss: 0.0464, Val Acc: 0.9677, Val F1: 0.9777\n",
            "Epoch: 13/75, Train Loss: 0.0330, Train Acc: 0.9755, Train F1: 0.9830 Val Loss: 0.0386, Val Acc: 0.9718, Val F1: 0.9806\n",
            "Epoch: 14/75, Train Loss: 0.0338, Train Acc: 0.9733, Train F1: 0.9815 Val Loss: 0.0407, Val Acc: 0.9639, Val F1: 0.9748\n",
            "Epoch: 15/75, Train Loss: 0.0333, Train Acc: 0.9726, Train F1: 0.9811 Val Loss: 0.0480, Val Acc: 0.9632, Val F1: 0.9745\n",
            "Epoch: 16/75, Train Loss: 0.0320, Train Acc: 0.9755, Train F1: 0.9829 Val Loss: 0.0418, Val Acc: 0.9663, Val F1: 0.9770\n",
            "Epoch: 17/75, Train Loss: 0.0301, Train Acc: 0.9761, Train F1: 0.9834 Val Loss: 0.0363, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 18/75, Train Loss: 0.0342, Train Acc: 0.9695, Train F1: 0.9787 Val Loss: 0.0986, Val Acc: 0.8797, Val F1: 0.9231\n",
            "Epoch: 19/75, Train Loss: 0.0349, Train Acc: 0.9709, Train F1: 0.9800 Val Loss: 0.0850, Val Acc: 0.9021, Val F1: 0.9277\n",
            "Epoch: 20/75, Train Loss: 0.0448, Train Acc: 0.9557, Train F1: 0.9694 Val Loss: 0.0412, Val Acc: 0.9684, Val F1: 0.9784\n",
            "Epoch: 21/75, Train Loss: 0.0290, Train Acc: 0.9793, Train F1: 0.9857 Val Loss: 0.0373, Val Acc: 0.9725, Val F1: 0.9809\n",
            "Epoch: 22/75, Train Loss: 0.0293, Train Acc: 0.9781, Train F1: 0.9848 Val Loss: 0.0475, Val Acc: 0.9625, Val F1: 0.9746\n",
            "Epoch: 23/75, Train Loss: 0.0318, Train Acc: 0.9724, Train F1: 0.9807 Val Loss: 0.0367, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 24/75, Train Loss: 0.0303, Train Acc: 0.9774, Train F1: 0.9843 Val Loss: 0.0374, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 25/75, Train Loss: 0.0334, Train Acc: 0.9712, Train F1: 0.9801 Val Loss: 0.0360, Val Acc: 0.9739, Val F1: 0.9821\n",
            "Epoch: 26/75, Train Loss: 0.0266, Train Acc: 0.9806, Train F1: 0.9865 Val Loss: 0.0467, Val Acc: 0.9612, Val F1: 0.9736\n",
            "Epoch: 27/75, Train Loss: 0.0300, Train Acc: 0.9774, Train F1: 0.9845 Val Loss: 0.0459, Val Acc: 0.9581, Val F1: 0.9704\n",
            "Epoch: 28/75, Train Loss: 0.0277, Train Acc: 0.9806, Train F1: 0.9866 Val Loss: 0.0350, Val Acc: 0.9735, Val F1: 0.9819\n",
            "Epoch: 29/75, Train Loss: 0.0274, Train Acc: 0.9806, Train F1: 0.9864 Val Loss: 0.0339, Val Acc: 0.9711, Val F1: 0.9801\n",
            "Epoch: 30/75, Train Loss: 0.0274, Train Acc: 0.9804, Train F1: 0.9864 Val Loss: 0.0367, Val Acc: 0.9725, Val F1: 0.9812\n",
            "Epoch: 31/75, Train Loss: 0.0247, Train Acc: 0.9840, Train F1: 0.9888 Val Loss: 0.0345, Val Acc: 0.9735, Val F1: 0.9816\n",
            "Epoch: 32/75, Train Loss: 0.0266, Train Acc: 0.9802, Train F1: 0.9863 Val Loss: 0.0336, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 33/75, Train Loss: 0.0261, Train Acc: 0.9803, Train F1: 0.9863 Val Loss: 0.0343, Val Acc: 0.9701, Val F1: 0.9792\n",
            "Epoch: 34/75, Train Loss: 0.0260, Train Acc: 0.9833, Train F1: 0.9884 Val Loss: 0.0342, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 35/75, Train Loss: 0.0257, Train Acc: 0.9827, Train F1: 0.9879 Val Loss: 0.0357, Val Acc: 0.9759, Val F1: 0.9835\n",
            "Epoch: 36/75, Train Loss: 0.0257, Train Acc: 0.9811, Train F1: 0.9869 Val Loss: 0.0319, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 37/75, Train Loss: 0.0253, Train Acc: 0.9820, Train F1: 0.9875 Val Loss: 0.0374, Val Acc: 0.9684, Val F1: 0.9779\n",
            "Epoch: 38/75, Train Loss: 0.0234, Train Acc: 0.9840, Train F1: 0.9889 Val Loss: 0.0320, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 39/75, Train Loss: 0.0249, Train Acc: 0.9837, Train F1: 0.9887 Val Loss: 0.0380, Val Acc: 0.9674, Val F1: 0.9777\n",
            "Epoch: 40/75, Train Loss: 0.0225, Train Acc: 0.9857, Train F1: 0.9900 Val Loss: 0.0322, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 41/75, Train Loss: 0.0272, Train Acc: 0.9786, Train F1: 0.9850 Val Loss: 0.0338, Val Acc: 0.9725, Val F1: 0.9811\n",
            "Epoch: 42/75, Train Loss: 0.0236, Train Acc: 0.9838, Train F1: 0.9888 Val Loss: 0.0311, Val Acc: 0.9749, Val F1: 0.9826\n",
            "Epoch: 43/75, Train Loss: 0.0240, Train Acc: 0.9834, Train F1: 0.9884 Val Loss: 0.0313, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 44/75, Train Loss: 0.0243, Train Acc: 0.9830, Train F1: 0.9883 Val Loss: 0.0328, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 45/75, Train Loss: 0.0224, Train Acc: 0.9863, Train F1: 0.9905 Val Loss: 0.0340, Val Acc: 0.9735, Val F1: 0.9816\n",
            "Epoch: 46/75, Train Loss: 0.0217, Train Acc: 0.9881, Train F1: 0.9918 Val Loss: 0.0317, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 47/75, Train Loss: 0.0242, Train Acc: 0.9826, Train F1: 0.9878 Val Loss: 0.0386, Val Acc: 0.9694, Val F1: 0.9792\n",
            "Epoch: 48/75, Train Loss: 0.0231, Train Acc: 0.9847, Train F1: 0.9894 Val Loss: 0.0323, Val Acc: 0.9753, Val F1: 0.9828\n",
            "Epoch: 49/75, Train Loss: 0.0215, Train Acc: 0.9858, Train F1: 0.9901 Val Loss: 0.0311, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 50/75, Train Loss: 0.0216, Train Acc: 0.9877, Train F1: 0.9915 Val Loss: 0.0304, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 51/75, Train Loss: 0.0210, Train Acc: 0.9873, Train F1: 0.9911 Val Loss: 0.0322, Val Acc: 0.9725, Val F1: 0.9809\n",
            "Epoch: 52/75, Train Loss: 0.0217, Train Acc: 0.9868, Train F1: 0.9909 Val Loss: 0.0313, Val Acc: 0.9753, Val F1: 0.9828\n",
            "Epoch: 53/75, Train Loss: 0.0212, Train Acc: 0.9873, Train F1: 0.9912 Val Loss: 0.0297, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 54/75, Train Loss: 0.0222, Train Acc: 0.9844, Train F1: 0.9891 Val Loss: 0.0306, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 55/75, Train Loss: 0.0207, Train Acc: 0.9890, Train F1: 0.9924 Val Loss: 0.0301, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 56/75, Train Loss: 0.0203, Train Acc: 0.9889, Train F1: 0.9923 Val Loss: 0.0298, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 57/75, Train Loss: 0.0202, Train Acc: 0.9889, Train F1: 0.9923 Val Loss: 0.0303, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 58/75, Train Loss: 0.0213, Train Acc: 0.9865, Train F1: 0.9906 Val Loss: 0.0301, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 59/75, Train Loss: 0.0202, Train Acc: 0.9887, Train F1: 0.9922 Val Loss: 0.0327, Val Acc: 0.9753, Val F1: 0.9828\n",
            "Epoch: 60/75, Train Loss: 0.0199, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0306, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 61/75, Train Loss: 0.0197, Train Acc: 0.9890, Train F1: 0.9924 Val Loss: 0.0302, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 62/75, Train Loss: 0.0199, Train Acc: 0.9885, Train F1: 0.9920 Val Loss: 0.0315, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 63/75, Train Loss: 0.0194, Train Acc: 0.9896, Train F1: 0.9927 Val Loss: 0.0294, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 64/75, Train Loss: 0.0191, Train Acc: 0.9892, Train F1: 0.9925 Val Loss: 0.0308, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 65/75, Train Loss: 0.0193, Train Acc: 0.9903, Train F1: 0.9932 Val Loss: 0.0306, Val Acc: 0.9759, Val F1: 0.9835\n",
            "Epoch: 66/75, Train Loss: 0.0192, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0294, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 67/75, Train Loss: 0.0190, Train Acc: 0.9907, Train F1: 0.9936 Val Loss: 0.0300, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 68/75, Train Loss: 0.0190, Train Acc: 0.9895, Train F1: 0.9927 Val Loss: 0.0292, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 69/75, Train Loss: 0.0189, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0295, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 70/75, Train Loss: 0.0190, Train Acc: 0.9898, Train F1: 0.9929 Val Loss: 0.0294, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 71/75, Train Loss: 0.0189, Train Acc: 0.9897, Train F1: 0.9927 Val Loss: 0.0295, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 72/75, Train Loss: 0.0187, Train Acc: 0.9907, Train F1: 0.9936 Val Loss: 0.0293, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 73/75, Train Loss: 0.0186, Train Acc: 0.9906, Train F1: 0.9935 Val Loss: 0.0294, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 74/75, Train Loss: 0.0186, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0293, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 75/75, Train Loss: 0.0186, Train Acc: 0.9905, Train F1: 0.9933 Val Loss: 0.0293, Val Acc: 0.9777, Val F1: 0.9846\n",
            "\n",
            " 🔎 search 31 : deep_rescnn --- lr : 0.0012742905281420724, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv5.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1224, Train Acc: 0.8629, Train F1: 0.9064 Val Loss: 0.0976, Val Acc: 0.9089, Val F1: 0.9364\n",
            "Epoch: 2/75, Train Loss: 0.0833, Train Acc: 0.9174, Train F1: 0.9424 Val Loss: 0.0761, Val Acc: 0.9313, Val F1: 0.9521\n",
            "Epoch: 3/75, Train Loss: 0.0709, Train Acc: 0.9310, Train F1: 0.9521 Val Loss: 0.0672, Val Acc: 0.9392, Val F1: 0.9582\n",
            "Epoch: 4/75, Train Loss: 0.0595, Train Acc: 0.9428, Train F1: 0.9602 Val Loss: 0.1070, Val Acc: 0.8759, Val F1: 0.9208\n",
            "Epoch: 5/75, Train Loss: 0.0576, Train Acc: 0.9428, Train F1: 0.9604 Val Loss: 0.0657, Val Acc: 0.9405, Val F1: 0.9600\n",
            "Epoch: 6/75, Train Loss: 0.0540, Train Acc: 0.9495, Train F1: 0.9650 Val Loss: 0.0595, Val Acc: 0.9471, Val F1: 0.9643\n",
            "Epoch: 7/75, Train Loss: 0.0519, Train Acc: 0.9501, Train F1: 0.9651 Val Loss: 0.0603, Val Acc: 0.9378, Val F1: 0.9557\n",
            "Epoch: 8/75, Train Loss: 0.0469, Train Acc: 0.9591, Train F1: 0.9716 Val Loss: 0.0594, Val Acc: 0.9371, Val F1: 0.9556\n",
            "Epoch: 9/75, Train Loss: 0.0429, Train Acc: 0.9641, Train F1: 0.9750 Val Loss: 0.0491, Val Acc: 0.9560, Val F1: 0.9692\n",
            "Epoch: 10/75, Train Loss: 0.0401, Train Acc: 0.9666, Train F1: 0.9764 Val Loss: 0.0608, Val Acc: 0.9323, Val F1: 0.9515\n",
            "Epoch: 11/75, Train Loss: 0.0403, Train Acc: 0.9671, Train F1: 0.9772 Val Loss: 0.0496, Val Acc: 0.9522, Val F1: 0.9664\n",
            "Epoch: 12/75, Train Loss: 0.0372, Train Acc: 0.9678, Train F1: 0.9775 Val Loss: 0.0419, Val Acc: 0.9677, Val F1: 0.9777\n",
            "Epoch: 13/75, Train Loss: 0.0349, Train Acc: 0.9733, Train F1: 0.9816 Val Loss: 0.0403, Val Acc: 0.9704, Val F1: 0.9797\n",
            "Epoch: 14/75, Train Loss: 0.0347, Train Acc: 0.9731, Train F1: 0.9815 Val Loss: 0.0395, Val Acc: 0.9677, Val F1: 0.9776\n",
            "Epoch: 15/75, Train Loss: 0.0395, Train Acc: 0.9627, Train F1: 0.9741 Val Loss: 0.0503, Val Acc: 0.9491, Val F1: 0.9639\n",
            "Epoch: 16/75, Train Loss: 0.0356, Train Acc: 0.9714, Train F1: 0.9802 Val Loss: 0.0390, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 17/75, Train Loss: 0.0297, Train Acc: 0.9775, Train F1: 0.9843 Val Loss: 0.0373, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 18/75, Train Loss: 0.0298, Train Acc: 0.9786, Train F1: 0.9851 Val Loss: 0.0535, Val Acc: 0.9512, Val F1: 0.9672\n",
            "Epoch: 19/75, Train Loss: 0.0357, Train Acc: 0.9677, Train F1: 0.9775 Val Loss: 0.0586, Val Acc: 0.9320, Val F1: 0.9510\n",
            "Epoch: 20/75, Train Loss: 0.0317, Train Acc: 0.9762, Train F1: 0.9833 Val Loss: 0.0359, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 21/75, Train Loss: 0.0306, Train Acc: 0.9761, Train F1: 0.9834 Val Loss: 0.0365, Val Acc: 0.9729, Val F1: 0.9811\n",
            "Epoch: 22/75, Train Loss: 0.0284, Train Acc: 0.9801, Train F1: 0.9861 Val Loss: 0.0377, Val Acc: 0.9729, Val F1: 0.9815\n",
            "Epoch: 23/75, Train Loss: 0.0269, Train Acc: 0.9808, Train F1: 0.9866 Val Loss: 0.0365, Val Acc: 0.9684, Val F1: 0.9779\n",
            "Epoch: 24/75, Train Loss: 0.0271, Train Acc: 0.9805, Train F1: 0.9864 Val Loss: 0.0384, Val Acc: 0.9687, Val F1: 0.9787\n",
            "Epoch: 25/75, Train Loss: 0.0272, Train Acc: 0.9802, Train F1: 0.9862 Val Loss: 0.0336, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 26/75, Train Loss: 0.0250, Train Acc: 0.9840, Train F1: 0.9889 Val Loss: 0.0342, Val Acc: 0.9749, Val F1: 0.9828\n",
            "Epoch: 27/75, Train Loss: 0.0265, Train Acc: 0.9812, Train F1: 0.9869 Val Loss: 0.0393, Val Acc: 0.9577, Val F1: 0.9702\n",
            "Epoch: 28/75, Train Loss: 0.0273, Train Acc: 0.9785, Train F1: 0.9851 Val Loss: 0.0383, Val Acc: 0.9698, Val F1: 0.9794\n",
            "Epoch: 29/75, Train Loss: 0.0315, Train Acc: 0.9725, Train F1: 0.9810 Val Loss: 0.0387, Val Acc: 0.9687, Val F1: 0.9787\n",
            "Epoch: 30/75, Train Loss: 0.0261, Train Acc: 0.9816, Train F1: 0.9872 Val Loss: 0.0318, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 31/75, Train Loss: 0.0246, Train Acc: 0.9836, Train F1: 0.9885 Val Loss: 0.0337, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 32/75, Train Loss: 0.0241, Train Acc: 0.9849, Train F1: 0.9895 Val Loss: 0.0322, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 33/75, Train Loss: 0.0233, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0319, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 34/75, Train Loss: 0.0243, Train Acc: 0.9833, Train F1: 0.9884 Val Loss: 0.0332, Val Acc: 0.9770, Val F1: 0.9842\n",
            "Epoch: 35/75, Train Loss: 0.0233, Train Acc: 0.9852, Train F1: 0.9897 Val Loss: 0.0317, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 36/75, Train Loss: 0.0229, Train Acc: 0.9847, Train F1: 0.9894 Val Loss: 0.0345, Val Acc: 0.9698, Val F1: 0.9788\n",
            "Epoch: 37/75, Train Loss: 0.0235, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0316, Val Acc: 0.9746, Val F1: 0.9823\n",
            "Epoch: 38/75, Train Loss: 0.0240, Train Acc: 0.9836, Train F1: 0.9886 Val Loss: 0.0338, Val Acc: 0.9759, Val F1: 0.9835\n",
            "Epoch: 39/75, Train Loss: 0.0254, Train Acc: 0.9814, Train F1: 0.9871 Val Loss: 0.0345, Val Acc: 0.9732, Val F1: 0.9817\n",
            "Epoch: 40/75, Train Loss: 0.0229, Train Acc: 0.9845, Train F1: 0.9893 Val Loss: 0.0304, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 41/75, Train Loss: 0.0248, Train Acc: 0.9821, Train F1: 0.9875 Val Loss: 0.0411, Val Acc: 0.9660, Val F1: 0.9769\n",
            "Epoch: 42/75, Train Loss: 0.0255, Train Acc: 0.9817, Train F1: 0.9873 Val Loss: 0.0308, Val Acc: 0.9780, Val F1: 0.9849\n",
            "Epoch: 43/75, Train Loss: 0.0217, Train Acc: 0.9873, Train F1: 0.9912 Val Loss: 0.0329, Val Acc: 0.9759, Val F1: 0.9835\n",
            "Epoch: 44/75, Train Loss: 0.0239, Train Acc: 0.9828, Train F1: 0.9881 Val Loss: 0.0353, Val Acc: 0.9742, Val F1: 0.9824\n",
            "Epoch: 45/75, Train Loss: 0.0243, Train Acc: 0.9829, Train F1: 0.9880 Val Loss: 0.0303, Val Acc: 0.9797, Val F1: 0.9861\n",
            "Epoch: 46/75, Train Loss: 0.0216, Train Acc: 0.9877, Train F1: 0.9915 Val Loss: 0.0304, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 47/75, Train Loss: 0.0216, Train Acc: 0.9868, Train F1: 0.9908 Val Loss: 0.0293, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 48/75, Train Loss: 0.0212, Train Acc: 0.9872, Train F1: 0.9911 Val Loss: 0.0296, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 49/75, Train Loss: 0.0206, Train Acc: 0.9876, Train F1: 0.9913 Val Loss: 0.0292, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 50/75, Train Loss: 0.0205, Train Acc: 0.9872, Train F1: 0.9911 Val Loss: 0.0297, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 51/75, Train Loss: 0.0208, Train Acc: 0.9882, Train F1: 0.9918 Val Loss: 0.0319, Val Acc: 0.9753, Val F1: 0.9828\n",
            "Epoch: 52/75, Train Loss: 0.0205, Train Acc: 0.9885, Train F1: 0.9921 Val Loss: 0.0302, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 53/75, Train Loss: 0.0210, Train Acc: 0.9881, Train F1: 0.9918 Val Loss: 0.0301, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 54/75, Train Loss: 0.0199, Train Acc: 0.9889, Train F1: 0.9923 Val Loss: 0.0304, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 55/75, Train Loss: 0.0204, Train Acc: 0.9877, Train F1: 0.9915 Val Loss: 0.0300, Val Acc: 0.9797, Val F1: 0.9861\n",
            "Epoch: 56/75, Train Loss: 0.0200, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0303, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 57/75, Train Loss: 0.0200, Train Acc: 0.9891, Train F1: 0.9924 Val Loss: 0.0326, Val Acc: 0.9735, Val F1: 0.9815\n",
            "Epoch: 58/75, Train Loss: 0.0202, Train Acc: 0.9884, Train F1: 0.9920 Val Loss: 0.0310, Val Acc: 0.9770, Val F1: 0.9840\n",
            "Epoch: 59/75, Train Loss: 0.0197, Train Acc: 0.9897, Train F1: 0.9928 Val Loss: 0.0293, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 60/75, Train Loss: 0.0203, Train Acc: 0.9873, Train F1: 0.9912 Val Loss: 0.0292, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 61/75, Train Loss: 0.0195, Train Acc: 0.9892, Train F1: 0.9925 Val Loss: 0.0293, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 62/75, Train Loss: 0.0195, Train Acc: 0.9890, Train F1: 0.9924 Val Loss: 0.0293, Val Acc: 0.9814, Val F1: 0.9872\n",
            "Epoch: 63/75, Train Loss: 0.0195, Train Acc: 0.9895, Train F1: 0.9927 Val Loss: 0.0293, Val Acc: 0.9780, Val F1: 0.9848\n",
            "Epoch: 64/75, Train Loss: 0.0197, Train Acc: 0.9892, Train F1: 0.9926 Val Loss: 0.0292, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 65/75, Train Loss: 0.0193, Train Acc: 0.9896, Train F1: 0.9927 Val Loss: 0.0290, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 66/75, Train Loss: 0.0194, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0292, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 67/75, Train Loss: 0.0190, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0288, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 68/75, Train Loss: 0.0191, Train Acc: 0.9903, Train F1: 0.9933 Val Loss: 0.0296, Val Acc: 0.9814, Val F1: 0.9872\n",
            "Epoch: 69/75, Train Loss: 0.0191, Train Acc: 0.9903, Train F1: 0.9932 Val Loss: 0.0289, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 70/75, Train Loss: 0.0189, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0289, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 71/75, Train Loss: 0.0188, Train Acc: 0.9900, Train F1: 0.9931 Val Loss: 0.0289, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 72/75, Train Loss: 0.0188, Train Acc: 0.9905, Train F1: 0.9934 Val Loss: 0.0289, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 73/75, Train Loss: 0.0188, Train Acc: 0.9901, Train F1: 0.9932 Val Loss: 0.0288, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 74/75, Train Loss: 0.0188, Train Acc: 0.9904, Train F1: 0.9934 Val Loss: 0.0288, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 75/75, Train Loss: 0.0188, Train Acc: 0.9903, Train F1: 0.9932 Val Loss: 0.0288, Val Acc: 0.9797, Val F1: 0.9860\n",
            "\n",
            " 🔎 search 32 : deep_rescnn --- lr : 0.0023290654300914477, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv2.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1290, Train Acc: 0.8588, Train F1: 0.9031 Val Loss: 0.1315, Val Acc: 0.8330, Val F1: 0.8939\n",
            "Epoch: 2/75, Train Loss: 0.1087, Train Acc: 0.8915, Train F1: 0.9244 Val Loss: 0.1037, Val Acc: 0.9014, Val F1: 0.9329\n",
            "Epoch: 3/75, Train Loss: 0.1035, Train Acc: 0.8964, Train F1: 0.9278 Val Loss: 0.1004, Val Acc: 0.9100, Val F1: 0.9384\n",
            "Epoch: 4/75, Train Loss: 0.0964, Train Acc: 0.9076, Train F1: 0.9359 Val Loss: 0.1200, Val Acc: 0.8674, Val F1: 0.9152\n",
            "Epoch: 5/75, Train Loss: 0.0944, Train Acc: 0.9076, Train F1: 0.9359 Val Loss: 0.0990, Val Acc: 0.9007, Val F1: 0.9306\n",
            "Epoch: 6/75, Train Loss: 0.0921, Train Acc: 0.9116, Train F1: 0.9384 Val Loss: 0.0898, Val Acc: 0.9186, Val F1: 0.9447\n",
            "Epoch: 7/75, Train Loss: 0.0865, Train Acc: 0.9208, Train F1: 0.9448 Val Loss: 0.1027, Val Acc: 0.9021, Val F1: 0.9310\n",
            "Epoch: 8/75, Train Loss: 0.0894, Train Acc: 0.9150, Train F1: 0.9408 Val Loss: 0.0868, Val Acc: 0.9213, Val F1: 0.9452\n",
            "Epoch: 9/75, Train Loss: 0.0851, Train Acc: 0.9204, Train F1: 0.9448 Val Loss: 0.0854, Val Acc: 0.9186, Val F1: 0.9424\n",
            "Epoch: 10/75, Train Loss: 0.0785, Train Acc: 0.9271, Train F1: 0.9491 Val Loss: 0.0809, Val Acc: 0.9271, Val F1: 0.9491\n",
            "Epoch: 11/75, Train Loss: 0.0795, Train Acc: 0.9277, Train F1: 0.9495 Val Loss: 0.0932, Val Acc: 0.9182, Val F1: 0.9453\n",
            "Epoch: 12/75, Train Loss: 0.0764, Train Acc: 0.9289, Train F1: 0.9506 Val Loss: 0.0806, Val Acc: 0.9296, Val F1: 0.9523\n",
            "Epoch: 13/75, Train Loss: 0.0774, Train Acc: 0.9291, Train F1: 0.9504 Val Loss: 0.0827, Val Acc: 0.9210, Val F1: 0.9443\n",
            "Epoch: 14/75, Train Loss: 0.0748, Train Acc: 0.9320, Train F1: 0.9523 Val Loss: 0.0757, Val Acc: 0.9347, Val F1: 0.9546\n",
            "Epoch: 15/75, Train Loss: 0.0789, Train Acc: 0.9254, Train F1: 0.9484 Val Loss: 0.0840, Val Acc: 0.9199, Val F1: 0.9434\n",
            "Epoch: 16/75, Train Loss: 0.0818, Train Acc: 0.9230, Train F1: 0.9463 Val Loss: 0.0774, Val Acc: 0.9265, Val F1: 0.9492\n",
            "Epoch: 17/75, Train Loss: 0.0731, Train Acc: 0.9332, Train F1: 0.9535 Val Loss: 0.0747, Val Acc: 0.9347, Val F1: 0.9551\n",
            "Epoch: 18/75, Train Loss: 0.0729, Train Acc: 0.9355, Train F1: 0.9550 Val Loss: 0.0757, Val Acc: 0.9340, Val F1: 0.9545\n",
            "Epoch: 19/75, Train Loss: 0.0711, Train Acc: 0.9365, Train F1: 0.9561 Val Loss: 0.1018, Val Acc: 0.8942, Val F1: 0.9313\n",
            "Epoch: 20/75, Train Loss: 0.0704, Train Acc: 0.9365, Train F1: 0.9560 Val Loss: 0.0755, Val Acc: 0.9296, Val F1: 0.9508\n",
            "Epoch: 21/75, Train Loss: 0.0705, Train Acc: 0.9349, Train F1: 0.9545 Val Loss: 0.0738, Val Acc: 0.9371, Val F1: 0.9573\n",
            "Epoch: 22/75, Train Loss: 0.0683, Train Acc: 0.9369, Train F1: 0.9562 Val Loss: 0.0760, Val Acc: 0.9268, Val F1: 0.9491\n",
            "Epoch: 23/75, Train Loss: 0.0714, Train Acc: 0.9340, Train F1: 0.9541 Val Loss: 0.0701, Val Acc: 0.9368, Val F1: 0.9563\n",
            "Epoch: 24/75, Train Loss: 0.0694, Train Acc: 0.9340, Train F1: 0.9541 Val Loss: 0.0877, Val Acc: 0.9210, Val F1: 0.9475\n",
            "Epoch: 25/75, Train Loss: 0.0681, Train Acc: 0.9375, Train F1: 0.9564 Val Loss: 0.0751, Val Acc: 0.9265, Val F1: 0.9487\n",
            "Epoch: 26/75, Train Loss: 0.0674, Train Acc: 0.9371, Train F1: 0.9564 Val Loss: 0.0755, Val Acc: 0.9296, Val F1: 0.9505\n",
            "Epoch: 27/75, Train Loss: 0.0667, Train Acc: 0.9383, Train F1: 0.9570 Val Loss: 0.0861, Val Acc: 0.9052, Val F1: 0.9321\n",
            "Epoch: 28/75, Train Loss: 0.0659, Train Acc: 0.9401, Train F1: 0.9584 Val Loss: 0.0765, Val Acc: 0.9258, Val F1: 0.9483\n",
            "Epoch: 29/75, Train Loss: 0.0644, Train Acc: 0.9392, Train F1: 0.9576 Val Loss: 0.0712, Val Acc: 0.9344, Val F1: 0.9546\n",
            "Epoch: 30/75, Train Loss: 0.0653, Train Acc: 0.9415, Train F1: 0.9593 Val Loss: 0.0696, Val Acc: 0.9399, Val F1: 0.9589\n",
            "Epoch: 31/75, Train Loss: 0.0660, Train Acc: 0.9391, Train F1: 0.9575 Val Loss: 0.0764, Val Acc: 0.9241, Val F1: 0.9481\n",
            "Epoch: 32/75, Train Loss: 0.0639, Train Acc: 0.9425, Train F1: 0.9597 Val Loss: 0.0701, Val Acc: 0.9388, Val F1: 0.9585\n",
            "Epoch: 33/75, Train Loss: 0.0634, Train Acc: 0.9417, Train F1: 0.9595 Val Loss: 0.0689, Val Acc: 0.9378, Val F1: 0.9577\n",
            "Epoch: 34/75, Train Loss: 0.0634, Train Acc: 0.9395, Train F1: 0.9579 Val Loss: 0.0671, Val Acc: 0.9405, Val F1: 0.9592\n",
            "Epoch: 35/75, Train Loss: 0.0622, Train Acc: 0.9408, Train F1: 0.9586 Val Loss: 0.0673, Val Acc: 0.9371, Val F1: 0.9565\n",
            "Epoch: 36/75, Train Loss: 0.0605, Train Acc: 0.9451, Train F1: 0.9621 Val Loss: 0.0668, Val Acc: 0.9361, Val F1: 0.9560\n",
            "Epoch: 37/75, Train Loss: 0.0619, Train Acc: 0.9414, Train F1: 0.9590 Val Loss: 0.0690, Val Acc: 0.9354, Val F1: 0.9561\n",
            "Epoch: 38/75, Train Loss: 0.0604, Train Acc: 0.9441, Train F1: 0.9613 Val Loss: 0.0656, Val Acc: 0.9447, Val F1: 0.9623\n",
            "Epoch: 39/75, Train Loss: 0.0610, Train Acc: 0.9450, Train F1: 0.9616 Val Loss: 0.0670, Val Acc: 0.9423, Val F1: 0.9605\n",
            "Epoch: 40/75, Train Loss: 0.0598, Train Acc: 0.9462, Train F1: 0.9624 Val Loss: 0.0628, Val Acc: 0.9440, Val F1: 0.9617\n",
            "Epoch: 41/75, Train Loss: 0.0587, Train Acc: 0.9451, Train F1: 0.9617 Val Loss: 0.0649, Val Acc: 0.9430, Val F1: 0.9614\n",
            "Epoch: 42/75, Train Loss: 0.0580, Train Acc: 0.9464, Train F1: 0.9628 Val Loss: 0.0651, Val Acc: 0.9361, Val F1: 0.9558\n",
            "Epoch: 43/75, Train Loss: 0.0577, Train Acc: 0.9454, Train F1: 0.9620 Val Loss: 0.0665, Val Acc: 0.9440, Val F1: 0.9620\n",
            "Epoch: 44/75, Train Loss: 0.0557, Train Acc: 0.9485, Train F1: 0.9641 Val Loss: 0.0624, Val Acc: 0.9409, Val F1: 0.9589\n",
            "Epoch: 45/75, Train Loss: 0.0601, Train Acc: 0.9417, Train F1: 0.9593 Val Loss: 0.0639, Val Acc: 0.9378, Val F1: 0.9569\n",
            "Epoch: 46/75, Train Loss: 0.0555, Train Acc: 0.9496, Train F1: 0.9651 Val Loss: 0.0617, Val Acc: 0.9388, Val F1: 0.9575\n",
            "Epoch: 47/75, Train Loss: 0.0534, Train Acc: 0.9486, Train F1: 0.9645 Val Loss: 0.0707, Val Acc: 0.9237, Val F1: 0.9460\n",
            "Epoch: 48/75, Train Loss: 0.0540, Train Acc: 0.9488, Train F1: 0.9643 Val Loss: 0.0601, Val Acc: 0.9440, Val F1: 0.9614\n",
            "Epoch: 49/75, Train Loss: 0.0538, Train Acc: 0.9494, Train F1: 0.9645 Val Loss: 0.0649, Val Acc: 0.9402, Val F1: 0.9598\n",
            "Epoch: 50/75, Train Loss: 0.0522, Train Acc: 0.9494, Train F1: 0.9647 Val Loss: 0.0580, Val Acc: 0.9433, Val F1: 0.9609\n",
            "Epoch: 51/75, Train Loss: 0.0505, Train Acc: 0.9533, Train F1: 0.9674 Val Loss: 0.0584, Val Acc: 0.9419, Val F1: 0.9597\n",
            "Epoch: 52/75, Train Loss: 0.0503, Train Acc: 0.9540, Train F1: 0.9680 Val Loss: 0.0582, Val Acc: 0.9402, Val F1: 0.9587\n",
            "Epoch: 53/75, Train Loss: 0.0494, Train Acc: 0.9537, Train F1: 0.9679 Val Loss: 0.0564, Val Acc: 0.9495, Val F1: 0.9656\n",
            "Epoch: 54/75, Train Loss: 0.0517, Train Acc: 0.9489, Train F1: 0.9643 Val Loss: 0.0592, Val Acc: 0.9474, Val F1: 0.9641\n",
            "Epoch: 55/75, Train Loss: 0.0488, Train Acc: 0.9526, Train F1: 0.9669 Val Loss: 0.0551, Val Acc: 0.9485, Val F1: 0.9645\n",
            "Epoch: 56/75, Train Loss: 0.0489, Train Acc: 0.9510, Train F1: 0.9658 Val Loss: 0.0556, Val Acc: 0.9505, Val F1: 0.9660\n",
            "Epoch: 57/75, Train Loss: 0.0480, Train Acc: 0.9541, Train F1: 0.9682 Val Loss: 0.0573, Val Acc: 0.9495, Val F1: 0.9657\n",
            "Epoch: 58/75, Train Loss: 0.0470, Train Acc: 0.9536, Train F1: 0.9676 Val Loss: 0.0543, Val Acc: 0.9481, Val F1: 0.9643\n",
            "Epoch: 59/75, Train Loss: 0.0462, Train Acc: 0.9566, Train F1: 0.9700 Val Loss: 0.0601, Val Acc: 0.9454, Val F1: 0.9631\n",
            "Epoch: 60/75, Train Loss: 0.0466, Train Acc: 0.9548, Train F1: 0.9687 Val Loss: 0.0551, Val Acc: 0.9522, Val F1: 0.9674\n",
            "Epoch: 61/75, Train Loss: 0.0456, Train Acc: 0.9550, Train F1: 0.9687 Val Loss: 0.0555, Val Acc: 0.9467, Val F1: 0.9630\n",
            "Epoch: 62/75, Train Loss: 0.0459, Train Acc: 0.9560, Train F1: 0.9695 Val Loss: 0.0539, Val Acc: 0.9491, Val F1: 0.9648\n",
            "Epoch: 63/75, Train Loss: 0.0453, Train Acc: 0.9549, Train F1: 0.9686 Val Loss: 0.0534, Val Acc: 0.9522, Val F1: 0.9675\n",
            "Epoch: 64/75, Train Loss: 0.0450, Train Acc: 0.9545, Train F1: 0.9682 Val Loss: 0.0525, Val Acc: 0.9522, Val F1: 0.9673\n",
            "Epoch: 65/75, Train Loss: 0.0445, Train Acc: 0.9560, Train F1: 0.9693 Val Loss: 0.0528, Val Acc: 0.9515, Val F1: 0.9668\n",
            "Epoch: 66/75, Train Loss: 0.0439, Train Acc: 0.9572, Train F1: 0.9700 Val Loss: 0.0527, Val Acc: 0.9550, Val F1: 0.9693\n",
            "Epoch: 67/75, Train Loss: 0.0441, Train Acc: 0.9573, Train F1: 0.9703 Val Loss: 0.0525, Val Acc: 0.9529, Val F1: 0.9676\n",
            "Epoch: 68/75, Train Loss: 0.0436, Train Acc: 0.9569, Train F1: 0.9698 Val Loss: 0.0523, Val Acc: 0.9540, Val F1: 0.9685\n",
            "Epoch: 69/75, Train Loss: 0.0435, Train Acc: 0.9582, Train F1: 0.9709 Val Loss: 0.0527, Val Acc: 0.9498, Val F1: 0.9653\n",
            "Epoch: 70/75, Train Loss: 0.0435, Train Acc: 0.9580, Train F1: 0.9707 Val Loss: 0.0521, Val Acc: 0.9546, Val F1: 0.9689\n",
            "Epoch: 71/75, Train Loss: 0.0433, Train Acc: 0.9580, Train F1: 0.9706 Val Loss: 0.0520, Val Acc: 0.9553, Val F1: 0.9695\n",
            "Epoch: 72/75, Train Loss: 0.0431, Train Acc: 0.9575, Train F1: 0.9704 Val Loss: 0.0521, Val Acc: 0.9567, Val F1: 0.9704\n",
            "Epoch: 73/75, Train Loss: 0.0430, Train Acc: 0.9578, Train F1: 0.9708 Val Loss: 0.0518, Val Acc: 0.9553, Val F1: 0.9694\n",
            "Epoch: 74/75, Train Loss: 0.0429, Train Acc: 0.9580, Train F1: 0.9709 Val Loss: 0.0518, Val Acc: 0.9546, Val F1: 0.9689\n",
            "Epoch: 75/75, Train Loss: 0.0429, Train Acc: 0.9580, Train F1: 0.9707 Val Loss: 0.0518, Val Acc: 0.9553, Val F1: 0.9694\n",
            "\n",
            " 🔎 search 33 : deep_rescnn --- lr : 0.0023290654300914477, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv3.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1205, Train Acc: 0.8706, Train F1: 0.9114 Val Loss: 0.0980, Val Acc: 0.9151, Val F1: 0.9420\n",
            "Epoch: 2/75, Train Loss: 0.0970, Train Acc: 0.9027, Train F1: 0.9323 Val Loss: 0.0971, Val Acc: 0.9110, Val F1: 0.9398\n",
            "Epoch: 3/75, Train Loss: 0.0883, Train Acc: 0.9118, Train F1: 0.9387 Val Loss: 0.0970, Val Acc: 0.9192, Val F1: 0.9447\n",
            "Epoch: 4/75, Train Loss: 0.0771, Train Acc: 0.9258, Train F1: 0.9484 Val Loss: 0.1133, Val Acc: 0.8636, Val F1: 0.9128\n",
            "Epoch: 5/75, Train Loss: 0.0763, Train Acc: 0.9269, Train F1: 0.9493 Val Loss: 0.0866, Val Acc: 0.9165, Val F1: 0.9406\n",
            "Epoch: 6/75, Train Loss: 0.0735, Train Acc: 0.9292, Train F1: 0.9505 Val Loss: 0.0795, Val Acc: 0.9309, Val F1: 0.9515\n",
            "Epoch: 7/75, Train Loss: 0.0683, Train Acc: 0.9324, Train F1: 0.9529 Val Loss: 0.1002, Val Acc: 0.8904, Val F1: 0.9203\n",
            "Epoch: 8/75, Train Loss: 0.0704, Train Acc: 0.9316, Train F1: 0.9526 Val Loss: 0.0766, Val Acc: 0.9282, Val F1: 0.9517\n",
            "Epoch: 9/75, Train Loss: 0.0676, Train Acc: 0.9347, Train F1: 0.9546 Val Loss: 0.0689, Val Acc: 0.9409, Val F1: 0.9600\n",
            "Epoch: 10/75, Train Loss: 0.0646, Train Acc: 0.9375, Train F1: 0.9565 Val Loss: 0.0766, Val Acc: 0.9144, Val F1: 0.9393\n",
            "Epoch: 11/75, Train Loss: 0.0669, Train Acc: 0.9339, Train F1: 0.9537 Val Loss: 0.0663, Val Acc: 0.9357, Val F1: 0.9554\n",
            "Epoch: 12/75, Train Loss: 0.0592, Train Acc: 0.9441, Train F1: 0.9613 Val Loss: 0.0722, Val Acc: 0.9230, Val F1: 0.9456\n",
            "Epoch: 13/75, Train Loss: 0.0579, Train Acc: 0.9442, Train F1: 0.9613 Val Loss: 0.0649, Val Acc: 0.9460, Val F1: 0.9636\n",
            "Epoch: 14/75, Train Loss: 0.0573, Train Acc: 0.9434, Train F1: 0.9606 Val Loss: 0.0687, Val Acc: 0.9223, Val F1: 0.9448\n",
            "Epoch: 15/75, Train Loss: 0.0554, Train Acc: 0.9452, Train F1: 0.9617 Val Loss: 0.0652, Val Acc: 0.9371, Val F1: 0.9563\n",
            "Epoch: 16/75, Train Loss: 0.0595, Train Acc: 0.9415, Train F1: 0.9595 Val Loss: 0.0598, Val Acc: 0.9357, Val F1: 0.9550\n",
            "Epoch: 17/75, Train Loss: 0.0538, Train Acc: 0.9468, Train F1: 0.9630 Val Loss: 0.0571, Val Acc: 0.9485, Val F1: 0.9644\n",
            "Epoch: 18/75, Train Loss: 0.0580, Train Acc: 0.9422, Train F1: 0.9597 Val Loss: 0.0594, Val Acc: 0.9433, Val F1: 0.9608\n",
            "Epoch: 19/75, Train Loss: 0.0494, Train Acc: 0.9528, Train F1: 0.9671 Val Loss: 0.0681, Val Acc: 0.9296, Val F1: 0.9534\n",
            "Epoch: 20/75, Train Loss: 0.0487, Train Acc: 0.9532, Train F1: 0.9676 Val Loss: 0.0609, Val Acc: 0.9426, Val F1: 0.9601\n",
            "Epoch: 21/75, Train Loss: 0.0565, Train Acc: 0.9434, Train F1: 0.9606 Val Loss: 0.0690, Val Acc: 0.9271, Val F1: 0.9491\n",
            "Epoch: 22/75, Train Loss: 0.0504, Train Acc: 0.9525, Train F1: 0.9667 Val Loss: 0.0576, Val Acc: 0.9457, Val F1: 0.9624\n",
            "Epoch: 23/75, Train Loss: 0.0505, Train Acc: 0.9498, Train F1: 0.9650 Val Loss: 0.0520, Val Acc: 0.9529, Val F1: 0.9675\n",
            "Epoch: 24/75, Train Loss: 0.0464, Train Acc: 0.9561, Train F1: 0.9694 Val Loss: 0.0539, Val Acc: 0.9536, Val F1: 0.9687\n",
            "Epoch: 25/75, Train Loss: 0.0480, Train Acc: 0.9545, Train F1: 0.9682 Val Loss: 0.0517, Val Acc: 0.9533, Val F1: 0.9681\n",
            "Epoch: 26/75, Train Loss: 0.0475, Train Acc: 0.9550, Train F1: 0.9687 Val Loss: 0.0536, Val Acc: 0.9522, Val F1: 0.9670\n",
            "Epoch: 27/75, Train Loss: 0.0475, Train Acc: 0.9521, Train F1: 0.9666 Val Loss: 0.0620, Val Acc: 0.9478, Val F1: 0.9642\n",
            "Epoch: 28/75, Train Loss: 0.0462, Train Acc: 0.9585, Train F1: 0.9710 Val Loss: 0.0512, Val Acc: 0.9560, Val F1: 0.9695\n",
            "Epoch: 29/75, Train Loss: 0.0451, Train Acc: 0.9574, Train F1: 0.9700 Val Loss: 0.0566, Val Acc: 0.9509, Val F1: 0.9670\n",
            "Epoch: 30/75, Train Loss: 0.0460, Train Acc: 0.9554, Train F1: 0.9691 Val Loss: 0.0463, Val Acc: 0.9601, Val F1: 0.9727\n",
            "Epoch: 31/75, Train Loss: 0.0445, Train Acc: 0.9570, Train F1: 0.9702 Val Loss: 0.0519, Val Acc: 0.9443, Val F1: 0.9611\n",
            "Epoch: 32/75, Train Loss: 0.0451, Train Acc: 0.9583, Train F1: 0.9708 Val Loss: 0.0520, Val Acc: 0.9522, Val F1: 0.9670\n",
            "Epoch: 33/75, Train Loss: 0.0458, Train Acc: 0.9552, Train F1: 0.9687 Val Loss: 0.0516, Val Acc: 0.9546, Val F1: 0.9691\n",
            "Epoch: 34/75, Train Loss: 0.0448, Train Acc: 0.9561, Train F1: 0.9694 Val Loss: 0.0543, Val Acc: 0.9457, Val F1: 0.9632\n",
            "Epoch: 35/75, Train Loss: 0.0452, Train Acc: 0.9552, Train F1: 0.9689 Val Loss: 0.0486, Val Acc: 0.9598, Val F1: 0.9726\n",
            "Epoch: 36/75, Train Loss: 0.0454, Train Acc: 0.9576, Train F1: 0.9705 Val Loss: 0.0508, Val Acc: 0.9564, Val F1: 0.9700\n",
            "Epoch: 37/75, Train Loss: 0.0418, Train Acc: 0.9625, Train F1: 0.9741 Val Loss: 0.0475, Val Acc: 0.9543, Val F1: 0.9686\n",
            "Epoch: 38/75, Train Loss: 0.0414, Train Acc: 0.9609, Train F1: 0.9729 Val Loss: 0.0459, Val Acc: 0.9615, Val F1: 0.9737\n",
            "Epoch: 39/75, Train Loss: 0.0392, Train Acc: 0.9632, Train F1: 0.9746 Val Loss: 0.0642, Val Acc: 0.9344, Val F1: 0.9530\n",
            "Epoch: 40/75, Train Loss: 0.0388, Train Acc: 0.9658, Train F1: 0.9761 Val Loss: 0.0435, Val Acc: 0.9643, Val F1: 0.9755\n",
            "Epoch: 41/75, Train Loss: 0.0413, Train Acc: 0.9617, Train F1: 0.9732 Val Loss: 0.0422, Val Acc: 0.9646, Val F1: 0.9757\n",
            "Epoch: 42/75, Train Loss: 0.0374, Train Acc: 0.9676, Train F1: 0.9774 Val Loss: 0.0441, Val Acc: 0.9649, Val F1: 0.9756\n",
            "Epoch: 43/75, Train Loss: 0.0367, Train Acc: 0.9688, Train F1: 0.9783 Val Loss: 0.0509, Val Acc: 0.9512, Val F1: 0.9671\n",
            "Epoch: 44/75, Train Loss: 0.0385, Train Acc: 0.9648, Train F1: 0.9753 Val Loss: 0.0440, Val Acc: 0.9595, Val F1: 0.9720\n",
            "Epoch: 45/75, Train Loss: 0.0367, Train Acc: 0.9682, Train F1: 0.9778 Val Loss: 0.0438, Val Acc: 0.9612, Val F1: 0.9734\n",
            "Epoch: 46/75, Train Loss: 0.0370, Train Acc: 0.9655, Train F1: 0.9761 Val Loss: 0.0440, Val Acc: 0.9646, Val F1: 0.9753\n",
            "Epoch: 47/75, Train Loss: 0.0353, Train Acc: 0.9704, Train F1: 0.9795 Val Loss: 0.0462, Val Acc: 0.9625, Val F1: 0.9745\n",
            "Epoch: 48/75, Train Loss: 0.0352, Train Acc: 0.9703, Train F1: 0.9793 Val Loss: 0.0424, Val Acc: 0.9653, Val F1: 0.9759\n",
            "Epoch: 49/75, Train Loss: 0.0359, Train Acc: 0.9706, Train F1: 0.9793 Val Loss: 0.0446, Val Acc: 0.9670, Val F1: 0.9776\n",
            "Epoch: 50/75, Train Loss: 0.0351, Train Acc: 0.9701, Train F1: 0.9792 Val Loss: 0.0441, Val Acc: 0.9670, Val F1: 0.9775\n",
            "Epoch: 51/75, Train Loss: 0.0351, Train Acc: 0.9699, Train F1: 0.9789 Val Loss: 0.0412, Val Acc: 0.9649, Val F1: 0.9756\n",
            "Epoch: 52/75, Train Loss: 0.0335, Train Acc: 0.9727, Train F1: 0.9809 Val Loss: 0.0427, Val Acc: 0.9698, Val F1: 0.9794\n",
            "Epoch: 53/75, Train Loss: 0.0333, Train Acc: 0.9747, Train F1: 0.9823 Val Loss: 0.0388, Val Acc: 0.9701, Val F1: 0.9793\n",
            "Epoch: 54/75, Train Loss: 0.0318, Train Acc: 0.9761, Train F1: 0.9831 Val Loss: 0.0399, Val Acc: 0.9643, Val F1: 0.9756\n",
            "Epoch: 55/75, Train Loss: 0.0313, Train Acc: 0.9745, Train F1: 0.9823 Val Loss: 0.0376, Val Acc: 0.9725, Val F1: 0.9810\n",
            "Epoch: 56/75, Train Loss: 0.0312, Train Acc: 0.9754, Train F1: 0.9828 Val Loss: 0.0399, Val Acc: 0.9694, Val F1: 0.9791\n",
            "Epoch: 57/75, Train Loss: 0.0312, Train Acc: 0.9767, Train F1: 0.9837 Val Loss: 0.0392, Val Acc: 0.9698, Val F1: 0.9792\n",
            "Epoch: 58/75, Train Loss: 0.0312, Train Acc: 0.9771, Train F1: 0.9841 Val Loss: 0.0392, Val Acc: 0.9704, Val F1: 0.9795\n",
            "Epoch: 59/75, Train Loss: 0.0300, Train Acc: 0.9774, Train F1: 0.9842 Val Loss: 0.0379, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 60/75, Train Loss: 0.0302, Train Acc: 0.9772, Train F1: 0.9838 Val Loss: 0.0375, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 61/75, Train Loss: 0.0294, Train Acc: 0.9785, Train F1: 0.9848 Val Loss: 0.0394, Val Acc: 0.9663, Val F1: 0.9766\n",
            "Epoch: 62/75, Train Loss: 0.0298, Train Acc: 0.9786, Train F1: 0.9850 Val Loss: 0.0373, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 63/75, Train Loss: 0.0289, Train Acc: 0.9789, Train F1: 0.9853 Val Loss: 0.0390, Val Acc: 0.9725, Val F1: 0.9812\n",
            "Epoch: 64/75, Train Loss: 0.0295, Train Acc: 0.9780, Train F1: 0.9847 Val Loss: 0.0368, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 65/75, Train Loss: 0.0289, Train Acc: 0.9796, Train F1: 0.9858 Val Loss: 0.0368, Val Acc: 0.9729, Val F1: 0.9812\n",
            "Epoch: 66/75, Train Loss: 0.0285, Train Acc: 0.9802, Train F1: 0.9861 Val Loss: 0.0370, Val Acc: 0.9708, Val F1: 0.9798\n",
            "Epoch: 67/75, Train Loss: 0.0284, Train Acc: 0.9808, Train F1: 0.9865 Val Loss: 0.0370, Val Acc: 0.9718, Val F1: 0.9805\n",
            "Epoch: 68/75, Train Loss: 0.0283, Train Acc: 0.9805, Train F1: 0.9865 Val Loss: 0.0369, Val Acc: 0.9725, Val F1: 0.9809\n",
            "Epoch: 69/75, Train Loss: 0.0281, Train Acc: 0.9802, Train F1: 0.9862 Val Loss: 0.0366, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 70/75, Train Loss: 0.0278, Train Acc: 0.9813, Train F1: 0.9870 Val Loss: 0.0364, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 71/75, Train Loss: 0.0277, Train Acc: 0.9814, Train F1: 0.9871 Val Loss: 0.0362, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 72/75, Train Loss: 0.0276, Train Acc: 0.9816, Train F1: 0.9872 Val Loss: 0.0363, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 73/75, Train Loss: 0.0276, Train Acc: 0.9817, Train F1: 0.9871 Val Loss: 0.0363, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 74/75, Train Loss: 0.0275, Train Acc: 0.9820, Train F1: 0.9875 Val Loss: 0.0363, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 75/75, Train Loss: 0.0275, Train Acc: 0.9819, Train F1: 0.9873 Val Loss: 0.0363, Val Acc: 0.9749, Val F1: 0.9827\n",
            "\n",
            " 🔎 search 34 : deep_rescnn --- lr : 0.0023290654300914477, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv4.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1465, Train Acc: 0.8251, Train F1: 0.8764 Val Loss: 0.1023, Val Acc: 0.8921, Val F1: 0.9236\n",
            "Epoch: 2/75, Train Loss: 0.0896, Train Acc: 0.9071, Train F1: 0.9350 Val Loss: 0.0770, Val Acc: 0.9326, Val F1: 0.9546\n",
            "Epoch: 3/75, Train Loss: 0.0761, Train Acc: 0.9233, Train F1: 0.9462 Val Loss: 0.0740, Val Acc: 0.9320, Val F1: 0.9533\n",
            "Epoch: 4/75, Train Loss: 0.0662, Train Acc: 0.9340, Train F1: 0.9541 Val Loss: 0.0656, Val Acc: 0.9423, Val F1: 0.9598\n",
            "Epoch: 5/75, Train Loss: 0.0646, Train Acc: 0.9359, Train F1: 0.9552 Val Loss: 0.0604, Val Acc: 0.9416, Val F1: 0.9592\n",
            "Epoch: 6/75, Train Loss: 0.0536, Train Acc: 0.9503, Train F1: 0.9652 Val Loss: 0.0637, Val Acc: 0.9306, Val F1: 0.9507\n",
            "Epoch: 7/75, Train Loss: 0.0516, Train Acc: 0.9517, Train F1: 0.9663 Val Loss: 0.0521, Val Acc: 0.9546, Val F1: 0.9685\n",
            "Epoch: 8/75, Train Loss: 0.0518, Train Acc: 0.9509, Train F1: 0.9657 Val Loss: 0.0554, Val Acc: 0.9564, Val F1: 0.9700\n",
            "Epoch: 9/75, Train Loss: 0.0497, Train Acc: 0.9546, Train F1: 0.9683 Val Loss: 0.0556, Val Acc: 0.9502, Val F1: 0.9649\n",
            "Epoch: 10/75, Train Loss: 0.0490, Train Acc: 0.9538, Train F1: 0.9678 Val Loss: 0.0468, Val Acc: 0.9639, Val F1: 0.9750\n",
            "Epoch: 11/75, Train Loss: 0.0476, Train Acc: 0.9564, Train F1: 0.9694 Val Loss: 0.0474, Val Acc: 0.9649, Val F1: 0.9759\n",
            "Epoch: 12/75, Train Loss: 0.0472, Train Acc: 0.9569, Train F1: 0.9700 Val Loss: 0.0483, Val Acc: 0.9625, Val F1: 0.9740\n",
            "Epoch: 13/75, Train Loss: 0.0422, Train Acc: 0.9616, Train F1: 0.9732 Val Loss: 0.0508, Val Acc: 0.9605, Val F1: 0.9731\n",
            "Epoch: 14/75, Train Loss: 0.0425, Train Acc: 0.9604, Train F1: 0.9722 Val Loss: 0.0529, Val Acc: 0.9625, Val F1: 0.9743\n",
            "Epoch: 15/75, Train Loss: 0.0405, Train Acc: 0.9653, Train F1: 0.9755 Val Loss: 0.0455, Val Acc: 0.9656, Val F1: 0.9761\n",
            "Epoch: 16/75, Train Loss: 0.0371, Train Acc: 0.9672, Train F1: 0.9772 Val Loss: 0.0428, Val Acc: 0.9636, Val F1: 0.9751\n",
            "Epoch: 17/75, Train Loss: 0.0382, Train Acc: 0.9645, Train F1: 0.9753 Val Loss: 0.0543, Val Acc: 0.9430, Val F1: 0.9605\n",
            "Epoch: 18/75, Train Loss: 0.0395, Train Acc: 0.9668, Train F1: 0.9767 Val Loss: 0.0450, Val Acc: 0.9560, Val F1: 0.9698\n",
            "Epoch: 19/75, Train Loss: 0.0412, Train Acc: 0.9623, Train F1: 0.9738 Val Loss: 0.0422, Val Acc: 0.9691, Val F1: 0.9787\n",
            "Epoch: 20/75, Train Loss: 0.0355, Train Acc: 0.9706, Train F1: 0.9793 Val Loss: 0.0516, Val Acc: 0.9471, Val F1: 0.9626\n",
            "Epoch: 21/75, Train Loss: 0.0368, Train Acc: 0.9695, Train F1: 0.9788 Val Loss: 0.0610, Val Acc: 0.9302, Val F1: 0.9500\n",
            "Epoch: 22/75, Train Loss: 0.0391, Train Acc: 0.9655, Train F1: 0.9758 Val Loss: 0.0442, Val Acc: 0.9643, Val F1: 0.9752\n",
            "Epoch: 23/75, Train Loss: 0.0372, Train Acc: 0.9678, Train F1: 0.9773 Val Loss: 0.0411, Val Acc: 0.9677, Val F1: 0.9777\n",
            "Epoch: 24/75, Train Loss: 0.0380, Train Acc: 0.9668, Train F1: 0.9769 Val Loss: 0.0629, Val Acc: 0.9395, Val F1: 0.9595\n",
            "Epoch: 25/75, Train Loss: 0.0371, Train Acc: 0.9686, Train F1: 0.9782 Val Loss: 0.0406, Val Acc: 0.9663, Val F1: 0.9769\n",
            "Epoch: 26/75, Train Loss: 0.0357, Train Acc: 0.9685, Train F1: 0.9781 Val Loss: 0.0390, Val Acc: 0.9680, Val F1: 0.9779\n",
            "Epoch: 27/75, Train Loss: 0.0337, Train Acc: 0.9721, Train F1: 0.9804 Val Loss: 0.0536, Val Acc: 0.9447, Val F1: 0.9608\n",
            "Epoch: 28/75, Train Loss: 0.0360, Train Acc: 0.9690, Train F1: 0.9782 Val Loss: 0.0455, Val Acc: 0.9639, Val F1: 0.9755\n",
            "Epoch: 29/75, Train Loss: 0.0309, Train Acc: 0.9759, Train F1: 0.9833 Val Loss: 0.0392, Val Acc: 0.9680, Val F1: 0.9780\n",
            "Epoch: 30/75, Train Loss: 0.0322, Train Acc: 0.9748, Train F1: 0.9823 Val Loss: 0.0394, Val Acc: 0.9674, Val F1: 0.9772\n",
            "Epoch: 31/75, Train Loss: 0.0351, Train Acc: 0.9712, Train F1: 0.9801 Val Loss: 0.0447, Val Acc: 0.9567, Val F1: 0.9706\n",
            "Epoch: 32/75, Train Loss: 0.0319, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0463, Val Acc: 0.9553, Val F1: 0.9686\n",
            "Epoch: 33/75, Train Loss: 0.0342, Train Acc: 0.9725, Train F1: 0.9808 Val Loss: 0.0377, Val Acc: 0.9701, Val F1: 0.9793\n",
            "Epoch: 34/75, Train Loss: 0.0295, Train Acc: 0.9773, Train F1: 0.9840 Val Loss: 0.0372, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 35/75, Train Loss: 0.0297, Train Acc: 0.9772, Train F1: 0.9842 Val Loss: 0.0463, Val Acc: 0.9560, Val F1: 0.9690\n",
            "Epoch: 36/75, Train Loss: 0.0294, Train Acc: 0.9777, Train F1: 0.9844 Val Loss: 0.0368, Val Acc: 0.9711, Val F1: 0.9800\n",
            "Epoch: 37/75, Train Loss: 0.0298, Train Acc: 0.9765, Train F1: 0.9835 Val Loss: 0.0350, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 38/75, Train Loss: 0.0295, Train Acc: 0.9762, Train F1: 0.9835 Val Loss: 0.0355, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 39/75, Train Loss: 0.0280, Train Acc: 0.9778, Train F1: 0.9844 Val Loss: 0.0370, Val Acc: 0.9698, Val F1: 0.9790\n",
            "Epoch: 40/75, Train Loss: 0.0276, Train Acc: 0.9796, Train F1: 0.9856 Val Loss: 0.0406, Val Acc: 0.9674, Val F1: 0.9778\n",
            "Epoch: 41/75, Train Loss: 0.0278, Train Acc: 0.9788, Train F1: 0.9851 Val Loss: 0.0383, Val Acc: 0.9684, Val F1: 0.9780\n",
            "Epoch: 42/75, Train Loss: 0.0268, Train Acc: 0.9792, Train F1: 0.9854 Val Loss: 0.0366, Val Acc: 0.9694, Val F1: 0.9790\n",
            "Epoch: 43/75, Train Loss: 0.0313, Train Acc: 0.9753, Train F1: 0.9827 Val Loss: 0.0351, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 44/75, Train Loss: 0.0259, Train Acc: 0.9816, Train F1: 0.9873 Val Loss: 0.0437, Val Acc: 0.9567, Val F1: 0.9695\n",
            "Epoch: 45/75, Train Loss: 0.0259, Train Acc: 0.9810, Train F1: 0.9867 Val Loss: 0.0343, Val Acc: 0.9722, Val F1: 0.9807\n",
            "Epoch: 46/75, Train Loss: 0.0253, Train Acc: 0.9806, Train F1: 0.9866 Val Loss: 0.0356, Val Acc: 0.9732, Val F1: 0.9814\n",
            "Epoch: 47/75, Train Loss: 0.0258, Train Acc: 0.9808, Train F1: 0.9865 Val Loss: 0.0359, Val Acc: 0.9715, Val F1: 0.9802\n",
            "Epoch: 48/75, Train Loss: 0.0274, Train Acc: 0.9790, Train F1: 0.9852 Val Loss: 0.0346, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 49/75, Train Loss: 0.0243, Train Acc: 0.9843, Train F1: 0.9891 Val Loss: 0.0339, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 50/75, Train Loss: 0.0242, Train Acc: 0.9849, Train F1: 0.9895 Val Loss: 0.0331, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 51/75, Train Loss: 0.0242, Train Acc: 0.9838, Train F1: 0.9887 Val Loss: 0.0327, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 52/75, Train Loss: 0.0240, Train Acc: 0.9849, Train F1: 0.9894 Val Loss: 0.0323, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 53/75, Train Loss: 0.0232, Train Acc: 0.9850, Train F1: 0.9895 Val Loss: 0.0330, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 54/75, Train Loss: 0.0232, Train Acc: 0.9855, Train F1: 0.9900 Val Loss: 0.0336, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 55/75, Train Loss: 0.0229, Train Acc: 0.9847, Train F1: 0.9893 Val Loss: 0.0329, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 56/75, Train Loss: 0.0223, Train Acc: 0.9864, Train F1: 0.9905 Val Loss: 0.0328, Val Acc: 0.9773, Val F1: 0.9844\n",
            "Epoch: 57/75, Train Loss: 0.0216, Train Acc: 0.9865, Train F1: 0.9905 Val Loss: 0.0332, Val Acc: 0.9753, Val F1: 0.9830\n",
            "Epoch: 58/75, Train Loss: 0.0211, Train Acc: 0.9869, Train F1: 0.9908 Val Loss: 0.0326, Val Acc: 0.9759, Val F1: 0.9833\n",
            "Epoch: 59/75, Train Loss: 0.0212, Train Acc: 0.9864, Train F1: 0.9905 Val Loss: 0.0318, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 60/75, Train Loss: 0.0209, Train Acc: 0.9872, Train F1: 0.9911 Val Loss: 0.0343, Val Acc: 0.9711, Val F1: 0.9799\n",
            "Epoch: 61/75, Train Loss: 0.0212, Train Acc: 0.9867, Train F1: 0.9908 Val Loss: 0.0308, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 62/75, Train Loss: 0.0212, Train Acc: 0.9872, Train F1: 0.9911 Val Loss: 0.0366, Val Acc: 0.9677, Val F1: 0.9774\n",
            "Epoch: 63/75, Train Loss: 0.0207, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0320, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 64/75, Train Loss: 0.0202, Train Acc: 0.9888, Train F1: 0.9921 Val Loss: 0.0311, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 65/75, Train Loss: 0.0201, Train Acc: 0.9879, Train F1: 0.9915 Val Loss: 0.0368, Val Acc: 0.9677, Val F1: 0.9780\n",
            "Epoch: 66/75, Train Loss: 0.0204, Train Acc: 0.9884, Train F1: 0.9919 Val Loss: 0.0306, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 67/75, Train Loss: 0.0198, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0328, Val Acc: 0.9722, Val F1: 0.9810\n",
            "Epoch: 68/75, Train Loss: 0.0198, Train Acc: 0.9875, Train F1: 0.9912 Val Loss: 0.0303, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 69/75, Train Loss: 0.0195, Train Acc: 0.9887, Train F1: 0.9921 Val Loss: 0.0308, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 70/75, Train Loss: 0.0195, Train Acc: 0.9891, Train F1: 0.9924 Val Loss: 0.0307, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 71/75, Train Loss: 0.0193, Train Acc: 0.9891, Train F1: 0.9924 Val Loss: 0.0307, Val Acc: 0.9794, Val F1: 0.9858\n",
            "Epoch: 72/75, Train Loss: 0.0193, Train Acc: 0.9895, Train F1: 0.9927 Val Loss: 0.0307, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 73/75, Train Loss: 0.0192, Train Acc: 0.9892, Train F1: 0.9924 Val Loss: 0.0307, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 74/75, Train Loss: 0.0192, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0306, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 75/75, Train Loss: 0.0191, Train Acc: 0.9896, Train F1: 0.9927 Val Loss: 0.0306, Val Acc: 0.9794, Val F1: 0.9857\n",
            "\n",
            " 🔎 search 35 : deep_rescnn --- lr : 0.0023290654300914477, weight_decay : 0.01, batch_size : 64, pretrain : True, start_layer : conv5.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1179, Train Acc: 0.8712, Train F1: 0.9075 Val Loss: 0.1170, Val Acc: 0.8529, Val F1: 0.9067\n",
            "Epoch: 2/75, Train Loss: 0.0781, Train Acc: 0.9223, Train F1: 0.9459 Val Loss: 0.0670, Val Acc: 0.9464, Val F1: 0.9633\n",
            "Epoch: 3/75, Train Loss: 0.0641, Train Acc: 0.9362, Train F1: 0.9554 Val Loss: 0.0611, Val Acc: 0.9495, Val F1: 0.9651\n",
            "Epoch: 4/75, Train Loss: 0.0623, Train Acc: 0.9388, Train F1: 0.9572 Val Loss: 0.0678, Val Acc: 0.9412, Val F1: 0.9606\n",
            "Epoch: 5/75, Train Loss: 0.0568, Train Acc: 0.9448, Train F1: 0.9616 Val Loss: 0.0523, Val Acc: 0.9533, Val F1: 0.9674\n",
            "Epoch: 6/75, Train Loss: 0.0523, Train Acc: 0.9495, Train F1: 0.9648 Val Loss: 0.0531, Val Acc: 0.9485, Val F1: 0.9647\n",
            "Epoch: 7/75, Train Loss: 0.0457, Train Acc: 0.9569, Train F1: 0.9698 Val Loss: 0.0489, Val Acc: 0.9564, Val F1: 0.9702\n",
            "Epoch: 8/75, Train Loss: 0.0455, Train Acc: 0.9578, Train F1: 0.9704 Val Loss: 0.0663, Val Acc: 0.9357, Val F1: 0.9573\n",
            "Epoch: 9/75, Train Loss: 0.0445, Train Acc: 0.9589, Train F1: 0.9712 Val Loss: 0.0475, Val Acc: 0.9567, Val F1: 0.9696\n",
            "Epoch: 10/75, Train Loss: 0.0483, Train Acc: 0.9521, Train F1: 0.9666 Val Loss: 0.0488, Val Acc: 0.9622, Val F1: 0.9742\n",
            "Epoch: 11/75, Train Loss: 0.0415, Train Acc: 0.9635, Train F1: 0.9747 Val Loss: 0.0445, Val Acc: 0.9619, Val F1: 0.9735\n",
            "Epoch: 12/75, Train Loss: 0.0389, Train Acc: 0.9644, Train F1: 0.9751 Val Loss: 0.0468, Val Acc: 0.9636, Val F1: 0.9751\n",
            "Epoch: 13/75, Train Loss: 0.0382, Train Acc: 0.9663, Train F1: 0.9766 Val Loss: 0.0414, Val Acc: 0.9608, Val F1: 0.9728\n",
            "Epoch: 14/75, Train Loss: 0.0401, Train Acc: 0.9631, Train F1: 0.9742 Val Loss: 0.0585, Val Acc: 0.9430, Val F1: 0.9614\n",
            "Epoch: 15/75, Train Loss: 0.0381, Train Acc: 0.9660, Train F1: 0.9763 Val Loss: 0.0477, Val Acc: 0.9605, Val F1: 0.9733\n",
            "Epoch: 16/75, Train Loss: 0.0358, Train Acc: 0.9696, Train F1: 0.9788 Val Loss: 0.0461, Val Acc: 0.9660, Val F1: 0.9769\n",
            "Epoch: 17/75, Train Loss: 0.0353, Train Acc: 0.9698, Train F1: 0.9789 Val Loss: 0.0484, Val Acc: 0.9598, Val F1: 0.9728\n",
            "Epoch: 18/75, Train Loss: 0.0379, Train Acc: 0.9667, Train F1: 0.9769 Val Loss: 0.0552, Val Acc: 0.9474, Val F1: 0.9647\n",
            "Epoch: 19/75, Train Loss: 0.0377, Train Acc: 0.9667, Train F1: 0.9766 Val Loss: 0.0372, Val Acc: 0.9729, Val F1: 0.9814\n",
            "Epoch: 20/75, Train Loss: 0.0336, Train Acc: 0.9724, Train F1: 0.9806 Val Loss: 0.0497, Val Acc: 0.9557, Val F1: 0.9700\n",
            "Epoch: 21/75, Train Loss: 0.0332, Train Acc: 0.9706, Train F1: 0.9794 Val Loss: 0.0352, Val Acc: 0.9718, Val F1: 0.9804\n",
            "Epoch: 22/75, Train Loss: 0.0360, Train Acc: 0.9686, Train F1: 0.9780 Val Loss: 0.0417, Val Acc: 0.9619, Val F1: 0.9735\n",
            "Epoch: 23/75, Train Loss: 0.0373, Train Acc: 0.9655, Train F1: 0.9759 Val Loss: 0.0366, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 24/75, Train Loss: 0.0337, Train Acc: 0.9699, Train F1: 0.9790 Val Loss: 0.0479, Val Acc: 0.9546, Val F1: 0.9679\n",
            "Epoch: 25/75, Train Loss: 0.0317, Train Acc: 0.9749, Train F1: 0.9826 Val Loss: 0.0381, Val Acc: 0.9722, Val F1: 0.9810\n",
            "Epoch: 26/75, Train Loss: 0.0313, Train Acc: 0.9742, Train F1: 0.9818 Val Loss: 0.0476, Val Acc: 0.9574, Val F1: 0.9712\n",
            "Epoch: 27/75, Train Loss: 0.0318, Train Acc: 0.9738, Train F1: 0.9817 Val Loss: 0.0400, Val Acc: 0.9718, Val F1: 0.9807\n",
            "Epoch: 28/75, Train Loss: 0.0342, Train Acc: 0.9714, Train F1: 0.9800 Val Loss: 0.0361, Val Acc: 0.9759, Val F1: 0.9835\n",
            "Epoch: 29/75, Train Loss: 0.0312, Train Acc: 0.9748, Train F1: 0.9823 Val Loss: 0.0351, Val Acc: 0.9732, Val F1: 0.9814\n",
            "Epoch: 30/75, Train Loss: 0.0293, Train Acc: 0.9784, Train F1: 0.9846 Val Loss: 0.0322, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 31/75, Train Loss: 0.0299, Train Acc: 0.9766, Train F1: 0.9837 Val Loss: 0.0447, Val Acc: 0.9622, Val F1: 0.9744\n",
            "Epoch: 32/75, Train Loss: 0.0280, Train Acc: 0.9802, Train F1: 0.9861 Val Loss: 0.0320, Val Acc: 0.9766, Val F1: 0.9839\n",
            "Epoch: 33/75, Train Loss: 0.0322, Train Acc: 0.9727, Train F1: 0.9808 Val Loss: 0.0347, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 34/75, Train Loss: 0.0317, Train Acc: 0.9753, Train F1: 0.9825 Val Loss: 0.0510, Val Acc: 0.9471, Val F1: 0.9623\n",
            "Epoch: 35/75, Train Loss: 0.0320, Train Acc: 0.9732, Train F1: 0.9812 Val Loss: 0.0329, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 36/75, Train Loss: 0.0267, Train Acc: 0.9808, Train F1: 0.9866 Val Loss: 0.0341, Val Acc: 0.9766, Val F1: 0.9840\n",
            "Epoch: 37/75, Train Loss: 0.0260, Train Acc: 0.9827, Train F1: 0.9880 Val Loss: 0.0307, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 38/75, Train Loss: 0.0288, Train Acc: 0.9778, Train F1: 0.9843 Val Loss: 0.0312, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 39/75, Train Loss: 0.0258, Train Acc: 0.9819, Train F1: 0.9874 Val Loss: 0.0331, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 40/75, Train Loss: 0.0247, Train Acc: 0.9825, Train F1: 0.9877 Val Loss: 0.0340, Val Acc: 0.9763, Val F1: 0.9838\n",
            "Epoch: 41/75, Train Loss: 0.0276, Train Acc: 0.9787, Train F1: 0.9850 Val Loss: 0.0327, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 42/75, Train Loss: 0.0267, Train Acc: 0.9810, Train F1: 0.9867 Val Loss: 0.0337, Val Acc: 0.9694, Val F1: 0.9788\n",
            "Epoch: 43/75, Train Loss: 0.0283, Train Acc: 0.9770, Train F1: 0.9839 Val Loss: 0.0389, Val Acc: 0.9677, Val F1: 0.9780\n",
            "Epoch: 44/75, Train Loss: 0.0255, Train Acc: 0.9814, Train F1: 0.9870 Val Loss: 0.0305, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 45/75, Train Loss: 0.0251, Train Acc: 0.9833, Train F1: 0.9885 Val Loss: 0.0308, Val Acc: 0.9770, Val F1: 0.9841\n",
            "Epoch: 46/75, Train Loss: 0.0250, Train Acc: 0.9845, Train F1: 0.9893 Val Loss: 0.0304, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 47/75, Train Loss: 0.0237, Train Acc: 0.9844, Train F1: 0.9892 Val Loss: 0.0356, Val Acc: 0.9680, Val F1: 0.9776\n",
            "Epoch: 48/75, Train Loss: 0.0254, Train Acc: 0.9829, Train F1: 0.9880 Val Loss: 0.0320, Val Acc: 0.9722, Val F1: 0.9807\n",
            "Epoch: 49/75, Train Loss: 0.0235, Train Acc: 0.9842, Train F1: 0.9889 Val Loss: 0.0310, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 50/75, Train Loss: 0.0228, Train Acc: 0.9850, Train F1: 0.9896 Val Loss: 0.0308, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 51/75, Train Loss: 0.0233, Train Acc: 0.9844, Train F1: 0.9891 Val Loss: 0.0303, Val Acc: 0.9784, Val F1: 0.9850\n",
            "Epoch: 52/75, Train Loss: 0.0241, Train Acc: 0.9827, Train F1: 0.9879 Val Loss: 0.0325, Val Acc: 0.9749, Val F1: 0.9825\n",
            "Epoch: 53/75, Train Loss: 0.0232, Train Acc: 0.9844, Train F1: 0.9892 Val Loss: 0.0315, Val Acc: 0.9780, Val F1: 0.9847\n",
            "Epoch: 54/75, Train Loss: 0.0227, Train Acc: 0.9858, Train F1: 0.9900 Val Loss: 0.0338, Val Acc: 0.9694, Val F1: 0.9786\n",
            "Epoch: 55/75, Train Loss: 0.0232, Train Acc: 0.9844, Train F1: 0.9891 Val Loss: 0.0302, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 56/75, Train Loss: 0.0218, Train Acc: 0.9868, Train F1: 0.9907 Val Loss: 0.0306, Val Acc: 0.9811, Val F1: 0.9870\n",
            "Epoch: 57/75, Train Loss: 0.0222, Train Acc: 0.9844, Train F1: 0.9891 Val Loss: 0.0310, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 58/75, Train Loss: 0.0223, Train Acc: 0.9860, Train F1: 0.9902 Val Loss: 0.0295, Val Acc: 0.9770, Val F1: 0.9840\n",
            "Epoch: 59/75, Train Loss: 0.0213, Train Acc: 0.9874, Train F1: 0.9912 Val Loss: 0.0299, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 60/75, Train Loss: 0.0213, Train Acc: 0.9877, Train F1: 0.9914 Val Loss: 0.0297, Val Acc: 0.9787, Val F1: 0.9852\n",
            "Epoch: 61/75, Train Loss: 0.0210, Train Acc: 0.9873, Train F1: 0.9911 Val Loss: 0.0292, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 62/75, Train Loss: 0.0207, Train Acc: 0.9881, Train F1: 0.9916 Val Loss: 0.0294, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 63/75, Train Loss: 0.0211, Train Acc: 0.9874, Train F1: 0.9913 Val Loss: 0.0289, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 64/75, Train Loss: 0.0205, Train Acc: 0.9871, Train F1: 0.9910 Val Loss: 0.0294, Val Acc: 0.9814, Val F1: 0.9872\n",
            "Epoch: 65/75, Train Loss: 0.0204, Train Acc: 0.9875, Train F1: 0.9913 Val Loss: 0.0293, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 66/75, Train Loss: 0.0203, Train Acc: 0.9873, Train F1: 0.9912 Val Loss: 0.0293, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 67/75, Train Loss: 0.0201, Train Acc: 0.9875, Train F1: 0.9912 Val Loss: 0.0290, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 68/75, Train Loss: 0.0202, Train Acc: 0.9877, Train F1: 0.9914 Val Loss: 0.0292, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 69/75, Train Loss: 0.0200, Train Acc: 0.9884, Train F1: 0.9920 Val Loss: 0.0292, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 70/75, Train Loss: 0.0199, Train Acc: 0.9880, Train F1: 0.9916 Val Loss: 0.0294, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 71/75, Train Loss: 0.0198, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0290, Val Acc: 0.9814, Val F1: 0.9872\n",
            "Epoch: 72/75, Train Loss: 0.0198, Train Acc: 0.9889, Train F1: 0.9923 Val Loss: 0.0291, Val Acc: 0.9811, Val F1: 0.9870\n",
            "Epoch: 73/75, Train Loss: 0.0197, Train Acc: 0.9885, Train F1: 0.9921 Val Loss: 0.0291, Val Acc: 0.9811, Val F1: 0.9870\n",
            "Epoch: 74/75, Train Loss: 0.0197, Train Acc: 0.9889, Train F1: 0.9923 Val Loss: 0.0291, Val Acc: 0.9811, Val F1: 0.9870\n",
            "Epoch: 75/75, Train Loss: 0.0197, Train Acc: 0.9889, Train F1: 0.9922 Val Loss: 0.0291, Val Acc: 0.9811, Val F1: 0.9870\n",
            "\n",
            " 🔎 search 36 : deep_rescnn --- lr : 0.0023290654300914477, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv2.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1227, Train Acc: 0.8668, Train F1: 0.9061 Val Loss: 0.1049, Val Acc: 0.9055, Val F1: 0.9342\n",
            "Epoch: 2/75, Train Loss: 0.0956, Train Acc: 0.9095, Train F1: 0.9373 Val Loss: 0.1017, Val Acc: 0.9034, Val F1: 0.9334\n",
            "Epoch: 3/75, Train Loss: 0.0954, Train Acc: 0.9104, Train F1: 0.9379 Val Loss: 0.1155, Val Acc: 0.8763, Val F1: 0.9101\n",
            "Epoch: 4/75, Train Loss: 0.0914, Train Acc: 0.9143, Train F1: 0.9406 Val Loss: 0.0880, Val Acc: 0.9168, Val F1: 0.9423\n",
            "Epoch: 5/75, Train Loss: 0.0842, Train Acc: 0.9249, Train F1: 0.9479 Val Loss: 0.0913, Val Acc: 0.9031, Val F1: 0.9307\n",
            "Epoch: 6/75, Train Loss: 0.0815, Train Acc: 0.9246, Train F1: 0.9480 Val Loss: 0.0810, Val Acc: 0.9340, Val F1: 0.9549\n",
            "Epoch: 7/75, Train Loss: 0.0796, Train Acc: 0.9266, Train F1: 0.9492 Val Loss: 0.0944, Val Acc: 0.9038, Val F1: 0.9361\n",
            "Epoch: 8/75, Train Loss: 0.0828, Train Acc: 0.9190, Train F1: 0.9439 Val Loss: 0.0910, Val Acc: 0.9134, Val F1: 0.9428\n",
            "Epoch: 9/75, Train Loss: 0.0759, Train Acc: 0.9321, Train F1: 0.9530 Val Loss: 0.0806, Val Acc: 0.9285, Val F1: 0.9511\n",
            "Epoch: 10/75, Train Loss: 0.0762, Train Acc: 0.9318, Train F1: 0.9529 Val Loss: 0.0774, Val Acc: 0.9292, Val F1: 0.9512\n",
            "Epoch: 11/75, Train Loss: 0.0769, Train Acc: 0.9283, Train F1: 0.9503 Val Loss: 0.0974, Val Acc: 0.8935, Val F1: 0.9228\n",
            "Epoch: 12/75, Train Loss: 0.0757, Train Acc: 0.9291, Train F1: 0.9508 Val Loss: 0.0772, Val Acc: 0.9330, Val F1: 0.9540\n",
            "Epoch: 13/75, Train Loss: 0.0697, Train Acc: 0.9378, Train F1: 0.9569 Val Loss: 0.0779, Val Acc: 0.9275, Val F1: 0.9491\n",
            "Epoch: 14/75, Train Loss: 0.0725, Train Acc: 0.9347, Train F1: 0.9549 Val Loss: 0.0721, Val Acc: 0.9326, Val F1: 0.9532\n",
            "Epoch: 15/75, Train Loss: 0.0695, Train Acc: 0.9379, Train F1: 0.9571 Val Loss: 0.0733, Val Acc: 0.9361, Val F1: 0.9563\n",
            "Epoch: 16/75, Train Loss: 0.0693, Train Acc: 0.9369, Train F1: 0.9561 Val Loss: 0.0954, Val Acc: 0.9003, Val F1: 0.9351\n",
            "Epoch: 17/75, Train Loss: 0.0736, Train Acc: 0.9317, Train F1: 0.9529 Val Loss: 0.0733, Val Acc: 0.9368, Val F1: 0.9564\n",
            "Epoch: 18/75, Train Loss: 0.0680, Train Acc: 0.9378, Train F1: 0.9570 Val Loss: 0.0735, Val Acc: 0.9347, Val F1: 0.9556\n",
            "Epoch: 19/75, Train Loss: 0.0692, Train Acc: 0.9378, Train F1: 0.9570 Val Loss: 0.0779, Val Acc: 0.9213, Val F1: 0.9452\n",
            "Epoch: 20/75, Train Loss: 0.0678, Train Acc: 0.9364, Train F1: 0.9559 Val Loss: 0.0719, Val Acc: 0.9381, Val F1: 0.9580\n",
            "Epoch: 21/75, Train Loss: 0.0691, Train Acc: 0.9352, Train F1: 0.9550 Val Loss: 0.0752, Val Acc: 0.9258, Val F1: 0.9480\n",
            "Epoch: 22/75, Train Loss: 0.0684, Train Acc: 0.9361, Train F1: 0.9555 Val Loss: 0.0888, Val Acc: 0.9158, Val F1: 0.9438\n",
            "Epoch: 23/75, Train Loss: 0.0651, Train Acc: 0.9395, Train F1: 0.9582 Val Loss: 0.0720, Val Acc: 0.9337, Val F1: 0.9552\n",
            "Epoch: 24/75, Train Loss: 0.0642, Train Acc: 0.9430, Train F1: 0.9606 Val Loss: 0.0819, Val Acc: 0.9179, Val F1: 0.9419\n",
            "Epoch: 25/75, Train Loss: 0.0663, Train Acc: 0.9408, Train F1: 0.9589 Val Loss: 0.0713, Val Acc: 0.9351, Val F1: 0.9557\n",
            "Epoch: 26/75, Train Loss: 0.0629, Train Acc: 0.9440, Train F1: 0.9613 Val Loss: 0.0673, Val Acc: 0.9361, Val F1: 0.9560\n",
            "Epoch: 27/75, Train Loss: 0.0621, Train Acc: 0.9447, Train F1: 0.9616 Val Loss: 0.0674, Val Acc: 0.9402, Val F1: 0.9594\n",
            "Epoch: 28/75, Train Loss: 0.0647, Train Acc: 0.9420, Train F1: 0.9599 Val Loss: 0.0681, Val Acc: 0.9330, Val F1: 0.9532\n",
            "Epoch: 29/75, Train Loss: 0.0668, Train Acc: 0.9369, Train F1: 0.9563 Val Loss: 0.0809, Val Acc: 0.9227, Val F1: 0.9480\n",
            "Epoch: 30/75, Train Loss: 0.0657, Train Acc: 0.9380, Train F1: 0.9571 Val Loss: 0.0662, Val Acc: 0.9392, Val F1: 0.9582\n",
            "Epoch: 31/75, Train Loss: 0.0623, Train Acc: 0.9408, Train F1: 0.9591 Val Loss: 0.0630, Val Acc: 0.9426, Val F1: 0.9607\n",
            "Epoch: 32/75, Train Loss: 0.0611, Train Acc: 0.9450, Train F1: 0.9621 Val Loss: 0.0697, Val Acc: 0.9340, Val F1: 0.9540\n",
            "Epoch: 33/75, Train Loss: 0.0581, Train Acc: 0.9471, Train F1: 0.9635 Val Loss: 0.0634, Val Acc: 0.9419, Val F1: 0.9603\n",
            "Epoch: 34/75, Train Loss: 0.0586, Train Acc: 0.9479, Train F1: 0.9639 Val Loss: 0.0626, Val Acc: 0.9426, Val F1: 0.9605\n",
            "Epoch: 35/75, Train Loss: 0.0603, Train Acc: 0.9454, Train F1: 0.9621 Val Loss: 0.0619, Val Acc: 0.9426, Val F1: 0.9605\n",
            "Epoch: 36/75, Train Loss: 0.0599, Train Acc: 0.9446, Train F1: 0.9616 Val Loss: 0.0750, Val Acc: 0.9371, Val F1: 0.9575\n",
            "Epoch: 37/75, Train Loss: 0.0562, Train Acc: 0.9503, Train F1: 0.9656 Val Loss: 0.0683, Val Acc: 0.9419, Val F1: 0.9606\n",
            "Epoch: 38/75, Train Loss: 0.0583, Train Acc: 0.9464, Train F1: 0.9628 Val Loss: 0.0645, Val Acc: 0.9381, Val F1: 0.9575\n",
            "Epoch: 39/75, Train Loss: 0.0556, Train Acc: 0.9493, Train F1: 0.9649 Val Loss: 0.0634, Val Acc: 0.9392, Val F1: 0.9577\n",
            "Epoch: 40/75, Train Loss: 0.0572, Train Acc: 0.9467, Train F1: 0.9630 Val Loss: 0.0617, Val Acc: 0.9471, Val F1: 0.9640\n",
            "Epoch: 41/75, Train Loss: 0.0565, Train Acc: 0.9495, Train F1: 0.9652 Val Loss: 0.0615, Val Acc: 0.9436, Val F1: 0.9610\n",
            "Epoch: 42/75, Train Loss: 0.0546, Train Acc: 0.9498, Train F1: 0.9653 Val Loss: 0.0655, Val Acc: 0.9378, Val F1: 0.9564\n",
            "Epoch: 43/75, Train Loss: 0.0573, Train Acc: 0.9463, Train F1: 0.9630 Val Loss: 0.0642, Val Acc: 0.9402, Val F1: 0.9594\n",
            "Epoch: 44/75, Train Loss: 0.0528, Train Acc: 0.9536, Train F1: 0.9679 Val Loss: 0.0576, Val Acc: 0.9485, Val F1: 0.9644\n",
            "Epoch: 45/75, Train Loss: 0.0518, Train Acc: 0.9528, Train F1: 0.9674 Val Loss: 0.0579, Val Acc: 0.9502, Val F1: 0.9660\n",
            "Epoch: 46/75, Train Loss: 0.0532, Train Acc: 0.9518, Train F1: 0.9665 Val Loss: 0.0634, Val Acc: 0.9388, Val F1: 0.9586\n",
            "Epoch: 47/75, Train Loss: 0.0552, Train Acc: 0.9499, Train F1: 0.9656 Val Loss: 0.0595, Val Acc: 0.9423, Val F1: 0.9600\n",
            "Epoch: 48/75, Train Loss: 0.0519, Train Acc: 0.9543, Train F1: 0.9683 Val Loss: 0.0613, Val Acc: 0.9478, Val F1: 0.9646\n",
            "Epoch: 49/75, Train Loss: 0.0538, Train Acc: 0.9518, Train F1: 0.9665 Val Loss: 0.0582, Val Acc: 0.9460, Val F1: 0.9630\n",
            "Epoch: 50/75, Train Loss: 0.0526, Train Acc: 0.9527, Train F1: 0.9673 Val Loss: 0.0624, Val Acc: 0.9423, Val F1: 0.9607\n",
            "Epoch: 51/75, Train Loss: 0.0508, Train Acc: 0.9542, Train F1: 0.9684 Val Loss: 0.0566, Val Acc: 0.9481, Val F1: 0.9644\n",
            "Epoch: 52/75, Train Loss: 0.0494, Train Acc: 0.9565, Train F1: 0.9700 Val Loss: 0.0555, Val Acc: 0.9495, Val F1: 0.9654\n",
            "Epoch: 53/75, Train Loss: 0.0499, Train Acc: 0.9544, Train F1: 0.9684 Val Loss: 0.0604, Val Acc: 0.9478, Val F1: 0.9647\n",
            "Epoch: 54/75, Train Loss: 0.0486, Train Acc: 0.9559, Train F1: 0.9695 Val Loss: 0.0552, Val Acc: 0.9519, Val F1: 0.9669\n",
            "Epoch: 55/75, Train Loss: 0.0483, Train Acc: 0.9553, Train F1: 0.9690 Val Loss: 0.0563, Val Acc: 0.9533, Val F1: 0.9682\n",
            "Epoch: 56/75, Train Loss: 0.0480, Train Acc: 0.9564, Train F1: 0.9699 Val Loss: 0.0542, Val Acc: 0.9519, Val F1: 0.9671\n",
            "Epoch: 57/75, Train Loss: 0.0471, Train Acc: 0.9569, Train F1: 0.9702 Val Loss: 0.0617, Val Acc: 0.9433, Val F1: 0.9617\n",
            "Epoch: 58/75, Train Loss: 0.0479, Train Acc: 0.9557, Train F1: 0.9693 Val Loss: 0.0558, Val Acc: 0.9543, Val F1: 0.9690\n",
            "Epoch: 59/75, Train Loss: 0.0470, Train Acc: 0.9560, Train F1: 0.9694 Val Loss: 0.0542, Val Acc: 0.9536, Val F1: 0.9682\n",
            "Epoch: 60/75, Train Loss: 0.0462, Train Acc: 0.9584, Train F1: 0.9713 Val Loss: 0.0537, Val Acc: 0.9533, Val F1: 0.9682\n",
            "Epoch: 61/75, Train Loss: 0.0456, Train Acc: 0.9590, Train F1: 0.9717 Val Loss: 0.0550, Val Acc: 0.9529, Val F1: 0.9680\n",
            "Epoch: 62/75, Train Loss: 0.0457, Train Acc: 0.9583, Train F1: 0.9711 Val Loss: 0.0520, Val Acc: 0.9526, Val F1: 0.9675\n",
            "Epoch: 63/75, Train Loss: 0.0453, Train Acc: 0.9588, Train F1: 0.9715 Val Loss: 0.0526, Val Acc: 0.9536, Val F1: 0.9683\n",
            "Epoch: 64/75, Train Loss: 0.0447, Train Acc: 0.9592, Train F1: 0.9718 Val Loss: 0.0521, Val Acc: 0.9540, Val F1: 0.9685\n",
            "Epoch: 65/75, Train Loss: 0.0442, Train Acc: 0.9599, Train F1: 0.9723 Val Loss: 0.0521, Val Acc: 0.9546, Val F1: 0.9689\n",
            "Epoch: 66/75, Train Loss: 0.0444, Train Acc: 0.9604, Train F1: 0.9727 Val Loss: 0.0527, Val Acc: 0.9546, Val F1: 0.9691\n",
            "Epoch: 67/75, Train Loss: 0.0443, Train Acc: 0.9591, Train F1: 0.9718 Val Loss: 0.0517, Val Acc: 0.9560, Val F1: 0.9699\n",
            "Epoch: 68/75, Train Loss: 0.0437, Train Acc: 0.9605, Train F1: 0.9727 Val Loss: 0.0515, Val Acc: 0.9564, Val F1: 0.9702\n",
            "Epoch: 69/75, Train Loss: 0.0437, Train Acc: 0.9603, Train F1: 0.9725 Val Loss: 0.0520, Val Acc: 0.9546, Val F1: 0.9691\n",
            "Epoch: 70/75, Train Loss: 0.0435, Train Acc: 0.9600, Train F1: 0.9725 Val Loss: 0.0511, Val Acc: 0.9557, Val F1: 0.9696\n",
            "Epoch: 71/75, Train Loss: 0.0434, Train Acc: 0.9603, Train F1: 0.9726 Val Loss: 0.0518, Val Acc: 0.9553, Val F1: 0.9695\n",
            "Epoch: 72/75, Train Loss: 0.0434, Train Acc: 0.9604, Train F1: 0.9726 Val Loss: 0.0510, Val Acc: 0.9553, Val F1: 0.9693\n",
            "Epoch: 73/75, Train Loss: 0.0432, Train Acc: 0.9609, Train F1: 0.9731 Val Loss: 0.0512, Val Acc: 0.9560, Val F1: 0.9699\n",
            "Epoch: 74/75, Train Loss: 0.0432, Train Acc: 0.9607, Train F1: 0.9729 Val Loss: 0.0510, Val Acc: 0.9553, Val F1: 0.9694\n",
            "Epoch: 75/75, Train Loss: 0.0431, Train Acc: 0.9608, Train F1: 0.9730 Val Loss: 0.0510, Val Acc: 0.9560, Val F1: 0.9699\n",
            "\n",
            " 🔎 search 37 : deep_rescnn --- lr : 0.0023290654300914477, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv3.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1446, Train Acc: 0.8325, Train F1: 0.8803 Val Loss: 0.1045, Val Acc: 0.9065, Val F1: 0.9376\n",
            "Epoch: 2/75, Train Loss: 0.0918, Train Acc: 0.9103, Train F1: 0.9380 Val Loss: 0.0857, Val Acc: 0.9299, Val F1: 0.9511\n",
            "Epoch: 3/75, Train Loss: 0.0805, Train Acc: 0.9238, Train F1: 0.9474 Val Loss: 0.0751, Val Acc: 0.9419, Val F1: 0.9599\n",
            "Epoch: 4/75, Train Loss: 0.0696, Train Acc: 0.9354, Train F1: 0.9555 Val Loss: 0.1104, Val Acc: 0.8711, Val F1: 0.9177\n",
            "Epoch: 5/75, Train Loss: 0.0665, Train Acc: 0.9377, Train F1: 0.9569 Val Loss: 0.1040, Val Acc: 0.8763, Val F1: 0.9088\n",
            "Epoch: 6/75, Train Loss: 0.0758, Train Acc: 0.9257, Train F1: 0.9481 Val Loss: 0.0725, Val Acc: 0.9357, Val F1: 0.9557\n",
            "Epoch: 7/75, Train Loss: 0.0610, Train Acc: 0.9414, Train F1: 0.9594 Val Loss: 0.0613, Val Acc: 0.9505, Val F1: 0.9664\n",
            "Epoch: 8/75, Train Loss: 0.0584, Train Acc: 0.9450, Train F1: 0.9619 Val Loss: 0.0750, Val Acc: 0.9227, Val F1: 0.9446\n",
            "Epoch: 9/75, Train Loss: 0.0593, Train Acc: 0.9404, Train F1: 0.9587 Val Loss: 0.0559, Val Acc: 0.9553, Val F1: 0.9695\n",
            "Epoch: 10/75, Train Loss: 0.0511, Train Acc: 0.9498, Train F1: 0.9652 Val Loss: 0.0556, Val Acc: 0.9498, Val F1: 0.9654\n",
            "Epoch: 11/75, Train Loss: 0.0523, Train Acc: 0.9504, Train F1: 0.9655 Val Loss: 0.0548, Val Acc: 0.9526, Val F1: 0.9680\n",
            "Epoch: 12/75, Train Loss: 0.0540, Train Acc: 0.9477, Train F1: 0.9638 Val Loss: 0.0692, Val Acc: 0.9340, Val F1: 0.9529\n",
            "Epoch: 13/75, Train Loss: 0.0527, Train Acc: 0.9485, Train F1: 0.9639 Val Loss: 0.0519, Val Acc: 0.9567, Val F1: 0.9705\n",
            "Epoch: 14/75, Train Loss: 0.0443, Train Acc: 0.9598, Train F1: 0.9721 Val Loss: 0.0548, Val Acc: 0.9478, Val F1: 0.9634\n",
            "Epoch: 15/75, Train Loss: 0.0502, Train Acc: 0.9513, Train F1: 0.9664 Val Loss: 0.0772, Val Acc: 0.9107, Val F1: 0.9350\n",
            "Epoch: 16/75, Train Loss: 0.0513, Train Acc: 0.9501, Train F1: 0.9653 Val Loss: 0.0578, Val Acc: 0.9454, Val F1: 0.9631\n",
            "Epoch: 17/75, Train Loss: 0.0452, Train Acc: 0.9575, Train F1: 0.9706 Val Loss: 0.0477, Val Acc: 0.9519, Val F1: 0.9666\n",
            "Epoch: 18/75, Train Loss: 0.0420, Train Acc: 0.9612, Train F1: 0.9731 Val Loss: 0.0492, Val Acc: 0.9605, Val F1: 0.9724\n",
            "Epoch: 19/75, Train Loss: 0.0466, Train Acc: 0.9566, Train F1: 0.9698 Val Loss: 0.0599, Val Acc: 0.9433, Val F1: 0.9612\n",
            "Epoch: 20/75, Train Loss: 0.0462, Train Acc: 0.9581, Train F1: 0.9708 Val Loss: 0.0497, Val Acc: 0.9495, Val F1: 0.9650\n",
            "Epoch: 21/75, Train Loss: 0.0430, Train Acc: 0.9596, Train F1: 0.9719 Val Loss: 0.0458, Val Acc: 0.9591, Val F1: 0.9719\n",
            "Epoch: 22/75, Train Loss: 0.0394, Train Acc: 0.9643, Train F1: 0.9752 Val Loss: 0.0473, Val Acc: 0.9615, Val F1: 0.9733\n",
            "Epoch: 23/75, Train Loss: 0.0443, Train Acc: 0.9592, Train F1: 0.9716 Val Loss: 0.0649, Val Acc: 0.9337, Val F1: 0.9559\n",
            "Epoch: 24/75, Train Loss: 0.0423, Train Acc: 0.9623, Train F1: 0.9739 Val Loss: 0.0433, Val Acc: 0.9629, Val F1: 0.9746\n",
            "Epoch: 25/75, Train Loss: 0.0406, Train Acc: 0.9630, Train F1: 0.9741 Val Loss: 0.0450, Val Acc: 0.9560, Val F1: 0.9693\n",
            "Epoch: 26/75, Train Loss: 0.0389, Train Acc: 0.9654, Train F1: 0.9760 Val Loss: 0.0436, Val Acc: 0.9643, Val F1: 0.9751\n",
            "Epoch: 27/75, Train Loss: 0.0392, Train Acc: 0.9639, Train F1: 0.9750 Val Loss: 0.0421, Val Acc: 0.9646, Val F1: 0.9756\n",
            "Epoch: 28/75, Train Loss: 0.0451, Train Acc: 0.9570, Train F1: 0.9702 Val Loss: 0.0620, Val Acc: 0.9316, Val F1: 0.9510\n",
            "Epoch: 29/75, Train Loss: 0.0377, Train Acc: 0.9661, Train F1: 0.9763 Val Loss: 0.0489, Val Acc: 0.9591, Val F1: 0.9722\n",
            "Epoch: 30/75, Train Loss: 0.0388, Train Acc: 0.9643, Train F1: 0.9752 Val Loss: 0.0428, Val Acc: 0.9625, Val F1: 0.9742\n",
            "Epoch: 31/75, Train Loss: 0.0376, Train Acc: 0.9674, Train F1: 0.9774 Val Loss: 0.0435, Val Acc: 0.9687, Val F1: 0.9786\n",
            "Epoch: 32/75, Train Loss: 0.0410, Train Acc: 0.9633, Train F1: 0.9746 Val Loss: 0.0412, Val Acc: 0.9632, Val F1: 0.9747\n",
            "Epoch: 33/75, Train Loss: 0.0375, Train Acc: 0.9672, Train F1: 0.9774 Val Loss: 0.0436, Val Acc: 0.9595, Val F1: 0.9722\n",
            "Epoch: 34/75, Train Loss: 0.0360, Train Acc: 0.9682, Train F1: 0.9779 Val Loss: 0.0418, Val Acc: 0.9660, Val F1: 0.9768\n",
            "Epoch: 35/75, Train Loss: 0.0353, Train Acc: 0.9694, Train F1: 0.9788 Val Loss: 0.0392, Val Acc: 0.9677, Val F1: 0.9776\n",
            "Epoch: 36/75, Train Loss: 0.0336, Train Acc: 0.9721, Train F1: 0.9804 Val Loss: 0.0396, Val Acc: 0.9677, Val F1: 0.9777\n",
            "Epoch: 37/75, Train Loss: 0.0345, Train Acc: 0.9710, Train F1: 0.9798 Val Loss: 0.0444, Val Acc: 0.9608, Val F1: 0.9734\n",
            "Epoch: 38/75, Train Loss: 0.0335, Train Acc: 0.9719, Train F1: 0.9806 Val Loss: 0.0480, Val Acc: 0.9557, Val F1: 0.9699\n",
            "Epoch: 39/75, Train Loss: 0.0336, Train Acc: 0.9714, Train F1: 0.9801 Val Loss: 0.0426, Val Acc: 0.9677, Val F1: 0.9780\n",
            "Epoch: 40/75, Train Loss: 0.0324, Train Acc: 0.9724, Train F1: 0.9809 Val Loss: 0.0374, Val Acc: 0.9667, Val F1: 0.9770\n",
            "Epoch: 41/75, Train Loss: 0.0336, Train Acc: 0.9711, Train F1: 0.9800 Val Loss: 0.0430, Val Acc: 0.9557, Val F1: 0.9691\n",
            "Epoch: 42/75, Train Loss: 0.0325, Train Acc: 0.9731, Train F1: 0.9814 Val Loss: 0.0414, Val Acc: 0.9625, Val F1: 0.9739\n",
            "Epoch: 43/75, Train Loss: 0.0318, Train Acc: 0.9732, Train F1: 0.9813 Val Loss: 0.0366, Val Acc: 0.9698, Val F1: 0.9793\n",
            "Epoch: 44/75, Train Loss: 0.0326, Train Acc: 0.9724, Train F1: 0.9809 Val Loss: 0.0369, Val Acc: 0.9670, Val F1: 0.9772\n",
            "Epoch: 45/75, Train Loss: 0.0309, Train Acc: 0.9742, Train F1: 0.9820 Val Loss: 0.0364, Val Acc: 0.9687, Val F1: 0.9785\n",
            "Epoch: 46/75, Train Loss: 0.0315, Train Acc: 0.9740, Train F1: 0.9819 Val Loss: 0.0409, Val Acc: 0.9663, Val F1: 0.9771\n",
            "Epoch: 47/75, Train Loss: 0.0309, Train Acc: 0.9753, Train F1: 0.9829 Val Loss: 0.0378, Val Acc: 0.9649, Val F1: 0.9759\n",
            "Epoch: 48/75, Train Loss: 0.0310, Train Acc: 0.9747, Train F1: 0.9823 Val Loss: 0.0449, Val Acc: 0.9564, Val F1: 0.9693\n",
            "Epoch: 49/75, Train Loss: 0.0296, Train Acc: 0.9753, Train F1: 0.9828 Val Loss: 0.0397, Val Acc: 0.9680, Val F1: 0.9782\n",
            "Epoch: 50/75, Train Loss: 0.0297, Train Acc: 0.9751, Train F1: 0.9827 Val Loss: 0.0361, Val Acc: 0.9701, Val F1: 0.9794\n",
            "Epoch: 51/75, Train Loss: 0.0290, Train Acc: 0.9773, Train F1: 0.9843 Val Loss: 0.0354, Val Acc: 0.9684, Val F1: 0.9782\n",
            "Epoch: 52/75, Train Loss: 0.0304, Train Acc: 0.9749, Train F1: 0.9825 Val Loss: 0.0362, Val Acc: 0.9670, Val F1: 0.9772\n",
            "Epoch: 53/75, Train Loss: 0.0291, Train Acc: 0.9759, Train F1: 0.9833 Val Loss: 0.0358, Val Acc: 0.9708, Val F1: 0.9798\n",
            "Epoch: 54/75, Train Loss: 0.0283, Train Acc: 0.9779, Train F1: 0.9847 Val Loss: 0.0352, Val Acc: 0.9680, Val F1: 0.9779\n",
            "Epoch: 55/75, Train Loss: 0.0295, Train Acc: 0.9769, Train F1: 0.9839 Val Loss: 0.0345, Val Acc: 0.9722, Val F1: 0.9809\n",
            "Epoch: 56/75, Train Loss: 0.0278, Train Acc: 0.9795, Train F1: 0.9857 Val Loss: 0.0359, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 57/75, Train Loss: 0.0280, Train Acc: 0.9779, Train F1: 0.9847 Val Loss: 0.0363, Val Acc: 0.9715, Val F1: 0.9805\n",
            "Epoch: 58/75, Train Loss: 0.0273, Train Acc: 0.9789, Train F1: 0.9854 Val Loss: 0.0366, Val Acc: 0.9684, Val F1: 0.9779\n",
            "Epoch: 59/75, Train Loss: 0.0275, Train Acc: 0.9795, Train F1: 0.9858 Val Loss: 0.0359, Val Acc: 0.9691, Val F1: 0.9785\n",
            "Epoch: 60/75, Train Loss: 0.0271, Train Acc: 0.9804, Train F1: 0.9863 Val Loss: 0.0351, Val Acc: 0.9715, Val F1: 0.9804\n",
            "Epoch: 61/75, Train Loss: 0.0268, Train Acc: 0.9795, Train F1: 0.9858 Val Loss: 0.0353, Val Acc: 0.9670, Val F1: 0.9771\n",
            "Epoch: 62/75, Train Loss: 0.0281, Train Acc: 0.9788, Train F1: 0.9852 Val Loss: 0.0344, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 63/75, Train Loss: 0.0269, Train Acc: 0.9787, Train F1: 0.9851 Val Loss: 0.0338, Val Acc: 0.9711, Val F1: 0.9801\n",
            "Epoch: 64/75, Train Loss: 0.0265, Train Acc: 0.9793, Train F1: 0.9856 Val Loss: 0.0352, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 65/75, Train Loss: 0.0266, Train Acc: 0.9806, Train F1: 0.9865 Val Loss: 0.0337, Val Acc: 0.9718, Val F1: 0.9806\n",
            "Epoch: 66/75, Train Loss: 0.0262, Train Acc: 0.9790, Train F1: 0.9854 Val Loss: 0.0340, Val Acc: 0.9715, Val F1: 0.9804\n",
            "Epoch: 67/75, Train Loss: 0.0260, Train Acc: 0.9808, Train F1: 0.9866 Val Loss: 0.0347, Val Acc: 0.9735, Val F1: 0.9819\n",
            "Epoch: 68/75, Train Loss: 0.0258, Train Acc: 0.9811, Train F1: 0.9869 Val Loss: 0.0335, Val Acc: 0.9722, Val F1: 0.9808\n",
            "Epoch: 69/75, Train Loss: 0.0257, Train Acc: 0.9806, Train F1: 0.9865 Val Loss: 0.0336, Val Acc: 0.9722, Val F1: 0.9808\n",
            "Epoch: 70/75, Train Loss: 0.0257, Train Acc: 0.9809, Train F1: 0.9867 Val Loss: 0.0335, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 71/75, Train Loss: 0.0255, Train Acc: 0.9812, Train F1: 0.9871 Val Loss: 0.0335, Val Acc: 0.9718, Val F1: 0.9806\n",
            "Epoch: 72/75, Train Loss: 0.0255, Train Acc: 0.9808, Train F1: 0.9866 Val Loss: 0.0334, Val Acc: 0.9715, Val F1: 0.9803\n",
            "Epoch: 73/75, Train Loss: 0.0255, Train Acc: 0.9813, Train F1: 0.9870 Val Loss: 0.0334, Val Acc: 0.9711, Val F1: 0.9801\n",
            "Epoch: 74/75, Train Loss: 0.0254, Train Acc: 0.9813, Train F1: 0.9870 Val Loss: 0.0334, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 75/75, Train Loss: 0.0254, Train Acc: 0.9811, Train F1: 0.9870 Val Loss: 0.0334, Val Acc: 0.9715, Val F1: 0.9803\n",
            "\n",
            " 🔎 search 38 : deep_rescnn --- lr : 0.0023290654300914477, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv4.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1419, Train Acc: 0.8434, Train F1: 0.8893 Val Loss: 0.0988, Val Acc: 0.9017, Val F1: 0.9337\n",
            "Epoch: 2/75, Train Loss: 0.0834, Train Acc: 0.9179, Train F1: 0.9430 Val Loss: 0.0819, Val Acc: 0.9289, Val F1: 0.9520\n",
            "Epoch: 3/75, Train Loss: 0.0679, Train Acc: 0.9361, Train F1: 0.9557 Val Loss: 0.0764, Val Acc: 0.9337, Val F1: 0.9548\n",
            "Epoch: 4/75, Train Loss: 0.0648, Train Acc: 0.9354, Train F1: 0.9555 Val Loss: 0.0635, Val Acc: 0.9405, Val F1: 0.9585\n",
            "Epoch: 5/75, Train Loss: 0.0601, Train Acc: 0.9414, Train F1: 0.9593 Val Loss: 0.0564, Val Acc: 0.9505, Val F1: 0.9660\n",
            "Epoch: 6/75, Train Loss: 0.0591, Train Acc: 0.9434, Train F1: 0.9604 Val Loss: 0.0823, Val Acc: 0.9103, Val F1: 0.9412\n",
            "Epoch: 7/75, Train Loss: 0.0490, Train Acc: 0.9535, Train F1: 0.9679 Val Loss: 0.0571, Val Acc: 0.9457, Val F1: 0.9621\n",
            "Epoch: 8/75, Train Loss: 0.0524, Train Acc: 0.9474, Train F1: 0.9636 Val Loss: 0.0654, Val Acc: 0.9378, Val F1: 0.9584\n",
            "Epoch: 9/75, Train Loss: 0.0484, Train Acc: 0.9525, Train F1: 0.9671 Val Loss: 0.0584, Val Acc: 0.9443, Val F1: 0.9622\n",
            "Epoch: 10/75, Train Loss: 0.0483, Train Acc: 0.9521, Train F1: 0.9667 Val Loss: 0.0569, Val Acc: 0.9502, Val F1: 0.9663\n",
            "Epoch: 11/75, Train Loss: 0.0443, Train Acc: 0.9598, Train F1: 0.9721 Val Loss: 0.0448, Val Acc: 0.9605, Val F1: 0.9726\n",
            "Epoch: 12/75, Train Loss: 0.0392, Train Acc: 0.9638, Train F1: 0.9748 Val Loss: 0.0487, Val Acc: 0.9550, Val F1: 0.9693\n",
            "Epoch: 13/75, Train Loss: 0.0382, Train Acc: 0.9680, Train F1: 0.9778 Val Loss: 0.0575, Val Acc: 0.9454, Val F1: 0.9633\n",
            "Epoch: 14/75, Train Loss: 0.0387, Train Acc: 0.9633, Train F1: 0.9746 Val Loss: 0.0522, Val Acc: 0.9519, Val F1: 0.9674\n",
            "Epoch: 15/75, Train Loss: 0.0408, Train Acc: 0.9604, Train F1: 0.9722 Val Loss: 0.0526, Val Acc: 0.9550, Val F1: 0.9695\n",
            "Epoch: 16/75, Train Loss: 0.0388, Train Acc: 0.9638, Train F1: 0.9749 Val Loss: 0.0646, Val Acc: 0.9326, Val F1: 0.9517\n",
            "Epoch: 17/75, Train Loss: 0.0396, Train Acc: 0.9632, Train F1: 0.9744 Val Loss: 0.0551, Val Acc: 0.9443, Val F1: 0.9606\n",
            "Epoch: 18/75, Train Loss: 0.0364, Train Acc: 0.9677, Train F1: 0.9775 Val Loss: 0.0392, Val Acc: 0.9680, Val F1: 0.9779\n",
            "Epoch: 19/75, Train Loss: 0.0354, Train Acc: 0.9690, Train F1: 0.9785 Val Loss: 0.0388, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 20/75, Train Loss: 0.0317, Train Acc: 0.9751, Train F1: 0.9827 Val Loss: 0.0804, Val Acc: 0.9131, Val F1: 0.9430\n",
            "Epoch: 21/75, Train Loss: 0.0375, Train Acc: 0.9654, Train F1: 0.9760 Val Loss: 0.0400, Val Acc: 0.9694, Val F1: 0.9791\n",
            "Epoch: 22/75, Train Loss: 0.0369, Train Acc: 0.9663, Train F1: 0.9769 Val Loss: 0.0445, Val Acc: 0.9591, Val F1: 0.9715\n",
            "Epoch: 23/75, Train Loss: 0.0322, Train Acc: 0.9732, Train F1: 0.9813 Val Loss: 0.0482, Val Acc: 0.9536, Val F1: 0.9671\n",
            "Epoch: 24/75, Train Loss: 0.0406, Train Acc: 0.9608, Train F1: 0.9727 Val Loss: 0.0430, Val Acc: 0.9598, Val F1: 0.9720\n",
            "Epoch: 25/75, Train Loss: 0.0340, Train Acc: 0.9703, Train F1: 0.9795 Val Loss: 0.0377, Val Acc: 0.9701, Val F1: 0.9794\n",
            "Epoch: 26/75, Train Loss: 0.0337, Train Acc: 0.9726, Train F1: 0.9810 Val Loss: 0.0382, Val Acc: 0.9660, Val F1: 0.9766\n",
            "Epoch: 27/75, Train Loss: 0.0308, Train Acc: 0.9745, Train F1: 0.9821 Val Loss: 0.0370, Val Acc: 0.9694, Val F1: 0.9788\n",
            "Epoch: 28/75, Train Loss: 0.0288, Train Acc: 0.9803, Train F1: 0.9863 Val Loss: 0.0669, Val Acc: 0.9271, Val F1: 0.9518\n",
            "Epoch: 29/75, Train Loss: 0.0335, Train Acc: 0.9699, Train F1: 0.9789 Val Loss: 0.0414, Val Acc: 0.9674, Val F1: 0.9777\n",
            "Epoch: 30/75, Train Loss: 0.0305, Train Acc: 0.9757, Train F1: 0.9830 Val Loss: 0.0518, Val Acc: 0.9515, Val F1: 0.9656\n",
            "Epoch: 31/75, Train Loss: 0.0335, Train Acc: 0.9717, Train F1: 0.9804 Val Loss: 0.0384, Val Acc: 0.9667, Val F1: 0.9772\n",
            "Epoch: 32/75, Train Loss: 0.0296, Train Acc: 0.9765, Train F1: 0.9837 Val Loss: 0.0337, Val Acc: 0.9722, Val F1: 0.9808\n",
            "Epoch: 33/75, Train Loss: 0.0283, Train Acc: 0.9773, Train F1: 0.9842 Val Loss: 0.0342, Val Acc: 0.9729, Val F1: 0.9813\n",
            "Epoch: 34/75, Train Loss: 0.0280, Train Acc: 0.9800, Train F1: 0.9861 Val Loss: 0.0413, Val Acc: 0.9612, Val F1: 0.9726\n",
            "Epoch: 35/75, Train Loss: 0.0317, Train Acc: 0.9724, Train F1: 0.9808 Val Loss: 0.0513, Val Acc: 0.9519, Val F1: 0.9658\n",
            "Epoch: 36/75, Train Loss: 0.0283, Train Acc: 0.9773, Train F1: 0.9842 Val Loss: 0.0347, Val Acc: 0.9732, Val F1: 0.9815\n",
            "Epoch: 37/75, Train Loss: 0.0267, Train Acc: 0.9805, Train F1: 0.9865 Val Loss: 0.0333, Val Acc: 0.9701, Val F1: 0.9793\n",
            "Epoch: 38/75, Train Loss: 0.0263, Train Acc: 0.9822, Train F1: 0.9876 Val Loss: 0.0336, Val Acc: 0.9718, Val F1: 0.9806\n",
            "Epoch: 39/75, Train Loss: 0.0261, Train Acc: 0.9808, Train F1: 0.9868 Val Loss: 0.0333, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 40/75, Train Loss: 0.0248, Train Acc: 0.9828, Train F1: 0.9881 Val Loss: 0.0354, Val Acc: 0.9698, Val F1: 0.9790\n",
            "Epoch: 41/75, Train Loss: 0.0243, Train Acc: 0.9842, Train F1: 0.9890 Val Loss: 0.0372, Val Acc: 0.9701, Val F1: 0.9791\n",
            "Epoch: 42/75, Train Loss: 0.0266, Train Acc: 0.9804, Train F1: 0.9863 Val Loss: 0.0328, Val Acc: 0.9694, Val F1: 0.9787\n",
            "Epoch: 43/75, Train Loss: 0.0265, Train Acc: 0.9813, Train F1: 0.9871 Val Loss: 0.0603, Val Acc: 0.9357, Val F1: 0.9538\n",
            "Epoch: 44/75, Train Loss: 0.0253, Train Acc: 0.9820, Train F1: 0.9874 Val Loss: 0.0356, Val Acc: 0.9708, Val F1: 0.9799\n",
            "Epoch: 45/75, Train Loss: 0.0251, Train Acc: 0.9817, Train F1: 0.9874 Val Loss: 0.0439, Val Acc: 0.9629, Val F1: 0.9738\n",
            "Epoch: 46/75, Train Loss: 0.0249, Train Acc: 0.9828, Train F1: 0.9881 Val Loss: 0.0317, Val Acc: 0.9711, Val F1: 0.9800\n",
            "Epoch: 47/75, Train Loss: 0.0233, Train Acc: 0.9834, Train F1: 0.9884 Val Loss: 0.0345, Val Acc: 0.9718, Val F1: 0.9807\n",
            "Epoch: 48/75, Train Loss: 0.0246, Train Acc: 0.9819, Train F1: 0.9875 Val Loss: 0.0355, Val Acc: 0.9725, Val F1: 0.9808\n",
            "Epoch: 49/75, Train Loss: 0.0237, Train Acc: 0.9841, Train F1: 0.9890 Val Loss: 0.0339, Val Acc: 0.9725, Val F1: 0.9808\n",
            "Epoch: 50/75, Train Loss: 0.0229, Train Acc: 0.9856, Train F1: 0.9900 Val Loss: 0.0309, Val Acc: 0.9742, Val F1: 0.9822\n",
            "Epoch: 51/75, Train Loss: 0.0222, Train Acc: 0.9871, Train F1: 0.9910 Val Loss: 0.0364, Val Acc: 0.9704, Val F1: 0.9793\n",
            "Epoch: 52/75, Train Loss: 0.0229, Train Acc: 0.9849, Train F1: 0.9895 Val Loss: 0.0325, Val Acc: 0.9739, Val F1: 0.9819\n",
            "Epoch: 53/75, Train Loss: 0.0233, Train Acc: 0.9849, Train F1: 0.9896 Val Loss: 0.0331, Val Acc: 0.9742, Val F1: 0.9823\n",
            "Epoch: 54/75, Train Loss: 0.0224, Train Acc: 0.9848, Train F1: 0.9894 Val Loss: 0.0352, Val Acc: 0.9711, Val F1: 0.9798\n",
            "Epoch: 55/75, Train Loss: 0.0219, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0333, Val Acc: 0.9732, Val F1: 0.9817\n",
            "Epoch: 56/75, Train Loss: 0.0216, Train Acc: 0.9859, Train F1: 0.9902 Val Loss: 0.0356, Val Acc: 0.9732, Val F1: 0.9813\n",
            "Epoch: 57/75, Train Loss: 0.0224, Train Acc: 0.9853, Train F1: 0.9899 Val Loss: 0.0321, Val Acc: 0.9725, Val F1: 0.9808\n",
            "Epoch: 58/75, Train Loss: 0.0209, Train Acc: 0.9880, Train F1: 0.9916 Val Loss: 0.0320, Val Acc: 0.9739, Val F1: 0.9818\n",
            "Epoch: 59/75, Train Loss: 0.0216, Train Acc: 0.9855, Train F1: 0.9899 Val Loss: 0.0309, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 60/75, Train Loss: 0.0206, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0312, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 61/75, Train Loss: 0.0203, Train Acc: 0.9881, Train F1: 0.9917 Val Loss: 0.0307, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 62/75, Train Loss: 0.0201, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0339, Val Acc: 0.9735, Val F1: 0.9819\n",
            "Epoch: 63/75, Train Loss: 0.0204, Train Acc: 0.9891, Train F1: 0.9925 Val Loss: 0.0307, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 64/75, Train Loss: 0.0200, Train Acc: 0.9895, Train F1: 0.9926 Val Loss: 0.0304, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 65/75, Train Loss: 0.0200, Train Acc: 0.9892, Train F1: 0.9925 Val Loss: 0.0300, Val Acc: 0.9746, Val F1: 0.9824\n",
            "Epoch: 66/75, Train Loss: 0.0198, Train Acc: 0.9888, Train F1: 0.9921 Val Loss: 0.0299, Val Acc: 0.9749, Val F1: 0.9827\n",
            "Epoch: 67/75, Train Loss: 0.0195, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0301, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 68/75, Train Loss: 0.0196, Train Acc: 0.9895, Train F1: 0.9927 Val Loss: 0.0301, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 69/75, Train Loss: 0.0194, Train Acc: 0.9896, Train F1: 0.9928 Val Loss: 0.0301, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 70/75, Train Loss: 0.0193, Train Acc: 0.9900, Train F1: 0.9930 Val Loss: 0.0301, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 71/75, Train Loss: 0.0193, Train Acc: 0.9897, Train F1: 0.9928 Val Loss: 0.0303, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 72/75, Train Loss: 0.0193, Train Acc: 0.9895, Train F1: 0.9926 Val Loss: 0.0300, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 73/75, Train Loss: 0.0192, Train Acc: 0.9900, Train F1: 0.9931 Val Loss: 0.0300, Val Acc: 0.9756, Val F1: 0.9831\n",
            "Epoch: 74/75, Train Loss: 0.0192, Train Acc: 0.9901, Train F1: 0.9932 Val Loss: 0.0300, Val Acc: 0.9759, Val F1: 0.9834\n",
            "Epoch: 75/75, Train Loss: 0.0191, Train Acc: 0.9901, Train F1: 0.9931 Val Loss: 0.0300, Val Acc: 0.9759, Val F1: 0.9834\n",
            "\n",
            " 🔎 search 39 : deep_rescnn --- lr : 0.0023290654300914477, weight_decay : 0.01, batch_size : 128, pretrain : True, start_layer : conv5.1_weight\n",
            "Epoch: 1/75, Train Loss: 0.1244, Train Acc: 0.8546, Train F1: 0.8984 Val Loss: 0.1137, Val Acc: 0.8629, Val F1: 0.9113\n",
            "Epoch: 2/75, Train Loss: 0.0838, Train Acc: 0.9115, Train F1: 0.9389 Val Loss: 0.0836, Val Acc: 0.9093, Val F1: 0.9352\n",
            "Epoch: 3/75, Train Loss: 0.0758, Train Acc: 0.9196, Train F1: 0.9444 Val Loss: 0.0744, Val Acc: 0.9241, Val F1: 0.9464\n",
            "Epoch: 4/75, Train Loss: 0.0707, Train Acc: 0.9284, Train F1: 0.9502 Val Loss: 0.0875, Val Acc: 0.8997, Val F1: 0.9345\n",
            "Epoch: 5/75, Train Loss: 0.0639, Train Acc: 0.9346, Train F1: 0.9549 Val Loss: 0.0649, Val Acc: 0.9402, Val F1: 0.9596\n",
            "Epoch: 6/75, Train Loss: 0.0484, Train Acc: 0.9565, Train F1: 0.9697 Val Loss: 0.1045, Val Acc: 0.8677, Val F1: 0.9002\n",
            "Epoch: 7/75, Train Loss: 0.0516, Train Acc: 0.9513, Train F1: 0.9661 Val Loss: 0.0907, Val Acc: 0.8863, Val F1: 0.9269\n",
            "Epoch: 8/75, Train Loss: 0.0455, Train Acc: 0.9576, Train F1: 0.9706 Val Loss: 0.0502, Val Acc: 0.9625, Val F1: 0.9745\n",
            "Epoch: 9/75, Train Loss: 0.0491, Train Acc: 0.9532, Train F1: 0.9678 Val Loss: 0.0498, Val Acc: 0.9643, Val F1: 0.9753\n",
            "Epoch: 10/75, Train Loss: 0.0385, Train Acc: 0.9646, Train F1: 0.9754 Val Loss: 0.0413, Val Acc: 0.9670, Val F1: 0.9773\n",
            "Epoch: 11/75, Train Loss: 0.0375, Train Acc: 0.9699, Train F1: 0.9789 Val Loss: 0.0400, Val Acc: 0.9708, Val F1: 0.9798\n",
            "Epoch: 12/75, Train Loss: 0.0395, Train Acc: 0.9647, Train F1: 0.9755 Val Loss: 0.0438, Val Acc: 0.9674, Val F1: 0.9776\n",
            "Epoch: 13/75, Train Loss: 0.0354, Train Acc: 0.9724, Train F1: 0.9807 Val Loss: 0.0432, Val Acc: 0.9694, Val F1: 0.9791\n",
            "Epoch: 14/75, Train Loss: 0.0371, Train Acc: 0.9698, Train F1: 0.9789 Val Loss: 0.0689, Val Acc: 0.9179, Val F1: 0.9402\n",
            "Epoch: 15/75, Train Loss: 0.0374, Train Acc: 0.9653, Train F1: 0.9759 Val Loss: 0.0387, Val Acc: 0.9663, Val F1: 0.9765\n",
            "Epoch: 16/75, Train Loss: 0.0327, Train Acc: 0.9739, Train F1: 0.9818 Val Loss: 0.0381, Val Acc: 0.9704, Val F1: 0.9796\n",
            "Epoch: 17/75, Train Loss: 0.0324, Train Acc: 0.9723, Train F1: 0.9807 Val Loss: 0.0433, Val Acc: 0.9581, Val F1: 0.9704\n",
            "Epoch: 18/75, Train Loss: 0.0296, Train Acc: 0.9781, Train F1: 0.9847 Val Loss: 0.0352, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 19/75, Train Loss: 0.0312, Train Acc: 0.9756, Train F1: 0.9830 Val Loss: 0.0408, Val Acc: 0.9608, Val F1: 0.9725\n",
            "Epoch: 20/75, Train Loss: 0.0305, Train Acc: 0.9773, Train F1: 0.9842 Val Loss: 0.0359, Val Acc: 0.9687, Val F1: 0.9783\n",
            "Epoch: 21/75, Train Loss: 0.0320, Train Acc: 0.9711, Train F1: 0.9799 Val Loss: 0.0400, Val Acc: 0.9684, Val F1: 0.9784\n",
            "Epoch: 22/75, Train Loss: 0.0328, Train Acc: 0.9730, Train F1: 0.9812 Val Loss: 0.0344, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 23/75, Train Loss: 0.0307, Train Acc: 0.9755, Train F1: 0.9830 Val Loss: 0.0350, Val Acc: 0.9735, Val F1: 0.9817\n",
            "Epoch: 24/75, Train Loss: 0.0305, Train Acc: 0.9756, Train F1: 0.9831 Val Loss: 0.0451, Val Acc: 0.9557, Val F1: 0.9686\n",
            "Epoch: 25/75, Train Loss: 0.0359, Train Acc: 0.9677, Train F1: 0.9776 Val Loss: 0.0376, Val Acc: 0.9753, Val F1: 0.9831\n",
            "Epoch: 26/75, Train Loss: 0.0272, Train Acc: 0.9810, Train F1: 0.9867 Val Loss: 0.0332, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 27/75, Train Loss: 0.0297, Train Acc: 0.9757, Train F1: 0.9832 Val Loss: 0.0335, Val Acc: 0.9725, Val F1: 0.9809\n",
            "Epoch: 28/75, Train Loss: 0.0287, Train Acc: 0.9782, Train F1: 0.9849 Val Loss: 0.0356, Val Acc: 0.9687, Val F1: 0.9782\n",
            "Epoch: 29/75, Train Loss: 0.0264, Train Acc: 0.9813, Train F1: 0.9870 Val Loss: 0.0697, Val Acc: 0.9179, Val F1: 0.9401\n",
            "Epoch: 30/75, Train Loss: 0.0308, Train Acc: 0.9756, Train F1: 0.9831 Val Loss: 0.0359, Val Acc: 0.9656, Val F1: 0.9760\n",
            "Epoch: 31/75, Train Loss: 0.0277, Train Acc: 0.9789, Train F1: 0.9854 Val Loss: 0.0332, Val Acc: 0.9739, Val F1: 0.9820\n",
            "Epoch: 32/75, Train Loss: 0.0245, Train Acc: 0.9850, Train F1: 0.9895 Val Loss: 0.0306, Val Acc: 0.9784, Val F1: 0.9851\n",
            "Epoch: 33/75, Train Loss: 0.0293, Train Acc: 0.9762, Train F1: 0.9834 Val Loss: 0.0321, Val Acc: 0.9746, Val F1: 0.9825\n",
            "Epoch: 34/75, Train Loss: 0.0255, Train Acc: 0.9818, Train F1: 0.9874 Val Loss: 0.0364, Val Acc: 0.9646, Val F1: 0.9752\n",
            "Epoch: 35/75, Train Loss: 0.0263, Train Acc: 0.9822, Train F1: 0.9876 Val Loss: 0.0321, Val Acc: 0.9729, Val F1: 0.9812\n",
            "Epoch: 36/75, Train Loss: 0.0266, Train Acc: 0.9806, Train F1: 0.9865 Val Loss: 0.0306, Val Acc: 0.9756, Val F1: 0.9832\n",
            "Epoch: 37/75, Train Loss: 0.0249, Train Acc: 0.9828, Train F1: 0.9881 Val Loss: 0.0313, Val Acc: 0.9763, Val F1: 0.9837\n",
            "Epoch: 38/75, Train Loss: 0.0231, Train Acc: 0.9855, Train F1: 0.9900 Val Loss: 0.0318, Val Acc: 0.9701, Val F1: 0.9792\n",
            "Epoch: 39/75, Train Loss: 0.0259, Train Acc: 0.9814, Train F1: 0.9871 Val Loss: 0.0686, Val Acc: 0.9278, Val F1: 0.9523\n",
            "Epoch: 40/75, Train Loss: 0.0273, Train Acc: 0.9787, Train F1: 0.9852 Val Loss: 0.0342, Val Acc: 0.9729, Val F1: 0.9814\n",
            "Epoch: 41/75, Train Loss: 0.0246, Train Acc: 0.9827, Train F1: 0.9881 Val Loss: 0.0302, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 42/75, Train Loss: 0.0252, Train Acc: 0.9830, Train F1: 0.9882 Val Loss: 0.0305, Val Acc: 0.9753, Val F1: 0.9829\n",
            "Epoch: 43/75, Train Loss: 0.0222, Train Acc: 0.9861, Train F1: 0.9903 Val Loss: 0.0314, Val Acc: 0.9780, Val F1: 0.9849\n",
            "Epoch: 44/75, Train Loss: 0.0223, Train Acc: 0.9875, Train F1: 0.9913 Val Loss: 0.0309, Val Acc: 0.9777, Val F1: 0.9846\n",
            "Epoch: 45/75, Train Loss: 0.0218, Train Acc: 0.9864, Train F1: 0.9906 Val Loss: 0.0304, Val Acc: 0.9766, Val F1: 0.9838\n",
            "Epoch: 46/75, Train Loss: 0.0235, Train Acc: 0.9853, Train F1: 0.9898 Val Loss: 0.0362, Val Acc: 0.9632, Val F1: 0.9741\n",
            "Epoch: 47/75, Train Loss: 0.0225, Train Acc: 0.9844, Train F1: 0.9891 Val Loss: 0.0307, Val Acc: 0.9763, Val F1: 0.9835\n",
            "Epoch: 48/75, Train Loss: 0.0222, Train Acc: 0.9872, Train F1: 0.9910 Val Loss: 0.0312, Val Acc: 0.9722, Val F1: 0.9806\n",
            "Epoch: 49/75, Train Loss: 0.0220, Train Acc: 0.9866, Train F1: 0.9906 Val Loss: 0.0318, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 50/75, Train Loss: 0.0215, Train Acc: 0.9861, Train F1: 0.9903 Val Loss: 0.0314, Val Acc: 0.9797, Val F1: 0.9861\n",
            "Epoch: 51/75, Train Loss: 0.0218, Train Acc: 0.9857, Train F1: 0.9900 Val Loss: 0.0331, Val Acc: 0.9763, Val F1: 0.9838\n",
            "Epoch: 52/75, Train Loss: 0.0235, Train Acc: 0.9836, Train F1: 0.9886 Val Loss: 0.0291, Val Acc: 0.9763, Val F1: 0.9836\n",
            "Epoch: 53/75, Train Loss: 0.0207, Train Acc: 0.9883, Train F1: 0.9920 Val Loss: 0.0295, Val Acc: 0.9790, Val F1: 0.9856\n",
            "Epoch: 54/75, Train Loss: 0.0209, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0311, Val Acc: 0.9797, Val F1: 0.9861\n",
            "Epoch: 55/75, Train Loss: 0.0207, Train Acc: 0.9881, Train F1: 0.9918 Val Loss: 0.0298, Val Acc: 0.9773, Val F1: 0.9843\n",
            "Epoch: 56/75, Train Loss: 0.0204, Train Acc: 0.9885, Train F1: 0.9920 Val Loss: 0.0299, Val Acc: 0.9749, Val F1: 0.9826\n",
            "Epoch: 57/75, Train Loss: 0.0219, Train Acc: 0.9859, Train F1: 0.9903 Val Loss: 0.0290, Val Acc: 0.9818, Val F1: 0.9875\n",
            "Epoch: 58/75, Train Loss: 0.0211, Train Acc: 0.9874, Train F1: 0.9913 Val Loss: 0.0296, Val Acc: 0.9787, Val F1: 0.9853\n",
            "Epoch: 59/75, Train Loss: 0.0210, Train Acc: 0.9868, Train F1: 0.9908 Val Loss: 0.0297, Val Acc: 0.9808, Val F1: 0.9868\n",
            "Epoch: 60/75, Train Loss: 0.0198, Train Acc: 0.9883, Train F1: 0.9919 Val Loss: 0.0291, Val Acc: 0.9787, Val F1: 0.9852\n",
            "Epoch: 61/75, Train Loss: 0.0201, Train Acc: 0.9892, Train F1: 0.9925 Val Loss: 0.0301, Val Acc: 0.9814, Val F1: 0.9873\n",
            "Epoch: 62/75, Train Loss: 0.0196, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0299, Val Acc: 0.9818, Val F1: 0.9875\n",
            "Epoch: 63/75, Train Loss: 0.0197, Train Acc: 0.9893, Train F1: 0.9926 Val Loss: 0.0290, Val Acc: 0.9818, Val F1: 0.9874\n",
            "Epoch: 64/75, Train Loss: 0.0193, Train Acc: 0.9900, Train F1: 0.9931 Val Loss: 0.0294, Val Acc: 0.9801, Val F1: 0.9863\n",
            "Epoch: 65/75, Train Loss: 0.0194, Train Acc: 0.9895, Train F1: 0.9926 Val Loss: 0.0288, Val Acc: 0.9777, Val F1: 0.9845\n",
            "Epoch: 66/75, Train Loss: 0.0192, Train Acc: 0.9899, Train F1: 0.9931 Val Loss: 0.0285, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 67/75, Train Loss: 0.0191, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0286, Val Acc: 0.9794, Val F1: 0.9857\n",
            "Epoch: 68/75, Train Loss: 0.0192, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0286, Val Acc: 0.9811, Val F1: 0.9869\n",
            "Epoch: 69/75, Train Loss: 0.0191, Train Acc: 0.9899, Train F1: 0.9930 Val Loss: 0.0285, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 70/75, Train Loss: 0.0190, Train Acc: 0.9904, Train F1: 0.9933 Val Loss: 0.0286, Val Acc: 0.9808, Val F1: 0.9867\n",
            "Epoch: 71/75, Train Loss: 0.0189, Train Acc: 0.9903, Train F1: 0.9932 Val Loss: 0.0286, Val Acc: 0.9804, Val F1: 0.9865\n",
            "Epoch: 72/75, Train Loss: 0.0188, Train Acc: 0.9903, Train F1: 0.9933 Val Loss: 0.0286, Val Acc: 0.9790, Val F1: 0.9855\n",
            "Epoch: 73/75, Train Loss: 0.0188, Train Acc: 0.9905, Train F1: 0.9935 Val Loss: 0.0285, Val Acc: 0.9797, Val F1: 0.9860\n",
            "Epoch: 74/75, Train Loss: 0.0188, Train Acc: 0.9904, Train F1: 0.9932 Val Loss: 0.0285, Val Acc: 0.9801, Val F1: 0.9862\n",
            "Epoch: 75/75, Train Loss: 0.0188, Train Acc: 0.9903, Train F1: 0.9933 Val Loss: 0.0285, Val Acc: 0.9801, Val F1: 0.9862\n",
            "CPU times: user 1h 5min 31s, sys: 22.6 s, total: 1h 5min 54s\n",
            "Wall time: 1h 6min 7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history, f'experiment 3 ({len(history)} models)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "RbIW2BY_7chz",
        "outputId": "b6243101-6a52-4abe-cac8-c880d550d661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"21c47c13-ce36-4dc1-b254-4d9c6de263a1\" class=\"plotly-graph-div\" style=\"height:800px; width:1600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"21c47c13-ce36-4dc1-b254-4d9c6de263a1\")) {                    Plotly.newPlot(                        \"21c47c13-ce36-4dc1-b254-4d9c6de263a1\",                        [{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(238, 87, 140)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv2.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.13209427457904488,0.09305324126867084,0.08544039580830184,0.0811750283168217,0.07740122604206255,0.0782433141889168,0.07715055477364845,0.07252390398499892,0.0714695578212727,0.07027848217836906,0.07069097114524207,0.0675024694337468,0.06635253658477656,0.06332542332399206,0.0646661864472009,0.06464711410619685,0.06209221486032077,0.06087198500251579,0.05728122627305001,0.0601528128201541,0.057428940948854194,0.05234412707618832,0.049711585444273405,0.052920093227771005,0.05280469446213658,0.04935673330862497,0.05001423582861377,0.049803121762194445,0.050498604243631895,0.04546797524578656,0.046621641303750794,0.043898605437520444,0.04456657823964629,0.045222006557255404,0.04325607865366057,0.0427813437710786,0.041543937716014334,0.0413540372759411,0.04141387816044883,0.04212838831759844,0.04178626604403827,0.03966773358927901,0.040161529040008886,0.04101736070534891,0.03887786050876528,0.03787742473808078,0.037315444999032006,0.03797034706241896,0.037422309628979036,0.03698476543660699,0.036339264490213866,0.035033327275364394,0.03524343103324015,0.03498835414178661,0.03607875698023099,0.035161277330891236,0.034126982229408666,0.03385083282244779,0.034181051813962124,0.03397420245667606,0.03329798398370869,0.032939315635928584,0.03255129953652176,0.032603937472811,0.03232521040051459,0.032146824691508646,0.031972744692658914,0.031898926753222194,0.031724959879874635,0.03177334375909923,0.03165895065008945,0.0315137134214174,0.03147503909828805,0.03142048737283835,0.03138794642099673],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(238, 87, 140)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8966829457596623,0.940356299404105,0.9455844373412937,0.94856937657892,0.9519396738117234,0.9499816544347076,0.949906407886821,0.9546974610961734,0.9561779622189661,0.9558538335655892,0.9550019929225948,0.9579600641927781,0.9589219364303689,0.9614408015300044,0.956821007251078,0.9605854847621981,0.9605157479188505,0.9604020640027714,0.9648418976452681,0.9615359399316316,0.9615339727831231,0.9658198068589121,0.9685071725410239,0.9643292579645903,0.966171509674581,0.9670103368253012,0.9674041587756741,0.965791702614467,0.9660281324512806,0.9712091254297002,0.9690098830382577,0.9723067854919881,0.9707702963102814,0.9705619539070512,0.9718142514762226,0.9717647060030564,0.9727959833793781,0.9732244782609056,0.9738649371720679,0.9729956830660208,0.9742874389614876,0.9765637922288841,0.9735115640736023,0.9735463753413692,0.9773356205114238,0.9774049074655434,0.9790506887905784,0.977678331824931,0.9771471994516242,0.9775943690694328,0.9785505065059836,0.9797512577935112,0.9795163334698477,0.9799943591350475,0.9781880977462238,0.9802835773027392,0.9814828787475902,0.9805542488532476,0.9800896327769556,0.9806871416493931,0.9810365483250238,0.9816579766955416,0.9823388753794968,0.981841667455299,0.9811818086433431,0.9835160667655383,0.9827931725537998,0.9833755956149021,0.9827092017532703,0.9836853034468609,0.9836762151746868,0.9829591529964166,0.983611508738954,0.9837604312625029,0.9835765294927331],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(238, 87, 140)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.119037235059689,0.09519565418823478,0.08797239126106308,0.08727584706036905,0.1047439766350071,0.07746538726222474,0.07999481272042003,0.07964139041519656,0.0808825653392015,0.07753240027788169,0.07238996846671776,0.0717584459539951,0.06889972691888252,0.07121635467018868,0.07316542099329204,0.0672843813742559,0.06406407166285204,0.06538648823999457,0.06533821350110765,0.07046006805298664,0.06093386435734038,0.057878965963817544,0.060709750058315054,0.05670333131379688,0.0581308343054093,0.05448552522667495,0.06161301401267756,0.07123603834114534,0.058018630261683385,0.06106911361524739,0.05322598014346922,0.05038989395466457,0.05379092515385438,0.05299020001466331,0.05418705221047926,0.04827523071485287,0.05229362400434271,0.04824654429741332,0.04824736270144633,0.04986552332889583,0.050629716466382604,0.048658673329554064,0.04692664523048909,0.046898425076016034,0.04671197524646303,0.046161704333787114,0.04754001548917023,0.04482145405972946,0.044109414763671834,0.05003421238761178,0.043655088505486855,0.045873947111294444,0.04952619549437487,0.04833209141064755,0.04530496574247006,0.044094418447554316,0.043096335787850965,0.042969363943203207,0.043379202060068595,0.0424620132240438,0.042054883408587415,0.04220262810434263,0.04180597182606504,0.04163558140038625,0.041354509640190606,0.0419653788311375,0.04161732973102032,0.04174501333142474,0.04144503840875789,0.04100487176392906,0.041162266405587344,0.04106642072604284,0.04107812897036575,0.04103439630255666,0.041047667775981615],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(238, 87, 140)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9119139014005895,0.9360162428139399,0.9435869307081556,0.9452655949422707,0.9251370459889179,0.9555001184740303,0.9539595642863495,0.953480568101477,0.939811291249013,0.9556420875871906,0.9559392872568848,0.9586216570335324,0.9606134480292564,0.9594660194143684,0.9505427342804171,0.959447977918005,0.9619528192543539,0.9630771638753489,0.9628654748662999,0.9491832209164808,0.9634067802175986,0.9609346861860144,0.9554764990891412,0.9635585674924739,0.9604888893880102,0.9682366343561051,0.9547140026527643,0.9474467351423975,0.9631831206119896,0.963671195069904,0.9657496998224792,0.9725922122053141,0.9680829739558447,0.9630971918442479,0.9683496102938075,0.9746082149482831,0.9645207784366147,0.9684526478759088,0.9685105758889925,0.9736100770223641,0.9701302921355194,0.9707691079882527,0.9735922958665348,0.9730212436648988,0.9689169414918228,0.9728610262087086,0.9701153745466355,0.9742943899557039,0.9740734311626769,0.9716140512788347,0.9776170371778796,0.9739915827936015,0.9727408259343265,0.9682195607693822,0.9705472768602973,0.9786083268659025,0.9761884608102142,0.9745293292832611,0.977202198207208,0.978536829497513,0.9767594161080136,0.9780938682772315,0.9774822998004108,0.9801844418914457,0.9796792452281751,0.9787861024969633,0.9806475821847561,0.979393346948596,0.9786585725900319,0.979193558863004,0.9780578967064516,0.9794427266015885,0.9794427266015885,0.9794427266015885,0.9794427266015885],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(249, 148, 87)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv3.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10883409552967425,0.07194820598604207,0.05945116239775907,0.05381375636568321,0.054371730617129105,0.05126669911021219,0.04788343207046543,0.0454307098577454,0.04255206066611535,0.04156502185715843,0.040511486959672466,0.03904655254259279,0.04022904140842739,0.043325282353587784,0.039382269151961136,0.03672991002667674,0.03916724165314798,0.03924800032687897,0.03639821154358802,0.03873802641703228,0.037883219681893565,0.035715534595211484,0.03496593765974181,0.0350103523449936,0.03642746449831152,0.03530978218087967,0.032701433149570726,0.03368704861514313,0.03523655755857149,0.032148510378967855,0.032773136234024,0.03585644915341512,0.03127258013152994,0.029783449694514276,0.03120254837381915,0.031155517690506725,0.031399772253062445,0.030281369268621093,0.03058669505361361,0.02876657286460457,0.028413008447023876,0.030352298114000725,0.02736552116148524,0.029014121668940285,0.027678753773149482,0.02729907574629217,0.02680986258206635,0.028345440350310773,0.027717887137388174,0.026002327299848593,0.02629073951126664,0.026223592714802336,0.025980310155268662,0.025813173460011357,0.02523528382924619,0.0246212071319882,0.02443714641815506,0.024599150610919124,0.023941606397762945,0.024216539803828367,0.02389028092157117,0.023696153260605306,0.023944791416421037,0.023359709854354017,0.023466935584840087,0.023148995418984866,0.023127062087889252,0.022935338462580248,0.022937150282053467,0.022789362058222705,0.022846576656809104,0.022680595290568685,0.022634717391511554,0.0226061480907128,0.02258388232375748],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(249, 148, 87)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9192434272091672,0.9510628090047605,0.9590296164147026,0.9665313915711831,0.9631448005419054,0.9643859186490898,0.9671235368963443,0.9712400792311201,0.972944161814617,0.9743826609600947,0.9747991339917628,0.9765202784239972,0.9757064631781882,0.972791953974551,0.9761012373831833,0.9779035247244234,0.9753932281850145,0.9749279870756272,0.9788299748825926,0.976002705206106,0.977490933182322,0.9787502350203106,0.978802023386897,0.9788354743757666,0.9776121430968802,0.9773212961418677,0.9805491317965018,0.9801723197002205,0.9781766345934744,0.9818165823135374,0.9813948826845422,0.978281422688424,0.9833125997144613,0.985241904624404,0.9817964680110854,0.9826973368468435,0.9818157313346934,0.9827679913073812,0.9818201342554923,0.9848757089558364,0.9849385740787129,0.9830729863900143,0.9876128701113726,0.9840843964857109,0.9861139438684327,0.9862759874976238,0.9871899445593307,0.984597364778206,0.9858957228471614,0.9877528589622072,0.9870898327935832,0.9875349647598038,0.98771421338315,0.9873936382619961,0.9875940511717033,0.9893203681737279,0.9897075993442567,0.9891506809308745,0.9889883790732852,0.9897922758208275,0.9891467976326701,0.9899655636511414,0.9897503711859339,0.9903387577331885,0.9891296152874391,0.9905369763716709,0.9904068513418879,0.990294882831774,0.9897190829299786,0.9908775130495336,0.9907560599415035,0.9911289199517306,0.9908586166919355,0.9906372162060973,0.9904977963275626],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(249, 148, 87)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.07572749795270539,0.06462353615928762,0.08449154750690427,0.05606838396375941,0.06214715223951438,0.05548194654260304,0.052672507892974056,0.04746843394903383,0.04878531409190692,0.05091952865885705,0.04230646967529431,0.05226317178650001,0.05721664944874871,0.042740573001788656,0.05045590189468,0.04527119233999465,0.04554101136797892,0.04485235142287929,0.04821052485468871,0.062003024239925174,0.04148196108972084,0.04329210318548163,0.06205735799578047,0.03750817949317165,0.059198517613496975,0.0439555051374886,0.037088841865562494,0.0547356886654785,0.03807592209415747,0.04067397355470051,0.04073195094402713,0.03998443474833089,0.036484583666951384,0.043409742048823134,0.03469990253755727,0.04629941287728929,0.043469164251666706,0.036572047827374894,0.03439116967502738,0.03426448923257208,0.037153025094381315,0.0355869822690577,0.03749595299055896,0.03933882474182398,0.03602529878878512,0.03294569150995962,0.037959279514260306,0.034250745676534695,0.03370227883105835,0.03478808934541093,0.03550824297406419,0.03306627874982726,0.03560382350511158,0.03400010355456998,0.0324291697354456,0.031598232548261426,0.03276295858816183,0.03171877752976729,0.03470332874721268,0.03122565790345169,0.031972029149737145,0.03258082612171206,0.031206074136843797,0.03146434248108225,0.031237810146050763,0.031584618906282474,0.030961513334942847,0.03115859948277883,0.031180512436579183,0.031028524553243238,0.03095801758141452,0.030933034524987243,0.030971059518069336,0.030940304335552393,0.030952769607510353],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(249, 148, 87)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9548262909925141,0.9647067099736272,0.9391788585181212,0.9674910879840105,0.964061743195918,0.9673741968630735,0.964960604945529,0.9716899395319695,0.9716454719027464,0.9734634680771322,0.978686321947583,0.9715037594192034,0.96224781181706,0.9755281590176339,0.9735537577195026,0.9716356734061016,0.9695565001155714,0.9768418563271557,0.9719363137756656,0.9519050487038908,0.9772653858037821,0.9786762365746433,0.954736993509838,0.9793504060898254,0.956518095965966,0.9724734771622899,0.9772776182554026,0.9578314874869746,0.9790171997901377,0.9760115840742802,0.9783363647819744,0.9783429538744803,0.9804664165285888,0.9690211207431627,0.9799790963747081,0.9656860895770375,0.9694571204197814,0.9760083029249692,0.9800583373523593,0.980972531781425,0.974755470783783,0.9782585530102992,0.9784877755056691,0.9784237987861393,0.9779254911169318,0.9838939875334821,0.9734891768939115,0.9815022209190514,0.9815426727728156,0.9821610463481846,0.980002465587481,0.9826120531400444,0.9764816976653857,0.9789584476031314,0.9820217448383168,0.9840916913219092,0.9831472901187116,0.9838258178275048,0.9822866226467193,0.9826521179496506,0.9828696687543612,0.9809017152771965,0.9828864713218383,0.9833695201802003,0.9843523717495427,0.9841123676531023,0.9836065272278456,0.9848048465642747,0.983594084435301,0.9838602497876058,0.9838514778870153,0.9843328262524781,0.984816031418734,0.9845710093157177,0.9843406252233652],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(172, 58, 142)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv4.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.11536723797946724,0.08088962966480207,0.06454892056265511,0.059187490935997455,0.05478441034105499,0.04706452381194389,0.045512987455227256,0.04283477456350507,0.041026890598758116,0.036671529524241665,0.038090366316705364,0.03472419215999928,0.03547319672694526,0.03269036735268256,0.03308919797921372,0.03484527868435282,0.029627529855599927,0.030834507240667375,0.03084747154150427,0.02994338250788353,0.029506889485753005,0.027294686512716482,0.028462321998259182,0.028461479527912052,0.026625576061447324,0.02744261940849469,0.027760702754581028,0.027871199654583807,0.02705393825473108,0.02436985370125899,0.025620730328378943,0.02649684837866758,0.02607458116804577,0.025479579525535662,0.02545753997309989,0.024588469549040613,0.023229511376611146,0.02463483340346936,0.025822707836570608,0.02445346988323662,0.02258288580390869,0.022642615124719795,0.022965328207608625,0.021857521967815166,0.023245771986700144,0.023278003148435863,0.02261830063223122,0.021146197547498163,0.022129753386902644,0.021653534491191204,0.021412128110214607,0.02209204518474118,0.021255120482479243,0.020285612153996716,0.020264697933029745,0.020405948694628948,0.019918751671061108,0.01992924250363621,0.019311580963577876,0.019841048381326533,0.019454960494685196,0.019284721226660792,0.01920268070187424,0.018813234108275085,0.01919295459533407,0.018781195283320146,0.01875934132501773,0.018935798190935233,0.01869206964897092,0.018598199067135925,0.01851148294820477,0.018465956226674005,0.01843466215360513,0.01840795012562187,0.018390316260127267],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(172, 58, 142)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9165919930328217,0.9429495417973722,0.9567463377929877,0.9593115010836666,0.9639936413547288,0.9697488054939208,0.9704930051595246,0.9734605350435688,0.9750966796646751,0.9786126181861362,0.9760913698154778,0.9811600971364015,0.9765373200279653,0.9813612687282594,0.9816465637647755,0.9794952759441307,0.9845530831682259,0.9833654154593214,0.9829111514172247,0.9840809110284932,0.9842817375520453,0.9858721882752614,0.9837420948962216,0.9832591481451534,0.9883391250614497,0.9864623256030469,0.9854040539849345,0.9859575226332289,0.9863752735719319,0.9876101805329621,0.9878842749526221,0.9863772749995976,0.9863844718469575,0.9877773886394966,0.9881357010299621,0.9877252858747286,0.9889706479870559,0.988455924588307,0.9865177624048234,0.9878098950575182,0.9904894274093075,0.9896886641639844,0.9894359669670791,0.9913182531909462,0.9889404451050782,0.9886908170203919,0.9895068106130256,0.9908532031120091,0.9899171088271501,0.9911969162042892,0.989925654630644,0.9904877676678308,0.9903598912813689,0.9918639099722867,0.9924570405440195,0.9906788506439308,0.9927094925701906,0.9923753766785615,0.9926066807610386,0.9922054070527241,0.993141928193769,0.9928231741629986,0.9924145626937955,0.9932369286478456,0.9927427050594264,0.9930708270146781,0.9935642297218374,0.9928200986615955,0.9929968172995148,0.9932211364065486,0.9937177542138076,0.9934405284083881,0.9937420350584589,0.9935166410587662,0.9936885187086899],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(172, 58, 142)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.08787121142923217,0.07000647679841805,0.06821877273292476,0.057987527882110625,0.05589175362050328,0.05583047340416007,0.04539700779052534,0.04466416510622116,0.045425249674578304,0.054007041065143965,0.06383681460139677,0.03875041152398611,0.03881227757359288,0.03977902552106536,0.03617806149665842,0.03935431299596718,0.044835204784710384,0.058548327837510614,0.03494971692050036,0.04848856426698645,0.03174178075186166,0.03346258599528742,0.0413100740830718,0.03656302371487995,0.031226002596497944,0.043144855796778736,0.030323999385346252,0.029486396586157614,0.030818313588084224,0.029923588029809833,0.03278587097718134,0.048591533392872595,0.03510291630408608,0.033598361891467136,0.033126519978865725,0.030804588277008115,0.0313809090105119,0.03150334734584867,0.03146308887353058,0.02915484637687706,0.02900905098343633,0.031992883788248924,0.027993816087387272,0.030045300143486036,0.028693639156744652,0.029143877624441258,0.029095677698600744,0.030240617477402244,0.028524254798837953,0.02967960458389672,0.027835694184571606,0.027217860532687703,0.030085516557148643,0.027172928999891807,0.027397515442977655,0.027320276991915457,0.029405281673540774,0.027249989657313962,0.027539813243287945,0.026758116941988673,0.02682951729859888,0.02715884432247824,0.02767219603752967,0.027157276401558694,0.02663231939973495,0.027669621036224758,0.02680937431086994,0.02737187547321172,0.026730422466942124,0.02661559867618215,0.02661872389006246,0.02658506344197337,0.026590473906383482,0.02660874647065946,0.026606437144000916],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(172, 58, 142)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.94536034849028,0.9619788451644504,0.9529762419284562,0.9655440230186874,0.960796295948794,0.9583235175896576,0.9747624956728029,0.9738428184160642,0.9665595172902305,0.9647296615571501,0.958263970764716,0.9760845263104965,0.9806177762928812,0.9768400273526683,0.9836190078464885,0.9793297847240773,0.9740219339982581,0.9571341537295396,0.977875509237715,0.9657859375632716,0.9833923131157529,0.9775640846067518,0.9773202820304118,0.9796063523048456,0.9831398998223084,0.9691543651801442,0.9842046682302518,0.9850695110051537,0.98163764556958,0.9840473898159721,0.9818730182054386,0.9655080180916903,0.9791241084443495,0.9821351219054719,0.9816723879205042,0.9827486687750018,0.9850913028929703,0.9830593298347486,0.9844372995924346,0.9850924107440667,0.9835714666935159,0.9820535310768547,0.9853339086979739,0.9825677390860411,0.9841197209698285,0.9837825808730525,0.9840077042643187,0.98475032151514,0.9837839625514054,0.9832922552231486,0.9859861488584601,0.9855220047539095,0.9828154514237467,0.9852596275220544,0.9861997256498236,0.986026507564745,0.9817939367142556,0.9860547444471047,0.9847344837986142,0.9864772467254864,0.986199130713823,0.985755743127591,0.9855630053519037,0.9859943394447401,0.9852533793178411,0.9855586215140354,0.9852717236320493,0.984738925186284,0.98548246083501,0.9855055052729892,0.9857365055328121,0.9857365055328121,0.9857365055328121,0.9857365055328121,0.9857365055328121],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(157, 66, 231)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv5.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.13849586296020094,0.0795310932123033,0.06717326368790927,0.056703683334527556,0.05086576184439222,0.05223214162315187,0.04547159712748839,0.039803418759064575,0.03790534419121748,0.03778008580002998,0.03570262960756767,0.033906542504673565,0.0312978002452905,0.033874980335519625,0.029449931691098802,0.03340841261766006,0.029713608356958677,0.028837369140856318,0.02887879139215676,0.026829755391588463,0.027333754636048587,0.029191768556654042,0.026998875064845756,0.02803623610414044,0.02409479000775265,0.02586158917617279,0.02532636135014335,0.026605956088260822,0.02824434123480989,0.024164888589618405,0.024054176696306277,0.02414149610839895,0.02370658445834499,0.023371180049639798,0.022466181716182934,0.023002111720601855,0.022370160286147297,0.022226238954398368,0.022135910323531904,0.022711753185074627,0.022639492200207466,0.021603840099005425,0.02162193222588007,0.021857725580533345,0.02256317537487031,0.021332261799904013,0.021048360740401083,0.02031772295258436,0.01998202129942445,0.02003020186228987,0.02017253394551225,0.019879267985408726,0.019408407585454288,0.01956879770530458,0.01942882200602745,0.01910543803932126,0.01942204886755105,0.018743120721047415,0.019018326048981418,0.01858076257215128,0.018670752694038728,0.018482681844310934,0.01843397785383538,0.01818376308582288,0.01826566090468074,0.018083272695558452,0.017970521765988307,0.017993279189652288,0.01783470278957865,0.017748361109582042,0.01776933061067796,0.01769168142310396,0.01766296310813293,0.017641208698505797,0.017627108013438742],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(157, 66, 231)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8817408318752193,0.9460290032492001,0.9530240876721685,0.9615022333907923,0.9671051503031832,0.9640613422309866,0.9708008310169463,0.9774107891453124,0.9777570632966084,0.9768179975889472,0.9789280343376313,0.9797020734846397,0.9819978427804651,0.9805246236214572,0.9837176269113874,0.9810709561978038,0.9840776057701414,0.9839161233399554,0.9841833955859023,0.9866730158469226,0.9857119112563545,0.9832713727286602,0.9849401779620454,0.9837021591170895,0.9880925792383309,0.9859989866622288,0.9868836357208941,0.9853752848238366,0.9834022238065026,0.9893589765441895,0.9879611091231459,0.9879992114574592,0.9878083578030429,0.9891159511237653,0.9900653295371694,0.9895450510110239,0.988828878711942,0.990930001249298,0.9895639521779028,0.989705996909523,0.9897080156284433,0.9900395436945251,0.9894182622345272,0.9902286501839376,0.9896078499306831,0.9901112113864485,0.9915931447105818,0.991475380392785,0.9917233893587732,0.9921778313055877,0.9906766575493127,0.9914175197884412,0.9918230852378452,0.992494928904477,0.9918212334626973,0.9928461405396688,0.9922109446809945,0.9935131519223216,0.9923864627381876,0.9929686852624613,0.9931442625164253,0.993938387610683,0.9935491125454722,0.9936279147502997,0.9938084848847857,0.9935892673341797,0.9933075073382072,0.9937759944622259,0.9939864419718288,0.9941247816380967,0.9940903630071616,0.9940812368463855,0.9940132746804186,0.9942377659006066,0.9942860760957479],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(157, 66, 231)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09404042819418858,0.07570056939862438,0.07435092660262413,0.05476574184200198,0.07767874480215545,0.05462111806900231,0.045492497611394046,0.045010557297066725,0.042254887024561566,0.0533302028052176,0.05118339220733987,0.039820001266666294,0.04959945271105291,0.03715302160720235,0.04615789111639626,0.03495908309145482,0.03481897521264774,0.03756868684107496,0.03465377943161427,0.0338348544750017,0.06336539554124845,0.03863304871873757,0.03472021094609782,0.032888088431657386,0.035431050406800925,0.033740784599096914,0.03932300059451271,0.03198667873221984,0.03118156358292422,0.03252967163357128,0.038941309744447365,0.03530161426597854,0.03190514010559652,0.03310808313312809,0.03163821004407922,0.031042858410332212,0.033085593719457844,0.035129504755805036,0.030673827151559883,0.030918041941841033,0.03350071771089563,0.03411350883927542,0.03442593361555096,0.030075874563652214,0.029670571930117625,0.030134735799224925,0.029616581815010083,0.030342137501207005,0.02876349443832214,0.03033095580810534,0.029189430835218366,0.030165660317308714,0.02891931306711587,0.030285606407832445,0.030099135240431094,0.030116253008240276,0.029026583897084304,0.02890675012086265,0.028227630293451225,0.028899063858686853,0.029105167457020978,0.02834675786984745,0.028593039177230134,0.0287270751503325,0.02832151646517806,0.02855252579008181,0.028262949868063745,0.028400983566680724,0.02821681637739398,0.028204492753518814,0.0281928120686631,0.028175209693072998,0.028189809446277487,0.028196833024115086,0.02818951136416586],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(157, 66, 231)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9340343603841478,0.950045101141584,0.9521895951791015,0.9670915097131039,0.9422790360057319,0.9710683799199249,0.9765209232205043,0.9739565998364136,0.9767711568803004,0.9619512583642038,0.9642200939837211,0.9809804264416098,0.9643699640834202,0.9807316812716846,0.976004276971097,0.9826964087325343,0.9834641924064474,0.9813382317968641,0.9836705256946453,0.9814794763707954,0.9581956263390641,0.9766873199117686,0.9813416212513947,0.982637809341692,0.9815607655372022,0.9837319836595771,0.9800863532714134,0.9841183605137714,0.9851012998941501,0.9835640087033914,0.9812489104051226,0.9825862581027878,0.9843721952267898,0.9821338893662263,0.9852841673760296,0.9841618403401933,0.9811612878430461,0.9793505239204381,0.9848143372811703,0.9848616655596343,0.9798647438182984,0.9793286957404489,0.979838110356055,0.9867356042536737,0.9855365013800104,0.9857985963549949,0.9840419185002586,0.985730165937353,0.9847735282708877,0.9840430778432854,0.9858080445627833,0.9853263066125962,0.9864727574443274,0.9858076150745756,0.9837681500125839,0.9839957324719697,0.9860234130869742,0.9860299864045228,0.9857490900991075,0.9858160893177247,0.9860107243384203,0.9864718402314565,0.9859873462089361,0.9859873462089361,0.9867181378887335,0.9871780801632493,0.9867022827622128,0.9867373203537839,0.98622812108568,0.9869544287402856,0.9859974206819979,0.9864789490533774,0.9867081785360019,0.9867081785360019,0.9867081785360019],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(78, 158, 95)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv2.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.13294998384832926,0.0955932106596566,0.08714129750500567,0.0828835312938499,0.08152214193808391,0.07805404504942456,0.0725216462918027,0.07197170072370651,0.06840008032793032,0.0681396100478074,0.06340832692002377,0.06321493067692235,0.0622195295776152,0.05848206596263393,0.05898528669332447,0.05917011624861419,0.05930567071789114,0.05225477500777064,0.05420173504917892,0.04939453430209236,0.05096295160014195,0.05146712058000821,0.05237375930941665,0.05597569742597392,0.05020945495483119,0.048277440397463574,0.04754500196988504,0.04536469860892116,0.048193344480666374,0.04780701321698,0.04856537240562707,0.04716302847056591,0.043481771988454966,0.04855725419924158,0.046784538811527165,0.043601047583692466,0.04147507306721748,0.043416385703377705,0.04004555953771115,0.04160100360148142,0.0436168535289281,0.04339325778092442,0.04084943701371226,0.03901919916929317,0.03896006443468429,0.039689105757664704,0.03748288290214566,0.038058327558319185,0.037564667656949435,0.03576423173123824,0.03604093447865284,0.035561250046083334,0.036521733526614114,0.03646436541555673,0.03644745115357166,0.034749570868286346,0.03401148673253644,0.03439353540334915,0.033937107953418984,0.03398607164143424,0.03376690185023952,0.033445035144772046,0.032989668615700046,0.032984657746068666,0.03305964860446401,0.032819614230835424,0.03272479201015192,0.03247291081822888,0.03234145330501995,0.032470723675118,0.03212877015272776,0.0320753002042145,0.0320325988589147,0.03199727791366845,0.031961031777854636],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(78, 158, 95)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9054511295383018,0.9396723001328017,0.9477945203233212,0.9476322094250389,0.9468442619930886,0.9516917169483571,0.9540534529909177,0.9509446985416713,0.9553645676739272,0.9556886190021836,0.9574278359500206,0.9576158791417055,0.9588723898958299,0.9601946812701712,0.9586319586712851,0.9608607208568287,0.960201539811761,0.9654490782333417,0.9643059687679306,0.9683430842908971,0.9662397659271161,0.9649349298939667,0.9646632737132756,0.9622443836659738,0.9660322756398738,0.9689117487301762,0.9685566334422091,0.9715645472196164,0.9700805988029597,0.9687778294508995,0.9678948678052589,0.9714271800565136,0.9722794889274382,0.9683441430688638,0.9709964132795129,0.9722016459453097,0.9737110886446327,0.9724073467708058,0.9753149484730971,0.9736293387802095,0.9719392106395237,0.9710149729488081,0.9752243546400217,0.9757759097828829,0.9760410152488346,0.9761774714346731,0.9785257482316982,0.9772154310112672,0.9779809248179371,0.9796655402092243,0.9782946986295836,0.9795463348014379,0.9795949294138653,0.9786016914732938,0.9785302050382333,0.9793831670680649,0.981947829791962,0.9803983851861852,0.9812230060802166,0.9817173396905802,0.9814527545781954,0.9822477398192448,0.9819363088306229,0.9824074375575553,0.9828536495149416,0.9823047809782257,0.9833032049852976,0.9840156712676227,0.9838243053056283,0.9837855029206407,0.984060575725895,0.9839715307588746,0.9839922043951576,0.9845895519937525,0.9839532487543465],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(78, 158, 95)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10377451170658328,0.10112240406655773,0.09306916828008042,0.08436920193769677,0.08376265650045421,0.08345239787679358,0.09864657546851234,0.07981143112444795,0.0790482338402689,0.06980243247399215,0.06880321339848115,0.06875044813066004,0.07369773743795775,0.06636023172193377,0.0697598655250474,0.06962623436938446,0.07176442868828364,0.0606625992100673,0.057176589796838075,0.05729569305771405,0.057607635067090955,0.06151071827333817,0.05690661594015626,0.06351450362156347,0.06000037444211363,0.05755605001732246,0.05703260903710762,0.05473939990208731,0.06210603782606289,0.05132373365544781,0.06625686495267238,0.05264711256596641,0.05172815432458399,0.0558898729608231,0.050338390564283554,0.05213652821033681,0.049942136806823134,0.05650171993421935,0.048054179674673736,0.04935818617952239,0.05309231847371023,0.05216001951202904,0.04775949145868882,0.05135434004398146,0.04699294661584589,0.05476052401094502,0.04862483933158347,0.04862091911002942,0.04499911281609863,0.04595020724735719,0.04616222108403842,0.049277620401579084,0.04913748551377726,0.04611226742722325,0.045416182896628,0.04425836916488061,0.04456243457919134,0.04348082167330067,0.04430362663113375,0.04344111397043127,0.04334828781149641,0.04308709404927349,0.043058341582821,0.04306775894515293,0.043253468965337036,0.042615218342784346,0.04291790244249544,0.042907244778683094,0.0426254152841994,0.04272025848940476,0.042563459430773234,0.04247026978591873,0.04240136138608366,0.04246501703954644,0.04247735660920028],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(78, 158, 95)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9337591317385974,0.9353292002622526,0.9425940223406948,0.949711261278863,0.9548358162076559,0.9489132628951292,0.9282974364503154,0.9432220250748187,0.9502663091639141,0.9622802188593743,0.9613431740350789,0.9549387482016939,0.9494570187343071,0.9604528908936689,0.9522675579102062,0.9559408580176946,0.9507233441372182,0.9609330760839302,0.9657156197850248,0.9625136367648841,0.9623942984089274,0.9604922579382503,0.9649435824594202,0.9611174099304866,0.9620666739384247,0.964614150822322,0.9658090404312231,0.9663036131723152,0.9629962931249516,0.9674792108149639,0.9569353752825107,0.9629228362212469,0.9692735945911956,0.9675965928671617,0.9675182011973457,0.9691272534917352,0.9707845034455697,0.9636839875207742,0.9727914331442543,0.9737882788894654,0.9697154749735074,0.9714312927518581,0.9670133167305346,0.9693903682552334,0.9734531831036056,0.9630374195574389,0.9698513985212802,0.970972423017836,0.9747301616406934,0.9719564550075659,0.9736820408071513,0.9715464028914139,0.970856311907313,0.9739651040385376,0.9759293917042497,0.9765344005369342,0.9769950283041275,0.9769966221311849,0.9743196669266944,0.9748952499970575,0.9765970574050217,0.9763002463562865,0.9760050371507177,0.9764646033595467,0.977526174329879,0.9775013518007778,0.9764116953609775,0.9761986609212598,0.9777539277857742,0.9764209386047681,0.9773290555703051,0.977555335752787,0.9777539277857742,0.9770801626739894,0.9770801626739894],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(238, 171, 197)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv3.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.1080227405225425,0.07212420836654315,0.06358995907870628,0.05663505106063438,0.05302694944957141,0.05476948967478734,0.049683294700456515,0.04935750221622359,0.04655859779656138,0.04886488028869186,0.04461117093915382,0.04649418459571514,0.043715103828313014,0.0414228396270454,0.04032591189015635,0.03960643208160024,0.03875122883688885,0.04209874046524775,0.036684344045365525,0.04136907922921175,0.040212452860564026,0.03528926350423833,0.03470710138582282,0.0353258745111141,0.04166061945989267,0.0376247587217976,0.0359099895786174,0.034080067400704274,0.03712870720624582,0.033712314134132955,0.03474978363655618,0.03200843683748004,0.03256358760115685,0.030916913887667218,0.03239339974251195,0.033242599465558895,0.031041215380647337,0.0332451043473418,0.03155995938000947,0.03066609074219737,0.031499883447606264,0.029764749463924975,0.03202309110559275,0.03193857541429423,0.028570143399676916,0.029105319767200388,0.028967793172765162,0.029961915582179477,0.0299131153068182,0.02987294351001444,0.02863552239291447,0.028451579257862947,0.02802573595908636,0.02810690994934528,0.027180792165990546,0.027014801062788613,0.026667104878666612,0.026601628819213838,0.026543537639550873,0.02628879998762583,0.026119213750562444,0.026064993728716075,0.02630169603115914,0.025907883503370813,0.025752559826119386,0.02597986391136361,0.02564000203514222,0.02541576250790184,0.02546747029083701,0.025431403462035755,0.025353705726845567,0.0252971651060304,0.025237309809385843,0.025223567220942465,0.02520507112239303],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(238, 171, 197)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9221406872781344,0.9532369885248501,0.9576538420382575,0.9621807935955161,0.9653110053960698,0.9610138085259242,0.9683743707363675,0.9680717283172036,0.9714025946788974,0.9678970673876803,0.9716767986423855,0.9689337373153649,0.9724141467689966,0.9744795622061787,0.9755459426399756,0.9757530672774489,0.9761566436711221,0.9732974344934783,0.9781151031388642,0.974999589834734,0.9762053242092905,0.9788188636398556,0.9799425103267109,0.978785835224501,0.9722253937915012,0.9777962100365065,0.9788488769077679,0.9802300640509208,0.9792069805698994,0.9807658635140855,0.9804686131170821,0.9834741018079106,0.9804598331849346,0.9830510700238366,0.9806929353430182,0.980951063122696,0.9833833303791832,0.9811781959720037,0.9817945161976024,0.9830406260635998,0.9830105161391061,0.9851143137690962,0.9830234575915174,0.9810805684619336,0.9854895976251766,0.9838522528549536,0.9844530212619359,0.982872839342114,0.9836727165579932,0.9833860718388439,0.9842476571485088,0.984700753265577,0.9859337460946422,0.9856909282743437,0.9866368906082672,0.9872167852504683,0.9869855010670963,0.9875884990050653,0.9871971181931557,0.9873284631854574,0.9874744217958339,0.9876469673138688,0.9868043031369714,0.988044634012942,0.9878500997810334,0.9871707658119218,0.9877293936883982,0.9881306011902214,0.9879655158815689,0.9879975854580018,0.9882822433883223,0.9880353018241234,0.9881939336355576,0.9880392146561154,0.9882602370588777],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(238, 171, 197)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.08092346713714993,0.08174547689272366,0.06133310518313929,0.07044811837144734,0.06350744545357334,0.05729156345692287,0.057712526127849655,0.05169763913525339,0.050852377638476824,0.053999832694985206,0.06218450737163373,0.05169346913746542,0.04931216075299532,0.05072792844264368,0.046242024296337794,0.053317193341316634,0.050629411374068345,0.045913156196218996,0.04650823410434002,0.04892907290882671,0.057640238060164696,0.045623329678659175,0.04465479340749918,0.04263431232717029,0.04342141662215449,0.04898745931454541,0.04350465967534334,0.04110013028395545,0.05292464479599212,0.04238199505097268,0.045102657362357856,0.0417000556345453,0.03757101981756614,0.04496033696067292,0.04119642783225197,0.037612820293792745,0.04237930256220483,0.042307964860880905,0.04553316756314838,0.038928495373103215,0.04183580533079675,0.03782880032543874,0.0408096273279272,0.03894181743110578,0.037002619432727084,0.03775114091401248,0.042523422450646504,0.039560541971442625,0.037128414062597495,0.03593599206393527,0.0370109252196407,0.036326690246047025,0.03744094745092785,0.036494573912362466,0.03744489255895729,0.03530891666195237,0.03503173978877641,0.03584969866623993,0.03523358721862134,0.035024043758915054,0.03453516102175123,0.03451979994978692,0.03476305433462575,0.03683269988630236,0.035864943959766235,0.03427263598159416,0.03430262000085562,0.03437828498146788,0.034326443690614605,0.034291405768431336,0.034390706832671085,0.03417680668052529,0.03422062798720045,0.03422985708283395,0.03423174421248567],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(238, 171, 197)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.950664206762829,0.9354203103084042,0.9650394860818088,0.953914785011998,0.9611651493097423,0.9692201618428407,0.9673647348227619,0.9692430823511841,0.9681146489310416,0.9686180671133203,0.963440433687933,0.9685226422764641,0.9694385002566347,0.9701741569105721,0.972301987516834,0.9666482234286556,0.9633078178757924,0.9720798591905051,0.968015364053813,0.9742696331253393,0.967187027989851,0.9738209172766571,0.9765435886529729,0.975540146930828,0.9732171166446986,0.9717708452060694,0.9774433838420297,0.9775352808631513,0.9662750924332244,0.9736287642752701,0.9756584959843595,0.979796268142221,0.9796637816340505,0.9733607045158309,0.9749516363888351,0.9794954822153968,0.9752099519479283,0.973539833360103,0.9700273190501133,0.9755644351695519,0.9788954848543272,0.97968206239188,0.9747446602332842,0.9784647155061943,0.9810171874161687,0.9758818781485572,0.9745657626193885,0.9733731600581325,0.9790885795287182,0.9788105367189881,0.978364144947012,0.9813166279664534,0.977738652963188,0.9802173092200845,0.9755686882564241,0.9815288304231761,0.9815064165231657,0.9792547495120989,0.9801658886568919,0.9805992173564573,0.9824346291016118,0.9810671562319416,0.981266978134249,0.9788411497402815,0.9799698780296854,0.9808322556217357,0.9815337748266961,0.981485875319789,0.9815433199628246,0.9817312737017315,0.9815433199628246,0.9815227291690597,0.9817635293935957,0.9817635293935957,0.9817635293935957],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(160, 186, 214)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv4.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.14195465028354257,0.08649339309263065,0.06951067264256881,0.06092343385686579,0.06040893023127406,0.05367093431546789,0.04665216393982149,0.0460759481666919,0.04193298515243902,0.03948784258263559,0.03836989943684854,0.03978044105776909,0.039196924820439925,0.03466779342836531,0.0317643374539613,0.03592024747098776,0.03227490700252209,0.030762486749959156,0.03125592106974599,0.030797754243903828,0.02992706789047311,0.029847428626791583,0.03202220308855774,0.028085928496489682,0.03166063227493397,0.028548252987936736,0.02673590572599966,0.031922520689862566,0.026573674736666105,0.028183705576045815,0.026046565678589397,0.02655301424722696,0.026039428536578552,0.025252473635976668,0.02645673711740445,0.024466650212719133,0.025412765399628626,0.025953927955335222,0.026306132449931706,0.024022410141745793,0.023907909468072388,0.02379887231539205,0.025784203914535005,0.023862858739215346,0.023755522589503284,0.022895807157540513,0.024167566844619563,0.02354715915027694,0.023244995831214275,0.02225334625627167,0.022841545615136145,0.022531772725164618,0.0221985058005545,0.021382615924786182,0.021600602475877206,0.021524228007540124,0.022010385767180143,0.021022959270291585,0.021398363309781166,0.02153794446454425,0.020841396143947987,0.020754634430642665,0.0204383345352825,0.020522577341599017,0.020403689251015687,0.020421824995577677,0.02034028839322078,0.02025268479396524,0.02023772990324653,0.02016646324871332,0.020096804153809978,0.020065458062908187,0.02004485897984428,0.020025889000685766,0.02000714099079473],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(160, 186, 214)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8880990784930346,0.938590204153053,0.9542437559206184,0.9613257474520185,0.9599550586998756,0.9655552916254478,0.9723048516618944,0.9707381685053652,0.9734669919541843,0.9770284539678682,0.9771880101097958,0.9762244555604815,0.9761845684024409,0.9803586664112467,0.9841740350340192,0.9788279595441491,0.9836345225224463,0.9845641973468656,0.9838032782433155,0.9838380725317026,0.985204358268924,0.9848832955535517,0.9823159371404603,0.9862768030173211,0.9825468337863013,0.9856552497007629,0.9876055654120587,0.9816235337752063,0.9883343734668124,0.9851809397163546,0.9876524766108828,0.9881048798358113,0.9871528646864722,0.988593629751975,0.9866279902137391,0.9894170341631725,0.9886568185530374,0.9879666669721218,0.9863755876994693,0.9894367171918022,0.9898430851393437,0.9896628006144946,0.9881807265743761,0.9895952615096827,0.9892186389418741,0.9907374042362757,0.989345775620732,0.9893091962555188,0.9898150030233511,0.9909021400703043,0.9903538413040884,0.9898472627708579,0.9907235072424259,0.9915968568050421,0.9910900673692526,0.9911405090330881,0.9909551328425922,0.9920190630556499,0.9926242250616711,0.9906007778339447,0.991970442142383,0.9916821635753179,0.9916511252960424,0.9919510440124086,0.9921534592410802,0.9920210275344914,0.9926083880455983,0.9922041323635444,0.9922943157519338,0.9924299434753906,0.9928360200832735,0.9926780976469088,0.9925347684675673,0.9928326954755536,0.9927582509977005],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(160, 186, 214)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09883004260022206,0.08842658192124153,0.07218150055592822,0.06409868663016872,0.05937792422230711,0.053603290326099624,0.06098722988080323,0.04857656756524777,0.05462077596445673,0.05209670227110591,0.05413806839394815,0.04632725920464165,0.04186159491129347,0.04702140732165874,0.07777455405066513,0.03912003563390565,0.04116357456111826,0.03934369892189183,0.053655619624861324,0.05021729691536566,0.03924782352195572,0.03512521967599072,0.0388695754930121,0.039911100037933626,0.040541269740288204,0.03599707967124854,0.0352151515634404,0.03453676994006658,0.03903748740240471,0.037350196232603175,0.03369129951979286,0.03819492151032609,0.03546245620422757,0.036190692780865834,0.03388941375045842,0.03622582625226466,0.033383863521707836,0.03396527706757444,0.03353782159970798,0.03411006021643013,0.034984064294198126,0.033476249507509964,0.03545609307043331,0.03523574499381367,0.03293388055003795,0.033207723063087134,0.03256136186907382,0.034989450628712414,0.031869281621016175,0.03421481029735398,0.033374232020472336,0.03240179579026511,0.031665902454213996,0.032357222991403436,0.03292618240789859,0.032089289295714335,0.03268833089223023,0.031580605652324116,0.03169249487445527,0.03234722938426991,0.031325002557428434,0.03149875679222988,0.03170468484874034,0.03103665347463896,0.031424377932888534,0.03148031595236657,0.031244001033175033,0.03114098667893623,0.03119220837080192,0.031056484754142893,0.030974659024132895,0.031065660126225646,0.031073358695941283,0.03102038008036073,0.031021478142320494],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(160, 186, 214)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9308202047514148,0.9352959220329959,0.9560724698333484,0.9612183457235094,0.9649445025868888,0.9682402883566433,0.964815520034581,0.9702666808669721,0.9709630554688747,0.9696197847679248,0.9667272223123068,0.9743071435018996,0.9737129097401754,0.9752853921465253,0.9424675861539077,0.9808266755845486,0.9755218659476422,0.980084559174231,0.9682856267039532,0.9699598589723479,0.9760138727536166,0.980865446467129,0.9805491327248473,0.9792516604262954,0.9808839497309075,0.9777731027587268,0.9813519962559333,0.9792651494411206,0.980965358005461,0.9754771972917584,0.9831567069165985,0.9796041680552167,0.9811655068793992,0.9792953558166859,0.9815856106189635,0.9808219936659203,0.9830851436308733,0.9809188515296295,0.9827660605403893,0.9815720364234605,0.9815687111175044,0.9820811229916753,0.979594391974763,0.9810238434411663,0.9828832733856545,0.9816913723432544,0.9848167571444654,0.9803043338179904,0.9831233828002924,0.9820402294825672,0.9840130994340172,0.983390489635366,0.9843215464736246,0.9843380282502525,0.9815805735539731,0.9844884396269553,0.9839126894448461,0.9838155273365067,0.983318070461762,0.982536799436991,0.9854805300827998,0.9847704696670265,0.9840327808143249,0.98456314816413,0.9843116260955341,0.9836427703770524,0.9848160091422543,0.9848160091422543,0.9838829823994859,0.9847814026784996,0.9847957570430371,0.9845706976492545,0.9841022044090277,0.9847957570430371,0.9847957570430371],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(204, 204, 196)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv5.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.13343916420845095,0.08323322367968565,0.07211722996499803,0.06297219174680431,0.05527371597675797,0.04896383455272802,0.04837916230213191,0.04490517038864778,0.041633655253759366,0.0405809109930252,0.038687922415523046,0.03652766705378634,0.037380022421598706,0.03399666444337655,0.03293187717511345,0.03171444488827988,0.03172220374112959,0.03309705926751865,0.03111164228732507,0.029259576002202015,0.028084616622496034,0.029233055147859784,0.028754356574322622,0.026718761653288396,0.03100504997737902,0.027282216088827124,0.027334134189149564,0.02628726786235081,0.026561974375834102,0.03156032530294319,0.025809485967540933,0.02500604754665873,0.02667177255539584,0.026169985970578243,0.02590014952355473,0.02418805895136394,0.023203911985625925,0.023688254373965257,0.023605259277037466,0.023896664569638713,0.022466256369635004,0.022955841566398655,0.02421864451465004,0.025311404752960048,0.02245148589337439,0.02351986328897334,0.022263940070613896,0.022000214363632878,0.02218819227568882,0.021629235002490664,0.021569829234876856,0.02142494711875574,0.02102898722951901,0.02126151528572162,0.020851750130138923,0.02070721010000094,0.02087386653407606,0.020733816524432014,0.02096697069824015,0.020378248253143803,0.020537101219430618,0.02018476969904916,0.020197009449698808,0.020109061254156416,0.01998732979414731,0.02016535475039332,0.02004865661417086,0.019944967056612858,0.01996984897439559,0.01979300343887059,0.019777710825409675,0.019655822876751217,0.019658775493195377,0.01964419329728833,0.019619626311408216],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(204, 204, 196)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8999480922694781,0.9423953824534455,0.9496108739469389,0.9567275748853202,0.9632765035959988,0.9693593597698105,0.968337609478414,0.9733909672904347,0.9740643528571135,0.9752188664368067,0.9773528214230708,0.9792268353567222,0.9783840254252804,0.9824698725509617,0.9820706853000215,0.9840012224319362,0.9838993120245482,0.9817557289198585,0.9836675178239338,0.9850871209686791,0.9863169366470955,0.9856675656735859,0.9866324958096546,0.9880557645874768,0.981427170005968,0.9863423197155248,0.9871721490695653,0.9882120453525189,0.9875857923355326,0.9819556518234868,0.9885899952789499,0.988617049624115,0.9874380688201191,0.9870932724512533,0.9869273208687911,0.9890564145514308,0.9899737833605212,0.9900825416156604,0.9907449246627906,0.990359965370449,0.9912052409776833,0.990635320902241,0.988226629384175,0.987706872534752,0.9899702687560186,0.9900871471203008,0.9909893811280782,0.9917243166197696,0.9913146324613451,0.991738106080638,0.9921232090483398,0.991861744429217,0.993234223024311,0.9918621163778572,0.9925821365519872,0.9924142765729851,0.9922103055433064,0.992487545567006,0.9923779464500124,0.9932565793212536,0.992638715347216,0.9934491211048788,0.9937766422154485,0.9938984955996623,0.9934628350251651,0.9933303833503035,0.9931330531093903,0.993311223330215,0.9932449023318912,0.9933142844959134,0.993932975864476,0.9933217869405612,0.9931363011661097,0.9936803430958219,0.9936075171445681],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(204, 204, 196)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09992072079906758,0.07926731110131208,0.066743525309661,0.0655932789457213,0.05652941250053468,0.05179328482431644,0.04771292153041797,0.04723208025097847,0.05427716070229245,0.04338689254936074,0.041257575164545854,0.044315527789166705,0.046113968850513505,0.041151279090708474,0.044626655114680225,0.0385960628076927,0.04347092005651432,0.03687066888225447,0.036026142238034416,0.0376050034390692,0.03616855406679239,0.036646168515137384,0.036121805555017546,0.03538054757306666,0.04604792886788083,0.03322823816507133,0.03544299821161322,0.041627860693997126,0.0381315587645339,0.03868831430923488,0.03317864815547704,0.04557924920443407,0.0324952579010598,0.03342518153827625,0.03698078020126959,0.031938410847355,0.032583759361525994,0.0320662690345774,0.039073583043318025,0.03246236984006728,0.031397219209634154,0.032749657153673596,0.05618065532130474,0.03176283797703658,0.032611929259144565,0.03236813054154419,0.030776190901130335,0.030216012651875256,0.03159944281698912,0.030726762968547566,0.03171153055484762,0.031270934277793386,0.033523832829957155,0.029819168060813164,0.029958312748531297,0.029874546080827714,0.030153285189694966,0.03352489699817605,0.030123000776337595,0.029743540742757806,0.02969607963799611,0.03186532971273173,0.029705205744074793,0.02985315309050157,0.029752534074881643,0.02963463119471196,0.029608018504282862,0.02972567857745587,0.029843964343218458,0.029624400656555118,0.029617144213509312,0.029605764642204206,0.029613834883236802,0.029607926122204135,0.029605998866951342],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(204, 204, 196)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0008753516269381944,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9313666411404904,0.9440133667589933,0.9569025828796021,0.9625496572404575,0.9689944525271716,0.9681606742761505,0.9734462687781823,0.9684885285365977,0.9614381529890366,0.9759849816051778,0.9776358021408834,0.9765432993933743,0.9741182777953571,0.980449474192428,0.9697676487588307,0.9796441693713984,0.9706880067994867,0.9806887906999394,0.9831918694834549,0.9832451046491687,0.9766885873966853,0.9823229388272784,0.9829321557965821,0.9841547576950267,0.9676817059778834,0.9835665356248694,0.9810344785994958,0.9754599683235716,0.9789148274905688,0.9741144127619354,0.9833484375035928,0.9748577966391291,0.9838067866501676,0.985085783071178,0.9753799288126332,0.9834255541209067,0.9838701465622595,0.9836594996071575,0.9796562551327955,0.9828499383367977,0.9840588829295025,0.9818392863107678,0.9632501374427636,0.9846509407126491,0.9842111803186584,0.9835018893112484,0.9855100905600442,0.9843185407827804,0.982296323784259,0.9855490814639748,0.984875280078549,0.9850984650863754,0.9830478989832736,0.9857553740204412,0.9857672534121249,0.985516406979166,0.9855264761864766,0.9800573617775155,0.9855218171919345,0.9848010582020315,0.9859881853664432,0.9844523009330799,0.9850344815635291,0.9852710901292092,0.9845278095829152,0.9847902928414674,0.984552208432051,0.9852710901292092,0.9845145586149939,0.984531311601322,0.9852794322226988,0.9847832201723102,0.984531311601322,0.9847832201723102,0.9847832201723102],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(147, 197, 163)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv2.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12252593072555866,0.10932034487576829,0.1008801587011328,0.09495030327999851,0.0989768487105801,0.08920352388971313,0.08871052822979318,0.08685433604121891,0.08263257452866056,0.08432278124293374,0.0806045965162476,0.07918116094764292,0.07777128207997631,0.075449520036288,0.07473371171084194,0.07387428235642415,0.0716498928244854,0.07063438669444495,0.07436919501061975,0.07137961842008746,0.07133235074024977,0.06887533554950258,0.06931602376740413,0.06803734358165409,0.0670007041997926,0.06622070023216059,0.0636050882242526,0.06449857389838153,0.06314420445746435,0.061952704128052906,0.06499047972185096,0.06195934753596168,0.06260856263433398,0.0594255870457777,0.05864989606051237,0.059239577729882247,0.05712394171478685,0.05822688850306427,0.06008841631953249,0.05827742432332667,0.055882174046315417,0.05480246636227098,0.05492668293527448,0.05408680992497543,0.05305132439695,0.053493905118650584,0.0527018707070632,0.05123318411486665,0.0522422370410184,0.0521483355687178,0.05046243121824041,0.05160759473379535,0.04856387721790686,0.04871305915626259,0.04937484593803804,0.04787567138782823,0.047717363095041126,0.04748531363366806,0.04713925513196715,0.04631664060645087,0.04623014037107957,0.045728531640283294,0.04544947337346388,0.04513869109090934,0.04459235037400687,0.0445910772780236,0.04431744896148557,0.04398844578524226,0.043793178721408935,0.04374261747059953,0.04352383477547188,0.043441197396451255,0.04335297071134921,0.04329906378841755,0.04326584881613242],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(147, 197, 163)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9123682640485897,0.9230999986585506,0.9305567267433906,0.9356999681711844,0.9305509833745312,0.9417022406365934,0.9394393791140837,0.9439217321277416,0.9460738222741975,0.945806784863537,0.9470935180110336,0.9473811132510157,0.9493753680035324,0.9513170720188667,0.9515724473192233,0.9507877513908574,0.9553264348884615,0.955682668267849,0.9522592843797834,0.9557819842409804,0.9550467575183814,0.9556415382850607,0.9554762748434551,0.9562016600083912,0.956626010043481,0.957106576685307,0.9590953366008772,0.9586242952027345,0.9601631687203765,0.9607388113204804,0.9583422454283703,0.960647883370108,0.9599680222749445,0.9622856016059153,0.9629214618600218,0.9627436047861759,0.9638353265730595,0.9647236415407899,0.9610412891204775,0.964196090042493,0.9664808552399711,0.9659015850536887,0.9660117372714283,0.965206984462323,0.9655339019903568,0.9665950925536068,0.9667009028721653,0.9669435271572197,0.9660082781600201,0.9662912432611841,0.9678739743550367,0.9677938895385586,0.9689772320407747,0.9687194043784386,0.9684498499191471,0.9703592307720502,0.9696534495827976,0.9694388418305822,0.9702163941818615,0.9712210850515861,0.9703194482155758,0.9708812901459016,0.9716581617423546,0.9709590624120649,0.9715308600975859,0.9722740265421901,0.9721476620666584,0.9731788594284269,0.9725115857615999,0.9727305399326858,0.973394120863051,0.9731879893769568,0.9730107390260404,0.9730405699992076,0.9730891765281533],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(147, 197, 163)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10956780144439121,0.10977516497943,0.10640803700460191,0.10182813552953943,0.10590877019355387,0.0925252235068898,0.09362772222851559,0.0907938768005453,0.08463400072350945,0.08719639041784293,0.09547719162251941,0.08434892327003053,0.07760070256658436,0.08485466548564917,0.08317117934784118,0.07455406481355327,0.07418181117355209,0.07521321155975774,0.08458416468089389,0.0771610332876956,0.07096800660247246,0.0797186448504425,0.0823207884002797,0.06885393045817044,0.08329743031169132,0.07748388865661784,0.07820697614724693,0.07489005428102008,0.06862773215238172,0.0769982346042325,0.06817046323490307,0.06575433021456105,0.06750921915794156,0.06462934297794329,0.06429141158193248,0.0664015458416693,0.07014037563731171,0.06472529113497522,0.06458366077482905,0.06298138073731943,0.06251920300865502,0.06338553174664474,0.06118898858729097,0.06587186484402398,0.06357910972075774,0.06303124413969591,0.060392241277235886,0.05749340405066808,0.05833878259068912,0.0631291434271229,0.05791995131068213,0.05827844365151068,0.061204025507792574,0.05908489077771242,0.056715585255540935,0.05499410582674328,0.05479165680220037,0.055066199406110954,0.055481308483585866,0.054716797557073774,0.054273617113988426,0.05347752128679728,0.05337966379687139,0.053440869115677074,0.05334479975229276,0.05334030592564455,0.05333144962787628,0.053199244247064556,0.05261799109606809,0.05263280065864632,0.05258106120519622,0.05260389509684441,0.05252587600415925,0.05253761544618819,0.05253551242124174],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(147, 197, 163)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9235336809798647,0.9264105041855155,0.9323662954281684,0.9320758185408613,0.93201382197577,0.9439106279118529,0.9339510573569374,0.9400152240716765,0.9488462520654743,0.9433732884273238,0.9360976205593468,0.9441876197232282,0.9486742473422436,0.9461924223128197,0.9426812730083897,0.9545509094362699,0.9530497969691685,0.9490914084190238,0.9390527200078819,0.9521287250348756,0.9567157206437641,0.9473307219094715,0.9474065731298454,0.9584461108019641,0.9493607566585793,0.9493388697629225,0.9542476028306278,0.9472518501274233,0.9571147497868522,0.9533945367352523,0.9596030485568007,0.9579640972417104,0.9587501299844474,0.9604414259533639,0.9597290922228808,0.9560477694549268,0.9550189354271637,0.959076677455457,0.961820549234816,0.957740981795265,0.9605799810324672,0.9585282873030291,0.9595879107479026,0.9551697406537,0.958639920177038,0.9585774261566835,0.9628294276969709,0.9661847963315805,0.9651380987089366,0.9622650927900191,0.964560333168564,0.9653097545880506,0.9638776605367564,0.9665678512615757,0.9658273674202252,0.9687358609972945,0.9669404453911149,0.9664092045154598,0.9688259736468094,0.9680211442427136,0.9684596719834141,0.9687950095039027,0.9675911084978243,0.967133848709406,0.970331733304305,0.968806707224292,0.9687711551597018,0.9691058757103727,0.968806707224292,0.9695810693114489,0.9698107830856034,0.9695810693114489,0.9688334462743955,0.9693220453250315,0.9693220453250315],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(132, 94, 39)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv3.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.11306098565408045,0.08882052836276139,0.08619668287100252,0.0763331014010095,0.07179386074668627,0.06669248897936472,0.06568097200965826,0.06169700137972149,0.06319308302195621,0.0573754114900019,0.05634482412130699,0.05559284518230685,0.055568718946062,0.055260433273695043,0.05305884796677996,0.05182367981948121,0.05101995660079577,0.05046643425304046,0.048674734982257585,0.04771892000907202,0.047896761697250816,0.04263414084988157,0.04471793941223198,0.044171571773706265,0.04662842519436232,0.04620210912936332,0.04292334071003148,0.04373701804755992,0.04124263465438648,0.04295956300825461,0.04520441971010905,0.04087431449080387,0.04055979991030857,0.04115457851375706,0.038849041907315196,0.03836447132190205,0.03893332561650842,0.03797265653245638,0.03738321274847543,0.037248244916621216,0.0361125921962819,0.035281409092785154,0.036896565809999546,0.03630884335144825,0.03495523798677383,0.03386195184746149,0.03583010987113022,0.03358433515533276,0.03254199833241115,0.032542272156823475,0.03196243345993491,0.031037864720561935,0.030554774443257306,0.03091654553326681,0.031144045811372662,0.030125836224934217,0.029606433767070065,0.03052243970991683,0.029817738002091476,0.029488010260725624,0.02878452619155794,0.02833648759696765,0.028276262439591096,0.027984659340475844,0.027434072407114,0.027518761248028017,0.027386515734360937,0.027084300437470617,0.026902262537280376,0.026778587170226877,0.026772897530940254,0.026664971587550053,0.026561485939709317,0.02651597361538009,0.026483067120388067],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(132, 94, 39)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9179320995149468,0.938761522863033,0.9424717351173203,0.9493681908010307,0.9525776776277195,0.9565391254575967,0.9565799698586994,0.9591606907836121,0.9575904530402544,0.9617284540714103,0.9614951451890688,0.9641005784415252,0.9628567197895842,0.9613159705688467,0.9628906773077711,0.9646781739923094,0.9652940075203744,0.9668080360857766,0.9670317056083252,0.9669748836690637,0.9662546829601834,0.9711284254568021,0.9706450887869689,0.9703018639306322,0.9685127380643415,0.9685695287861968,0.9716854825675049,0.9684558746974937,0.9734527180974976,0.9712470647488708,0.9698444786277739,0.9746862213792732,0.973924850614924,0.9745624736188239,0.9762474110423363,0.9758855933954975,0.9748054928661777,0.9745137891569854,0.9770754116960615,0.9774693610836174,0.9772244653928829,0.9781133834733796,0.978283269407861,0.9775171584241256,0.9782792043058435,0.9796614897023838,0.9777236492561024,0.9799313686490118,0.9818121844040033,0.9810091025117755,0.9815915542488871,0.9815714082246164,0.9832965226736579,0.9826458467123604,0.9819008979294698,0.9830420503132221,0.9837289548084279,0.9816087016254161,0.9841893050423746,0.9834099838728837,0.984678313946692,0.9858395835521697,0.9856065211080279,0.9853706215801677,0.9860388564281761,0.9853523332525296,0.9868679044656348,0.9863002032432246,0.9872085205515712,0.9865972553634257,0.9862645168539108,0.9872486490899248,0.9867352493335738,0.9866973626012471,0.9867908494812186],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(132, 94, 39)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.1084151033120057,0.09010515305184827,0.08182939195243763,0.10168558163536373,0.0742422529256221,0.07223839114109674,0.06573508392596983,0.06971469723789143,0.07646515426254764,0.05904030639076561,0.07204633005398656,0.05715760525405612,0.059886893240856554,0.05877452621242844,0.052052667841980954,0.06548924509090247,0.0533139735434678,0.07317564651830909,0.05967949374076427,0.05700030362175912,0.05262727003890214,0.04598175486440921,0.04880279047075416,0.04970263551498197,0.057749390023475664,0.04712192866298341,0.05351959862506267,0.04815706008744404,0.049333034896154176,0.04531600221581885,0.06994680716204889,0.0475051052433109,0.04291150011506277,0.0460929391720041,0.0423815827522286,0.047566615697956574,0.046930249460374364,0.043914518177611724,0.040348366793897965,0.05196197841994951,0.04506975666354202,0.04180704788448884,0.03960047399823608,0.03998188663952539,0.04083653684334247,0.041627053222602996,0.0482061451448198,0.03866675301259736,0.04855989864755332,0.036523304687640104,0.036950982397876656,0.03923516874665657,0.04151135121833827,0.039678957198084015,0.03617510759748544,0.03687192305564061,0.03734483143820386,0.036914745262500756,0.03567556196062016,0.03535751397001374,0.0352819159294117,0.03529427985299084,0.03492299428152055,0.034369409616870156,0.03447689655720163,0.035282769929809665,0.03422661239594938,0.034333771075477304,0.034094158511391214,0.03450618227783757,0.03426710387941488,0.03402529010658002,0.03405737433185692,0.0340722190120171,0.03406339993898811],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(132, 94, 39)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9114989676828215,0.9506180638898338,0.9455775359726651,0.9129893975676326,0.9552405415500651,0.9587372629003578,0.9601467329435056,0.9540188899520132,0.9453035439616961,0.9621193784861602,0.9523671205183865,0.9640288963264037,0.9655393887438618,0.9654968867114917,0.9699070750396384,0.9537534983098691,0.963070110767707,0.9561842905593604,0.961927139488881,0.9644669260549124,0.9683558770224836,0.9706654242202656,0.9737008416058509,0.9673768230694632,0.9637022253513872,0.9711030618949485,0.9703731674393072,0.9718211393760405,0.9663138381913764,0.9733074795667838,0.9512998460194602,0.9708360770785694,0.9732876640995881,0.9729667276590523,0.9767555974606138,0.9701607195295432,0.9734126570175274,0.9742697503789794,0.9751797095370067,0.967160426230794,0.9742633475857477,0.9754454982158918,0.9752681465862258,0.9789900581606498,0.9793355074984177,0.9744433185184553,0.9714364010053367,0.9757329948789097,0.9658053588880514,0.9776791982373836,0.9763393450154522,0.9792939601598202,0.9752716583928374,0.9789191190171652,0.9780626983721102,0.9798676186742276,0.9783620405936511,0.9760854038731038,0.9817555520983929,0.9811139058487399,0.9816820964738935,0.9827247746058818,0.9788226863843699,0.979575572425241,0.9803725361941562,0.9832630622680296,0.9817628897747,0.9810951394592425,0.9807690520622874,0.982275699398447,0.9813077920994433,0.9812604932697198,0.9812799103798918,0.9812872803985916,0.9812872803985916],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(203, 206, 94)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv4.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.11136169056370769,0.07847362598158923,0.06231413965816487,0.06245085007307319,0.05706442323132479,0.058634152390566345,0.050096443353513126,0.04972720885689862,0.047899535999879804,0.043988623421421576,0.046832800595109,0.041105100072126616,0.042491986385769585,0.03877315971240794,0.04168074413161098,0.03990665380794978,0.04224621634183607,0.037383399289293386,0.03756024317318768,0.039389053300313254,0.03451920210939639,0.03501199042298676,0.03846530066493724,0.03473517250606763,0.031639829203742476,0.034032558560806656,0.03455672233588439,0.03177919668686349,0.03095648267817388,0.031368987260968824,0.030832511807054725,0.02835380067410884,0.029393351660296746,0.029109230308547052,0.03399981735071502,0.02912328816557736,0.029010780097197694,0.026764468055854276,0.02571937278476368,0.027881314435638786,0.030178606709548597,0.02608720501298074,0.02560423196561182,0.02489880947662861,0.02466308112338373,0.024400637909889493,0.024001204438895876,0.024376938957614315,0.026915303665935517,0.024381077095849135,0.023443938885630886,0.024385528504967963,0.022907600279381116,0.02172783596008026,0.023130510035232307,0.02301233279447307,0.02190470514437109,0.021890494689656866,0.021658974209158143,0.021132746162821132,0.021123297090929125,0.02124105036847575,0.020674726888452063,0.020586893741651634,0.020525484526143722,0.020042343822575654,0.020034924744364867,0.01987417146375772,0.020147675236757475,0.019778167052708677,0.019795395540583446,0.019647778603936388,0.01957531835273642,0.019522030130114068,0.019507622290381315],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(203, 206, 94)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9111461039234688,0.9448882691157502,0.9584111747454771,0.9581339893548518,0.9612011626501837,0.9587990931130467,0.9654955579123018,0.9654555265365854,0.9678453626426355,0.9690440738842233,0.9683086115739212,0.9736265760490111,0.9722812423324181,0.9750438150752071,0.9724187106952126,0.9750512566160641,0.9730246348717028,0.9767069426697889,0.9760227557537998,0.9747633709179915,0.9797791764643314,0.9794557398029741,0.9754660243805005,0.9788855895707547,0.9821867322795852,0.9794602368183903,0.9788824393143857,0.982399174953856,0.9821984830094486,0.982410989020149,0.9827338914280118,0.98398836948463,0.9841496442496617,0.9835440610327462,0.9799452900748135,0.9842233191510054,0.9843420691497279,0.9869897352172753,0.9872197250371298,0.9851144419596188,0.9835171106034996,0.9862882387028733,0.9877829915181884,0.9878622649159151,0.988427125761778,0.9888349026454638,0.9888138871721837,0.9885545196036799,0.9855035152874204,0.9875943926762888,0.9881154262675671,0.9883549077767122,0.9886406784960506,0.9904337558520273,0.9897422478519625,0.9891898046992392,0.9904365797622603,0.9904098607465205,0.9904065560908122,0.9911837103377881,0.9915430681833517,0.9915108898522373,0.9922939750930787,0.9917809616839127,0.9924898890725357,0.9929930742279288,0.9930547265287211,0.9922248436233784,0.9914887926920986,0.9928779770482175,0.9928836713050326,0.9930758287036825,0.9931271514569692,0.9933933975406651,0.9933709690416646],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(203, 206, 94)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09159620859983451,0.07959781517482706,0.06667076433749543,0.0649820906855806,0.05979869716458304,0.05750672309003335,0.052948869296263176,0.0561738413044882,0.04991081392078875,0.05500428140470662,0.04976265495995066,0.04944351342893958,0.04653201391760426,0.0565088812377035,0.04308733196858688,0.07138031569031096,0.04043510083326769,0.047521824963211605,0.038957132332513425,0.039618826309327816,0.048588532957014756,0.04205681892348729,0.0412646052891651,0.03729113691143973,0.04607586379825455,0.04033514420754721,0.03876721197387197,0.03852024539229796,0.04177330514204871,0.03623119975949071,0.039265826984574294,0.036147812794881176,0.03899234898157956,0.04854357554689306,0.04622042753647283,0.034935698594219494,0.03955512539934866,0.04250385059216588,0.03283604017596474,0.03641971009447403,0.03513282508784553,0.034706252882468334,0.03342553667749736,0.03546960836730872,0.03443814541670875,0.03250209720120397,0.031743406148505786,0.033803661011441055,0.03302875373678928,0.035684325930076775,0.032606938099840665,0.032176819384814946,0.03131439223680709,0.031466325311009416,0.0317332479065841,0.03154500747208333,0.030993214879453796,0.03450837995332131,0.032757819346648315,0.03080862077701952,0.030987567764377267,0.03071916200655842,0.03060568505290038,0.03055558590749695,0.030672364018831874,0.03111824917885446,0.030897765957407935,0.032456212567607154,0.030416516928943163,0.030250170632326316,0.03030738821349193,0.03023114354698519,0.03027000874792997,0.03024661057081419,0.03025163893181434],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(203, 206, 94)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9367090597175675,0.9490762075491195,0.9570343620429912,0.9615528803664548,0.957371414503164,0.9633329748001631,0.9604932157689222,0.9633867010334074,0.9659965398223865,0.9589151899696643,0.9704393787141029,0.9671561765768574,0.972252162665017,0.9616079739468366,0.9771009559205179,0.9472734752790529,0.976051221873684,0.9738195105955343,0.9781258756546167,0.9762490786305382,0.9687935961227783,0.9734794465349579,0.9751316513197336,0.9802182246675994,0.9713309557810004,0.9760523411515634,0.9760872664148835,0.9767800435241586,0.9733708496846168,0.9795894775814872,0.9774497591170931,0.9807173244987238,0.9779326328951508,0.9674987806309409,0.9701944918287562,0.9823743565060532,0.9766755208753138,0.9731806384094641,0.9843804645429445,0.976444314573936,0.9804272793048392,0.9807116604758292,0.9813655204507234,0.979151771945612,0.9812926968417319,0.9833145280017764,0.9835990086522648,0.981331249730408,0.9834123417644917,0.980237411237971,0.9834720983807218,0.984296303852514,0.9853134952850107,0.9827137603071312,0.983083717976543,0.983548092158148,0.9841144937138255,0.981380987888027,0.9822836076066416,0.9833349442676218,0.9835841948748714,0.983356503149799,0.9848376865982479,0.9838738726707024,0.9834165978068825,0.9834450449218508,0.9834411215814718,0.9827644895925408,0.9834286579333745,0.9840884027616583,0.9845749562437464,0.9843341661300068,0.9848203186930601,0.9845795285793204,0.9845795285793204],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(139, 225, 85)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv5.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.11018450252310653,0.07283154743268044,0.06623529680884034,0.06340651885955194,0.05686179367742178,0.04687394759311572,0.0526049907870787,0.04403250830382416,0.04779097013354711,0.043298523803961646,0.04088844957741197,0.040394897915994724,0.03761596135018481,0.04280213070233253,0.039393591558195884,0.03297595641013546,0.03744668068464543,0.037919910123223974,0.03326018799570008,0.034328823920684995,0.035373469978946456,0.03200651802515178,0.03178403123248825,0.03306564632013764,0.029155892376091334,0.03148563941353921,0.030205158843100925,0.03194515786420678,0.03196782073451959,0.029636505173619532,0.027504859255351562,0.02805513198501329,0.028046161619112116,0.027356507017235576,0.026444620203401763,0.02724378354173209,0.030409295467141363,0.027913095372785544,0.027092314961730136,0.028715468733171417,0.02822080587227866,0.027070295075882753,0.024573527896970954,0.024919636681556428,0.0242614655867475,0.027125622651284775,0.02559527787897322,0.024475335884937444,0.02419659179237427,0.02302816752664412,0.023161258964979635,0.023197145384116543,0.023069008750966905,0.022621846240001037,0.022586809155261324,0.022761211804149487,0.022568134574767924,0.021840392830616284,0.022385319939388852,0.02137765466893798,0.02133879644267116,0.021146823291721212,0.021145071151569265,0.02106723055896378,0.021033369483509423,0.02058942689958273,0.02052898785293546,0.020473392849269603,0.020328241997619265,0.02030804190552129,0.020182910986150798,0.020101662436318835,0.020067482995763337,0.020010305259308487,0.019980498204457936],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(139, 225, 85)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9111428386834456,0.9495979646984668,0.9523682311099549,0.9527019235405964,0.9589291677894936,0.970755005682215,0.9650950027544699,0.9722042935484758,0.9684120122268132,0.9731953796413775,0.9726448578127286,0.9751968738702342,0.976191770604106,0.9718825128322478,0.9745214671135795,0.9803521451369186,0.9755308830859502,0.9766329038530013,0.981068824741341,0.9788625058645903,0.9781479564609308,0.9804019428157646,0.9817861666914884,0.980148505410091,0.985003020109505,0.9835109798327804,0.9836364074751262,0.9814700010406767,0.981782599771576,0.9837416886365623,0.9854604152875096,0.984796975221499,0.9851325674487152,0.9849291111855222,0.9868165889367797,0.9849805770015572,0.9814941281464258,0.9851051896217137,0.9848305640475516,0.9840801285883557,0.9845123863532264,0.9861697941914929,0.9886880342437793,0.9874448433128457,0.9888518061854984,0.9862413861309313,0.9870280639316842,0.9885958773175084,0.9885354626425354,0.9907992846351499,0.989356710493415,0.9897079202435578,0.9902564344802688,0.9894698939459255,0.9906811350755431,0.9902279941608398,0.9902100966417774,0.9917065720890076,0.9904831877145244,0.9910666293469976,0.9907430201791855,0.991307484758314,0.9918782434351782,0.9926397927347369,0.9920501232453901,0.9923029891707673,0.9917911781172154,0.991638592219121,0.9922030165339689,0.9921049459707916,0.9920693423668997,0.992461543971026,0.9921202774870783,0.9922158761893575,0.992282819674657],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(139, 225, 85)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.08211647218854977,0.06772560386108778,0.06279997984363452,0.057264000113375,0.054082215385338694,0.06541934899243293,0.053080977100072445,0.054318416528275744,0.05162350708471541,0.05173183810157874,0.05103417657750988,0.04453082445663275,0.0638489074621004,0.03848390037200295,0.04020889514090679,0.05985575817602197,0.04511074701330506,0.04156653744863071,0.039163139595608534,0.03666905895950868,0.03651642516204172,0.03601883692327643,0.038950048799264884,0.05120935794670148,0.03475371756402078,0.041012638128807455,0.03487753040602117,0.03388536188917881,0.04577129171373918,0.03460988750572467,0.036683966298795646,0.036659872350106947,0.03653744042329362,0.03242264305244606,0.03401650185744787,0.03416672812141094,0.035731214935222445,0.032765491498807045,0.034623670447425745,0.034031989502230874,0.034559547760335034,0.03358073925849089,0.03835344043947577,0.03293336835206579,0.039803682503216864,0.03133498037752417,0.033377108446408794,0.03899602770190878,0.03210921575318497,0.03200861969098602,0.03132063846305474,0.030848476836054597,0.031247371926750103,0.03005781747221537,0.040020539219846434,0.030979505488552998,0.030095115641957704,0.03368560709187255,0.03092504609952268,0.030074913149437135,0.029795162492396495,0.029548596475542206,0.029983611753101613,0.03027400062819527,0.031127414619390087,0.031040602085209385,0.029695980278999126,0.02963120950149097,0.029983903280750582,0.029654093091840188,0.029703230290478447,0.029884007176275516,0.029606040546676957,0.029610794447234405,0.02961310998527045],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(139, 225, 85)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9495614537759482,0.9542098896192366,0.959552910751743,0.9678613398074827,0.9649425369719564,0.9574533079008802,0.9605014595792215,0.9666873256193228,0.9652150780664507,0.9584760965499096,0.9608550411408376,0.9726048565266434,0.9581065463514716,0.9800121905931409,0.978335842757578,0.9588550657344503,0.9707150469274435,0.9755388134681365,0.9779426124318379,0.9810106180916154,0.97938687176418,0.9814650764470696,0.9756955637381469,0.9714693967648457,0.9803365407208833,0.9768480631860832,0.9823973218069748,0.9826491941643442,0.9653882815775289,0.981510208206439,0.9803228343341526,0.9811441414780865,0.9806928683328652,0.9836083278620036,0.9810921753064576,0.9815313012458844,0.9805247874608392,0.981388776544769,0.9821780156242872,0.9806525434524923,0.9808983685684514,0.9822220642049712,0.9782087840910924,0.9824996879290526,0.9743508252868446,0.9854540575230278,0.9803405744374091,0.9745789292153971,0.9844168750556456,0.9827788402258308,0.9819423914854948,0.9824282342745072,0.9845244809314534,0.9845657058549085,0.9771093075804451,0.9825150131223744,0.9854477853326742,0.9825574812669772,0.9846168876256629,0.9831560243166126,0.9847648580220024,0.9840912130837031,0.9849838098099443,0.9853145144581276,0.9851147430286694,0.9837159014883424,0.9845737126829798,0.9845737126829798,0.9845866910032489,0.9840979656779998,0.9843338877071041,0.9852985252031429,0.9845601559422171,0.9845601559422171,0.9845601559422171],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(151, 236, 39)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv2.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12111674528590319,0.09192233542055062,0.08842771577671221,0.08287775454549855,0.07857667195940346,0.07936596154893523,0.07601967245331813,0.07461042331290546,0.08096563243536536,0.06930596153683884,0.07119883671744309,0.07101933353613606,0.06437022301015302,0.06724025605743557,0.07228418811558311,0.06821881809270805,0.06420057280691083,0.06358215398702424,0.06397623161423657,0.05860537892935066,0.06009165824381347,0.061364553357129925,0.06534869837289824,0.05644908320470228,0.05711681080847671,0.05956670173016064,0.05338884322026068,0.05359740307360444,0.053837846128220956,0.05130846326123125,0.053625997726859914,0.05330255403431967,0.05258799212785248,0.049769317248637736,0.04975473288988603,0.047952215070542877,0.04591833559576452,0.049739597101972136,0.04899469655250356,0.04760711457055459,0.04529120645230852,0.04547313498023722,0.04496831936422492,0.04332372720469587,0.04307941380559647,0.04683192060643728,0.04478411148648969,0.04289305547053871,0.04234410594521519,0.042211267123380754,0.04181429971978154,0.0417473158488671,0.03993247300898496,0.04110376269760342,0.03881527048718069,0.03853365392760211,0.03837143033230837,0.03830213841789094,0.037854617560988575,0.03771288329084646,0.037642544994444374,0.036936974806235284,0.03655548515000977,0.03652231222336508,0.03653162665076272,0.03638257783798145,0.03589378131077735,0.03594589346266626,0.03573790561548623,0.03568012801650805,0.03554083461204345,0.035541246312948846,0.03539830930018958,0.0353460312423052,0.035303034057322234],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(151, 236, 39)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9112113982086303,0.9392140287507256,0.9414437280215799,0.9458091775492787,0.9511980995348064,0.9497440950207139,0.9519835094783545,0.9501826089548445,0.9425519074147858,0.9576477745716119,0.9542873639192545,0.9539296249367187,0.9592725012665838,0.9559841252144086,0.9515063363779017,0.9544899310816972,0.9587883888330717,0.9582311921802498,0.9574941787207385,0.9619750949317993,0.9588606765241242,0.9588264087141417,0.9561208607824643,0.9627590120049665,0.962936070808669,0.9605790445824681,0.9637715343357329,0.9649255794137894,0.9630098554360343,0.9659609366629206,0.9634449822027225,0.9645448210750558,0.9643344397360597,0.9686709358624511,0.9673831556780348,0.9693897621475679,0.9723774353468772,0.9674298793403303,0.9679804323708786,0.9685657499270319,0.9715107366125757,0.9713716689831943,0.9719960785258129,0.9744117951594468,0.9729301068178632,0.9685908528277513,0.9721336594792229,0.9739128611815676,0.9741680840821098,0.9751610340051781,0.974188016721224,0.9746607757803682,0.9763881837084537,0.9761083618059624,0.97816320900232,0.9775308736066897,0.9790379414343219,0.9781842680540768,0.9786489554377656,0.9784949405193663,0.9787271660831671,0.9792662931499647,0.9796044664026687,0.9794990953097351,0.9796839380446757,0.9799074567567269,0.9799487693530302,0.9802652838986052,0.9807207498203032,0.9808075413100077,0.9810820514869982,0.9806097920033388,0.9809804251892142,0.9808083861469832,0.9811603163412135],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(151, 236, 39)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10339207196051313,0.08794980473125104,0.09078525288408155,0.090384633091512,0.09258013191501709,0.07898524706203913,0.09273423994427285,0.08495020500265855,0.0773099506792334,0.07305386813543097,0.08523297515726581,0.08033132228961926,0.06955468732466813,0.08313924783898383,0.08434440497168151,0.07385068648971643,0.07035812547116754,0.07391069167872884,0.06506683492066524,0.07020865203588689,0.08043722782245616,0.06727301651976772,0.08791612829641787,0.06399430558341475,0.06575030547749136,0.060757446836974614,0.06090524419374073,0.05922353698318357,0.06158666293748056,0.07630120757519175,0.058730589805804576,0.058602290674788025,0.060511939703803704,0.05622445483695191,0.05766868728030588,0.054405823792890995,0.05446331849724976,0.060281097305189706,0.06757008941824903,0.05240193950300364,0.0532915032862388,0.05278936106826841,0.05509147142472955,0.05395408382581681,0.05774435049172529,0.050718413362490763,0.05966250857024668,0.05134422744006635,0.04965016918698537,0.05305454231209771,0.048755644840473164,0.049687714285866914,0.04981975686765209,0.04812579785323225,0.051661998750417916,0.0483139331654175,0.050311009788124014,0.047112069138136925,0.04693368290810241,0.0473070870252819,0.046908394600619976,0.04679216758389653,0.04747229274042283,0.04634584693359755,0.045949400126729226,0.04630792262064632,0.046361449389011176,0.04599765852810591,0.04583104785621371,0.0457250605794982,0.0457700568427335,0.04555709135184173,0.045562109212899944,0.045579987067946866,0.045578970721701986],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(151, 236, 39)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9328537460798707,0.9427270022612817,0.941683281081365,0.9408148871047852,0.946068930613621,0.9483147305586163,0.9353851904789808,0.9462537279781346,0.9517070097530546,0.9528402313738039,0.9342273494811552,0.9439726263830968,0.9581432439487183,0.9441685312034748,0.9431745771226799,0.9475872987235048,0.9506983420690632,0.9416179297349287,0.9599481484257053,0.9548381492024846,0.9413806218749017,0.9527801379546044,0.9416106092593897,0.962823235367599,0.9564820385131826,0.9624500056881088,0.9660566100559154,0.9595702729096667,0.9625952123381968,0.9475804186907094,0.9641107235227871,0.9584226580033277,0.9640602563430514,0.9690901283195854,0.9609475280215124,0.9732947192638145,0.966764177503384,0.9649385555984883,0.9577847788864627,0.9689741730525079,0.9720953863689314,0.9707416758036046,0.9656358472942289,0.9635035798827623,0.9604445518293077,0.9680468840530635,0.9598591109802569,0.9702136126018881,0.9718643942313377,0.9684405090830721,0.974241790799822,0.974594549650822,0.9738892410607028,0.9759403003527934,0.9648783602325776,0.9726955524811196,0.9721907150794135,0.9753781570714677,0.9763417627459744,0.9774115294274767,0.9757466488014197,0.9764552316593845,0.974131798856393,0.9747874356936889,0.9756269179088869,0.9749802632752754,0.9764333478128966,0.9754605457954647,0.977054992309451,0.976553671803678,0.9761341179891571,0.9767623218388229,0.9765301558852862,0.9762833043227992,0.9765301558852862],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(239, 244, 23)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv3.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.15105734371493773,0.10565130332354691,0.0897071512627233,0.07577507920042618,0.07253384845841244,0.07225564221925138,0.06235318922044076,0.05801023090213435,0.05374586144024837,0.05666387373707412,0.05630939377883592,0.05736702533368392,0.048653648839783424,0.0456272157026457,0.05146466470520862,0.04629574703118782,0.047793841569386805,0.04358690883644258,0.04128341642459097,0.04324185592031697,0.042679399402997746,0.043092995997249466,0.04375684997930969,0.040191766191252594,0.03970260412345842,0.04059542638483326,0.04031306929024361,0.04087838129250318,0.03816878924835452,0.03659691757438246,0.03444324983294546,0.03447859772360475,0.03640517111684175,0.035074207428224036,0.03487092565664038,0.03477249012199601,0.03575255483696141,0.03338437558398214,0.03332764391124863,0.0324878323279142,0.03242918675741925,0.032198784077478164,0.030563419802543224,0.032664207334097854,0.03405972290720112,0.03264155574356294,0.03325587031127456,0.03033696855125012,0.030353109754476897,0.03072659194059798,0.029709257734224694,0.029504880653896965,0.029393924152284416,0.029494040251153715,0.028431697031920444,0.02908341461175974,0.028848552141436733,0.028240591735671887,0.02941820882047438,0.028380432782987002,0.027246553199483083,0.027576958650417992,0.027031721309348004,0.02670884730607988,0.02737025858755442,0.026569596775311375,0.026385462529560443,0.026293267063019883,0.02622043840194554,0.026283677210785953,0.02612509341891279,0.02605336097422198,0.025999948421209403,0.02593918000897318,0.025921367854733375],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(239, 244, 23)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8763998526565945,0.9240189357785896,0.9400169051013595,0.9496268127579208,0.9490696423238898,0.9490456868186494,0.9587363396987086,0.9618892847670677,0.965144357103019,0.960341733914692,0.9601903831062791,0.9604311056358801,0.9687613935493176,0.9691482183219249,0.9632444956519136,0.9710399244210188,0.9678228244701741,0.9722330867303164,0.9753322718835334,0.9739417610644717,0.9729973502868138,0.97191714482758,0.9702576704740536,0.9767570661001783,0.9767785553961073,0.973371258519212,0.9751261694913432,0.9745898541629803,0.9764184999010665,0.9792414622277797,0.98078747018504,0.9821036916110841,0.9796064404520107,0.9785502438637568,0.9812950401173111,0.9799804380400523,0.9799599367322445,0.981947508554934,0.9813798355343182,0.9820904815795933,0.9817928612462993,0.9841815243633328,0.985577762460283,0.9818972272792722,0.9806718651075822,0.9818405322238793,0.9816543407327949,0.9846893427711425,0.9853316912427059,0.9835596238382944,0.984539694366743,0.984621411735091,0.9856250334403039,0.9859175635872682,0.986868405317274,0.9857695339046891,0.986728187535346,0.9869977009334064,0.9841140076198602,0.9861094024240723,0.9871666986667638,0.9874686355693643,0.988354931995712,0.9878353568372906,0.9876614813529142,0.9882200124497453,0.9879606132360651,0.9880476287314058,0.9885716973167072,0.9884934025522397,0.9881247453542593,0.9885445514523661,0.988322356699077,0.9885553398276131,0.9885462206955488],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(239, 244, 23)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12461010353876553,0.11055400874913764,0.08308413389724555,0.07552437916449255,0.07544936540815018,0.07707953638432362,0.10944402724299644,0.05792629238973369,0.06728665344289078,0.06156459530092187,0.057817478360179364,0.05733348284800028,0.05325084557033486,0.04739150444615338,0.06187765510733595,0.05516740495908711,0.05335617938504596,0.054605142459836616,0.04926769422091979,0.04698984495450541,0.046832615807908505,0.058368326016922586,0.05472377757333808,0.04767108114877927,0.04308722444467528,0.051437600663642294,0.04532849511381277,0.049001602242185485,0.04751531256689239,0.042066522444441555,0.04141770377243098,0.04174356420373999,0.040968075678827834,0.046713371439180834,0.03956673145806257,0.04208022443852884,0.044499114455328774,0.04394143993911874,0.04055167972734294,0.0389173349692035,0.03947306082318329,0.03873300034924061,0.03746195503116883,0.06559632247973143,0.04117298486972183,0.041504115949279254,0.0390655110484546,0.037361573299898726,0.04165561359977395,0.03838115263077402,0.03721040879789087,0.04069823230715961,0.037373230137775854,0.03635059595620099,0.036547704230469116,0.039466722573304094,0.0369780685355778,0.035818326055901156,0.036776789725850946,0.03547814632045854,0.03498441357024756,0.03479772675846451,0.03545884034273141,0.03534887241794891,0.03489497445805376,0.03521844865786251,0.034601578958767795,0.03469508554005541,0.034659867782363366,0.03464200200442596,0.03465579557664616,0.034454907589556834,0.03447313816174609,0.03443798551854399,0.03444044542118036],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(239, 244, 23)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.912478128538066,0.9102380977236676,0.9476326347611967,0.9578054035305071,0.9469921413462841,0.9464609407504665,0.9233142433357152,0.9670531155196083,0.9521872363406751,0.9646501937253923,0.964311882540735,0.9650230610051453,0.9661342869401263,0.9701047642635994,0.9608315473666842,0.9603535949808025,0.9692751844870025,0.9736253045184226,0.9770098287802025,0.9718830163346793,0.9693592796912129,0.9601366274401291,0.9679508505286154,0.967359103579799,0.9765828487100661,0.9690136329978164,0.9753386574892117,0.9686994892636281,0.9658842697210489,0.9752157879225507,0.9775731569207139,0.9747394435273092,0.9806780277155707,0.9722717150620368,0.9772282267200426,0.9750827666723135,0.9712024524633185,0.9789241738063241,0.9788170433420657,0.9779975914199948,0.9795416313846148,0.9794733488762468,0.9818362007562487,0.9527362743785065,0.9762100078915844,0.9765745930159235,0.9784038040699442,0.9801025964268131,0.974205047671451,0.9806406667994807,0.9822130434263843,0.9737820017800821,0.9787145699319562,0.9809550869591273,0.9817267605249606,0.9754735948353469,0.9823307513283331,0.980773319857126,0.9782515147256656,0.9818011977994557,0.9819512938960524,0.9836639046312506,0.9832195998399953,0.9829050947814914,0.9819485283182188,0.9832095319107939,0.9822184669559348,0.9839198717123196,0.98389575233543,0.983204822096838,0.9834470668551333,0.9834197083387363,0.9836763738119482,0.9836763738119482,0.9836763738119482],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(220, 8, 37)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv4.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.1327941011539303,0.0783776424816657,0.06354429826294024,0.06528238043069566,0.05498350002084947,0.04752147299198078,0.04287275592759166,0.05303059165181436,0.04382952469457602,0.038457525149414494,0.035775142956315995,0.03988688874686775,0.03893447520823686,0.035701901016741956,0.03816988370877361,0.03988715285970856,0.032264703332968186,0.03515100097806456,0.030838611190295847,0.03282988672331402,0.0299653014097734,0.035825501835530564,0.03171317846218779,0.031054176736669988,0.02823932782041692,0.028872641955181223,0.03422274333120826,0.030384706864549534,0.02697864300133991,0.027633526071667126,0.029120064094739478,0.026594746651347697,0.02848588548310502,0.028268946999127104,0.02968907754138573,0.026221206305038616,0.024441293664575307,0.024365789487206923,0.024315333741056686,0.0241541869067859,0.02509283447832313,0.023660487148223736,0.02306273726624824,0.022856098414747575,0.024434548904873933,0.023356327518206283,0.023827794599608185,0.023574874635448707,0.023743777781434758,0.023409190357695807,0.02132100684916065,0.023007156165212154,0.022757590199817497,0.021174367570573248,0.0205687821533356,0.020413931276977812,0.020215083208555976,0.019944314656934924,0.01978802930815455,0.019814727578273753,0.020302717584182103,0.019447620864210806,0.019627781493113484,0.01914577678503108,0.01911731077654699,0.01890238212882397,0.018958761368079148,0.01896514332573754,0.018695788935065814,0.018693439374418603,0.018702012816825955,0.018616997619572372,0.018578174026983814,0.01853123667627522,0.018514518945009575],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(220, 8, 37)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8968454949594947,0.947933279370274,0.9577801308193431,0.9554436283541821,0.9618954723854982,0.9688771687452682,0.9734585541810555,0.9618207040301296,0.9717176438837282,0.976224524048748,0.9796960706091592,0.9743487327149815,0.9761087883316213,0.979130072301597,0.9766227234298223,0.9747242534442986,0.9818352479766461,0.9784415790154678,0.9830821866802609,0.9795622253207186,0.9836897806842793,0.9765729679515661,0.9814084599535846,0.9824260041748659,0.9848003034965295,0.9848788590751755,0.9787771820232763,0.9822600798223674,0.9865554962679937,0.9846074976921826,0.9846868499706217,0.986424484342139,0.9838356418339325,0.9846360384586083,0.9822374507312646,0.9862223331585166,0.987952185442321,0.9872317327746625,0.9878770908416242,0.9885426982323318,0.9875457200520015,0.9880676728474009,0.9897164324886352,0.9895437443373365,0.9879989994709988,0.9890879851621528,0.9884310421925244,0.9878751436900761,0.987566663745571,0.9890198372645884,0.990919951563709,0.9879768591806191,0.9893382888889143,0.9916871636810107,0.9914813655960113,0.991267573413387,0.991565749466062,0.9923138282108352,0.9912224762305474,0.9919946696208971,0.991641470037268,0.9925016317897298,0.9922524587294489,0.9927981907061261,0.9924584136251902,0.9931870432634918,0.9925442884892131,0.9935945843459322,0.9929748029282416,0.9935961883425131,0.9930213696995357,0.9934370876026098,0.9934066735245427,0.9935639732151569,0.9932281101037522],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(220, 8, 37)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09976228632263301,0.10577261795600255,0.08722859998953711,0.060931771123122515,0.05401438327179742,0.05118365428087228,0.046894631155884964,0.04523809368360493,0.05044281642717594,0.042193802161929535,0.06612847684482528,0.04309886100724391,0.047357586133725864,0.043079627681638775,0.04147892653737281,0.04176973017937539,0.05990564008963477,0.03972591080565223,0.05371519256498396,0.04091146250617053,0.055420873137478975,0.03785259771029564,0.03851670067744566,0.04572623870319517,0.03968476442383327,0.05210926103632885,0.042792203231263405,0.037199631031231375,0.03814836135486147,0.03434456301104162,0.034724876294840654,0.041726744692145344,0.037637524199239984,0.040357168303015306,0.04415830727602608,0.03385771071583135,0.03702952815187756,0.03185543423051277,0.034666621165279674,0.03368184766716154,0.033034777244444154,0.03489022253919713,0.03430416884086386,0.03359037067523527,0.03894649773221655,0.038049706284123186,0.035061744754154654,0.03230059839964323,0.038639772219961045,0.03127034680591416,0.031944993143433964,0.030365788086583113,0.029775028319907762,0.029990770314464864,0.034080519435024756,0.03067045406633636,0.03141740621979704,0.0300864890627435,0.031290579018826335,0.034766089055956025,0.031185563515449308,0.02985431164810338,0.029824694855413895,0.029603602967618665,0.02977543277121901,0.029609054383338523,0.029781360995933363,0.029794912792972684,0.029537943640525397,0.029660272298707175,0.02950121219881212,0.02943827324255635,0.029447762337849313,0.029469752375714968,0.029463528104356883],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(220, 8, 37)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9312457737224836,0.9178415543486526,0.9274193570130833,0.9598637261768455,0.9672594580524552,0.9713682066223327,0.9745070782966503,0.9754026263121213,0.9633354640070451,0.9775573115200651,0.950740804431492,0.9722613464694498,0.9704156813556846,0.9747518561438893,0.9764575538270249,0.9767747257675744,0.9554311632459601,0.9764918216234472,0.9608890168798542,0.9768453532494265,0.9633884501836855,0.9796248002004817,0.9768214788675436,0.9704341255998996,0.9761733251113356,0.962028518825725,0.9771508073664286,0.9795261276558964,0.97814856119039,0.9833744984530569,0.9813293070814278,0.974231595513103,0.9769326043799196,0.9759056796546596,0.9714184102059246,0.9804186449356503,0.9791352001544015,0.9822515866659443,0.9835946415120072,0.9808693097970479,0.9806161015818513,0.9815941588850386,0.981619908147133,0.9803408654481969,0.9742342237318259,0.9773143556509447,0.9804975268905727,0.9822914485165689,0.9761403692750688,0.9828865456521814,0.9815909019519201,0.9828656145827347,0.9836716844776063,0.9819819949393045,0.9811883122825591,0.9829519344138951,0.9819861580022007,0.9817149421938833,0.9820710798333391,0.980968214306959,0.9835533019065636,0.9828768979678072,0.983128381370335,0.9831866961982073,0.9824354030763999,0.9829223450066279,0.9829485293021135,0.9828793190092981,0.9821990437131639,0.9834245468036024,0.9824268913641324,0.9831703997825968,0.9831703997825968,0.9827027723794087,0.9827027723794087],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(121, 199, 155)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv5.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12198976886897153,0.07561823331237795,0.06971903153677031,0.057041895049458786,0.049541570488598204,0.04885096793475839,0.04791554785402916,0.0409061263252438,0.041339412856450194,0.039596799949241666,0.04807373952730756,0.0359343096409057,0.03688608143875484,0.034838227911469316,0.035948158854369035,0.03757180260219115,0.03510493305506439,0.0322079645147609,0.03034593419247613,0.03284671145863618,0.02968492706531375,0.03323218000611079,0.027013979227477333,0.02962813767575726,0.02941859170628065,0.02918738866350768,0.03016287159912911,0.0273792267974385,0.02768789327533794,0.033936678929700335,0.0312334622133775,0.028150338856215328,0.026550905599353785,0.027512546161025386,0.024828414355254257,0.028395743548276942,0.02910231031511589,0.024918533761890654,0.0251498003612952,0.02514364567221508,0.024637038656697103,0.025133677251969827,0.024101705971426777,0.024828467336520638,0.0230586970233511,0.023020681274579017,0.023772752897777645,0.023309374225986237,0.022080666859466733,0.022456805330337393,0.021895268761722493,0.022378318229816077,0.02559305043295794,0.021611782499212442,0.021189864112527237,0.021576848243366403,0.021200008344284992,0.020737399249620318,0.020704336056031312,0.020372571724916105,0.020324656270009137,0.021033175436850265,0.020837412039746123,0.020181260939862448,0.020179909655028465,0.019893166679517713,0.019704835859787696,0.01965932684725434,0.01961920590130324,0.019613893389411673,0.019538325853517783,0.019443381288838824,0.019404585096229804,0.01936547415712923,0.019356621235591984],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(121, 199, 155)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9061339094922607,0.9462912459604539,0.9498057157046879,0.9623280084203636,0.9689057463805899,0.969893370251445,0.9691146727899438,0.9753956874786552,0.9740935948359598,0.9753766944620313,0.9665128188430779,0.9790210877401563,0.977689466737696,0.9802822440567371,0.9788714252177109,0.9771548264996108,0.9796343301262809,0.9814494798810058,0.983753498138191,0.9794157885180562,0.9842494438919044,0.9811835254510778,0.986784854293286,0.9851259432595286,0.9839668584003531,0.9848843776532283,0.9832363114480999,0.985960468228506,0.9844008560587089,0.9812130373547331,0.981865153695536,0.9857299535313688,0.986703187726982,0.985780368131824,0.9875511497644504,0.9833743864763776,0.9839626418324328,0.9884442919000529,0.9873364533040784,0.9874789999068592,0.9878207007530517,0.986595553535502,0.9890650786200008,0.9862928058182773,0.9891556306906205,0.9898169640153012,0.9890488282799513,0.988854529134712,0.9900969254190858,0.9897915283421015,0.9913222254511362,0.9904277003544446,0.986973736686298,0.9904589936508138,0.9916725375755479,0.9909136240107567,0.9916911882537452,0.9918551718840029,0.99169744370989,0.9925653230254132,0.991936326464901,0.9915618501693082,0.9917753580181201,0.9925940493570844,0.9919651343016075,0.992361250450459,0.9926987075897431,0.9929141030256059,0.9929235597210905,0.9929177231897075,0.9929356306827011,0.9931841355055644,0.9934794835290575,0.9933369690507279,0.9934314549160594],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(121, 199, 155)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.08558792146722885,0.07269382285805502,0.06116059857750267,0.05499375978695978,0.05920285867763959,0.0530534720144321,0.059253186223023536,0.04655966114291211,0.05008033978723988,0.0510839579620517,0.040859545595764706,0.04587523276658402,0.07097994770176222,0.04935037343157935,0.052993868083990724,0.04254713205435022,0.049261531223546186,0.03632133907264041,0.050786375100772406,0.03599073507685432,0.0414515732745944,0.03468195007685124,0.035052830418667844,0.03290827426150492,0.038024666342129004,0.04333195106167974,0.048609196566224505,0.03362749352590325,0.04452938557541657,0.04046481509286513,0.040934691324676434,0.031803038217050515,0.035661913436610265,0.03079397107420099,0.03632614337599155,0.03552606271202212,0.034074115837664946,0.030312357415345936,0.03164896149559529,0.03024369248819515,0.03148010544402083,0.030973607818900104,0.03397870960155713,0.03487547099641508,0.03037842646804462,0.03365421102116608,0.029598646265329775,0.03110030937901477,0.03235591068812662,0.029366428760933302,0.029524156010847322,0.037535041481051655,0.03942477565604387,0.030007950521211853,0.02899961767788605,0.032249530069401997,0.030461247429200464,0.029118968477261434,0.029369806884899992,0.030183219423203945,0.02966194478250861,0.0350857103454698,0.029195719588663162,0.02871507055212542,0.028952825911778356,0.02848087014867268,0.028982024401733555,0.02897556688367706,0.028606379969218342,0.0285923814138596,0.028454252719059842,0.028478832092276963,0.028463157901341973,0.02846122671085125,0.02846132828453972],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(121, 199, 155)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.001981796079782972,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9421460019342592,0.9509426633963961,0.9628486126484354,0.964739793249542,0.9574929155659823,0.9703016747318046,0.9581981832244022,0.97059748927287,0.9700854030754825,0.9600627923170902,0.9778623185620123,0.9684379824750411,0.9497487452932737,0.9700798934456899,0.9688419411549067,0.9698270754285251,0.9656383156013076,0.9798429870518425,0.9620110143107035,0.9813426254965686,0.9742440936465149,0.9845331987948339,0.9801111552595925,0.9850483979043976,0.9812882213683441,0.9696449584419028,0.9656363826792004,0.9791811359666223,0.9666737582604806,0.977557764752655,0.9755042202736269,0.9823947998404092,0.9800107135484042,0.9850595913676243,0.97961529719469,0.9779682782745722,0.9807905685682939,0.9826339974623458,0.9848983524572256,0.9811808583226513,0.9841743706832521,0.9853190842122282,0.9792443141516545,0.9790715182678151,0.985956717334428,0.9799854839102046,0.9832279409734542,0.9848971495934254,0.9805369282715363,0.982664438383252,0.9857839416608967,0.9776005317241728,0.978282981500865,0.9841864114025555,0.9862408703760965,0.9809822668087171,0.9841896093059089,0.9859905561411502,0.9862676832958303,0.9858382664843771,0.9844646699875459,0.9772596207178902,0.985578267594463,0.9860253774981447,0.9856863779829199,0.9859484748776407,0.9859406578340325,0.9859358578971076,0.9857070188619447,0.9852452794788908,0.9862465377268648,0.9860010043125024,0.9857627423842301,0.9860035426087661,0.9860035426087661],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(239, 54, 151)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv2.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10330151860588331,0.08204444446787665,0.076597182054973,0.07443972210840398,0.07320678643754258,0.0706390974106521,0.07195089634418282,0.06597087624795385,0.06455853695960935,0.059738244181168174,0.06024409356566502,0.062073033443840915,0.057327640342514405,0.05832092802788111,0.059049538943707465,0.05518588936308405,0.051559454487276106,0.05508645995512588,0.05359061714197762,0.05439499693859757,0.0516620303116918,0.050081908937547895,0.053785017378551445,0.05100683697924991,0.04937879714607032,0.05155689955047862,0.048466472398118465,0.04632966669205948,0.04638531674617071,0.04763440815870568,0.04778184098408121,0.046744972155061376,0.04592308083854249,0.047494534612179895,0.04583016997896656,0.04482175795721981,0.04479258764035923,0.043680808593744125,0.04442636271061766,0.042816373129788136,0.04594602824486408,0.043515969037087925,0.04189665082816508,0.04089952682839431,0.04146673872742593,0.040478550627366786,0.03899606993488054,0.04038281004282754,0.03925580134506488,0.04075948582686919,0.03953063501106983,0.039449916161246865,0.03820815331901062,0.037835574024628935,0.03711520640130442,0.03651615702520463,0.03658999972651368,0.03624336077508443,0.03588842709014301,0.03623138523115462,0.03549404380179489,0.03540379958822555,0.03486142586874013,0.03465358610697014,0.034532757940673335,0.034458493194697515,0.03442401051401819,0.03404221586170441,0.03390957737791716,0.033803898255678935,0.033655724229818086,0.03365798630460431,0.033572315040630164,0.03351238164773375,0.0334892222186271],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(239, 54, 151)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9269246428015614,0.9467366419365651,0.9510955969115432,0.952358967157727,0.9528751486588161,0.9536547280148491,0.9528795798036884,0.957964631787255,0.9565709549269555,0.9627817798995567,0.9609611991481418,0.960545505203168,0.9640974618000898,0.961876876852883,0.9598249513819318,0.9646540961384953,0.9663373029746087,0.963823117075119,0.964658246001284,0.9646148946465513,0.967530595674032,0.9675331379026993,0.9642973237528057,0.9675074716834624,0.9674606534089258,0.9665994592093411,0.9686343453077412,0.9708437521055426,0.9711130563486501,0.9694540345763262,0.9697134616208878,0.9704602661453384,0.970356143795994,0.9686040369803953,0.9695600660320082,0.9710234029468794,0.9702825839665078,0.9722671729078395,0.9726427212366029,0.9731025442923137,0.970323229738669,0.9732649490929173,0.9731287638420612,0.9738091916638921,0.9738635994178347,0.9743780653265535,0.975980197446209,0.9751428960256449,0.9756935002032929,0.9733734622534257,0.9743515659801115,0.9765761671344312,0.9770422286813122,0.9769568533076333,0.9778457161576135,0.9791791140437034,0.9770662873569493,0.9778660820276284,0.9789400584723575,0.9781058156718684,0.9791897350214847,0.9794312439751425,0.9798756028059933,0.9794270219938975,0.9798217078978327,0.9799883823375555,0.9804739250462776,0.9801767918276437,0.9802280248631712,0.9808254609738145,0.9804952418485546,0.9812028543520913,0.9808183721054338,0.9807936200408642,0.9806596884299134],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(239, 54, 151)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09102156311273575,0.07791109050774492,0.08272272995657118,0.08019244369772292,0.07961504061504737,0.07880395185906452,0.07467158980078714,0.0658742428747649,0.06664634504166665,0.06447739157172823,0.06479395833826557,0.0657843953606599,0.06220072093595754,0.06158210708410879,0.06296126264886757,0.06207959873160136,0.060150989347307136,0.07199032989769047,0.056815850734710696,0.057765923706731434,0.064204727645797,0.08717678498566356,0.05556122129110946,0.05586500385475322,0.07709263181153851,0.060261700325405475,0.057275246974733685,0.05752436689187571,0.05224078018794355,0.05164414229722777,0.055671905434315966,0.05476287328398105,0.05139448331905804,0.05408702638653136,0.051168475038919255,0.05033057214775446,0.04885710062542322,0.04995789573723098,0.049534978371920044,0.05269388144010121,0.06401097117523147,0.051571506860944416,0.04973408361941679,0.04889330337291321,0.04875510027984164,0.047150946300976056,0.05235295927350464,0.04789209167930678,0.049024764064353764,0.05009904818231707,0.048355015195726936,0.046586317855775154,0.04750691117187546,0.04502353320519129,0.046693349516985755,0.04458944256516666,0.04527327668728288,0.04486284481547133,0.04480764672569802,0.0447130911052227,0.04454628065791736,0.043953748843616636,0.04365014930054085,0.04405998712296748,0.04327751434443333,0.045276051516487836,0.04349710288428769,0.04317122198308456,0.04421497835070407,0.04308517212976295,0.043161703922699406,0.04310476603274493,0.04307553773278633,0.04310971201234257,0.04310520333830024],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(239, 54, 151)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9437437550901193,0.9548447530718547,0.9447304133161041,0.9445162343037943,0.9504579212878661,0.9560916069696087,0.9503937090569422,0.9568375035380141,0.9561113759327348,0.9562799128449249,0.9569387652648516,0.9558533027839629,0.9630496741164714,0.9573752743481155,0.9603097796036308,0.9639625588347198,0.9598275634193155,0.9551891658800861,0.9637207942978259,0.9603653944554483,0.9592867042409997,0.9434698838081783,0.9615566831247672,0.9623729738576903,0.9513370020433958,0.9663581024783967,0.9666571383866637,0.9660557453263426,0.9676795333823592,0.9676042863302033,0.9651359668915447,0.9657574954634005,0.9674917656454759,0.9602503609587361,0.9709744208236119,0.9686459795604629,0.972018516894951,0.9685433337218229,0.9728492084547711,0.9690203075230032,0.9564475954522474,0.9678677771650699,0.9728167456441792,0.9719531337407856,0.9705584581268256,0.9755695274373297,0.9650530904778944,0.9731386368445704,0.9669035426280355,0.9724804175811663,0.9659616880431267,0.9704035514137055,0.9704579142462914,0.9740206483760446,0.9767024227510546,0.9770734221285565,0.9716331357767672,0.9764814106182605,0.9729063137791175,0.975560496691191,0.9735596911417678,0.9751128081090307,0.9752933305606395,0.9762691073737573,0.9770934635224776,0.9749453508885401,0.9758427132592467,0.9766624487784943,0.9762862281289341,0.9773535743876965,0.9761084192552565,0.9766179007193072,0.9771036571071988,0.977114813226521,0.977114813226521],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(248, 219, 124)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv3.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.11062453858392209,0.07676399897395676,0.0669058007438841,0.0649027240866921,0.06344677455987854,0.055301426950615976,0.053623883200026326,0.05513859103345243,0.050081218773966395,0.0445909882610811,0.04830174858149521,0.04548752159050923,0.04770873986090718,0.041333432324264806,0.04396150097899557,0.04422525093130093,0.04288015543970729,0.03901868203243098,0.040774270299851964,0.038653426092514875,0.03673303814269901,0.03695148012695785,0.03661843673299405,0.03779940869902417,0.0351239121558501,0.04013331107035672,0.03419776070833138,0.03426941959582107,0.03658242063278936,0.036376048681696395,0.03365551849607408,0.03358061118531473,0.032225192928812624,0.032598308956022254,0.03289215910202586,0.0317569404759392,0.03136074672979041,0.031033017528152684,0.031405997444008224,0.030954717137218205,0.029616974112401714,0.029113786484948,0.029141926669002536,0.029463102797394083,0.03028943517201613,0.02862768681583348,0.028468698362168032,0.027480958843388234,0.028066787145016255,0.02762757559890463,0.028053233571103075,0.02743204783473296,0.026785834612834767,0.026424793303354226,0.026481676978386624,0.02609640000449901,0.0255590647587047,0.02554786781922307,0.02487190444543019,0.02507070463773755,0.025378189929776994,0.024883433233387556,0.024574106392308746,0.024483280896048093,0.02435268951155339,0.024239238132247945,0.024075603926338826,0.024100378832917442,0.023924718351708243,0.02381741098317084,0.023764506324630695,0.023740112530611635,0.02363862741892178,0.02362542678895412,0.023593976547918095],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(248, 219, 124)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9157370080383527,0.9508295031814734,0.956544901194018,0.9556210084401947,0.9583412190395143,0.965702822917369,0.9640466178284818,0.9618717156079014,0.9678893030517014,0.9736494456790241,0.9680547045839701,0.9711438526175288,0.9675858933972653,0.9740858839124351,0.9709656030990115,0.9729074732388219,0.9735171370971502,0.9782224526861985,0.9751742219995462,0.9776097137460777,0.9799905563074026,0.9798044007669631,0.9782446212552287,0.9773158531674707,0.981070792600385,0.9745036631908877,0.9816205086721604,0.9813568214202351,0.9790318582364065,0.9808402137573748,0.9819359221052798,0.9813422027148977,0.9837955743815692,0.9820908490866482,0.9813230607759431,0.9837767983842106,0.9834211942112294,0.9839984773230964,0.9832049116627087,0.9838323149981043,0.9854582802397877,0.985403259991564,0.9854932498801035,0.9848242670179251,0.9833144458213107,0.9857380210629125,0.9871975522399283,0.9872796924518152,0.9866110312034784,0.9868989381076227,0.9868359856349952,0.9868330368560423,0.987704499743366,0.988922961344236,0.9885880230312584,0.9883896256854083,0.9895801934777269,0.9892077087113944,0.9898202187436171,0.9901395517693214,0.9893749210810361,0.9901696408852072,0.989687767085522,0.9902531723071032,0.9906807041212268,0.9907308053911478,0.9900006371195607,0.9910110232185614,0.9905422373668357,0.9907206426584221,0.9910112206402839,0.9911174012056947,0.9908557692839217,0.9909992007620922,0.9910478225406151],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(248, 219, 124)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.08628554683677929,0.07315603763479547,0.07800954433855732,0.06386390801045493,0.0653157942739549,0.06328057340023034,0.05527759969132053,0.07005392123026537,0.057476942876155436,0.05087924113695564,0.06055751670779232,0.04946788208130299,0.054750984126247494,0.06324653662357134,0.06310853709265128,0.05686646718851889,0.05854132488830802,0.04748608119452942,0.04672979699172515,0.04268967619620238,0.04695873608447842,0.0474688303557663,0.04571050901439591,0.041928931005631935,0.04222093221709081,0.04295576198148154,0.045794859538782914,0.04322936008988377,0.04427591364869137,0.044470266265557806,0.04591690334155388,0.04207502179283047,0.041127444624491166,0.03802541443162767,0.039900587396728214,0.04384250032021008,0.03923486397694476,0.04089127046750583,0.03839249821872646,0.0366914786065567,0.03871097732911405,0.03774960422792385,0.03956838867457462,0.0397984311778316,0.03969059256497527,0.039127130517127996,0.040026258633718455,0.038465853501431314,0.035260258737913115,0.035556941028825194,0.03490094260610256,0.03484992030303913,0.03591677191126388,0.03512357604462666,0.03598860224958548,0.03542335533962627,0.034222962051834845,0.03470578190336113,0.033777582048857745,0.03481237330182721,0.03389105761737348,0.03410766140343397,0.0345389438351405,0.033604076617362165,0.0336789574000434,0.03381912406162708,0.033533977066835585,0.03344533270884215,0.033537996301741126,0.03369392394968324,0.03357873951497766,0.03362006317964944,0.033613811172160904,0.033538659011682695,0.033539992017844286],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(248, 219, 124)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9457853900161861,0.9572180427801988,0.9475847635282458,0.9630472991905469,0.9573413404155584,0.9650808116910375,0.9690443691387913,0.9455264478970727,0.9615054699820714,0.9699308251621158,0.9620223774242457,0.9720154369777464,0.9646533937010842,0.9623796503661601,0.9622540724600139,0.9652664534726005,0.9661899051414791,0.9743997053433495,0.9752939751253098,0.9786710095826098,0.9727927431404745,0.9753725961703137,0.9766851217241848,0.9791950130130251,0.9745621870003448,0.9759657403351353,0.9753607246133019,0.9766862386224838,0.9761272262933045,0.9734401533148354,0.9722252761884331,0.9780860326155499,0.9775877172970105,0.9800054821761943,0.977648811583821,0.9781679845491998,0.9789583282849574,0.97650943873933,0.9810420000978742,0.9817406019188706,0.9773172278292065,0.980291647841766,0.9747846077245732,0.9804932773031434,0.9793889024173955,0.9774520911898888,0.979380914836942,0.9807104807963779,0.9823965583232642,0.9821778419176819,0.9838634191976732,0.9836723952183128,0.9821508726199835,0.9833522761010943,0.9820650013207238,0.9831941779057777,0.9838778750886936,0.9824820644171264,0.9831825948305312,0.9813988045551686,0.9829303335722325,0.9821909581109386,0.9833434499584033,0.9833736166418393,0.9823968589493116,0.9838451126800557,0.9838326708193805,0.9838632127281767,0.9831197731200247,0.9833993643712837,0.9836043077083322,0.9836328236610721,0.9836328236610721,0.9836230604690431,0.9836230604690431],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(233, 142, 105)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv4.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.13197925424145668,0.08574332833665615,0.06523009319827046,0.057172296842280666,0.057076515055945375,0.051859466515626555,0.049565983247033235,0.04455516967508528,0.04344704021771203,0.040206351350963045,0.043210090541911286,0.03762018974328506,0.038973519949398225,0.03568912317799266,0.03564641549713901,0.03774706683306179,0.03845604825658214,0.03537956997325467,0.03638704625307182,0.03151275656076163,0.029608605731223182,0.03139801487702138,0.031089051105380742,0.02813314265504326,0.03128332127470263,0.03468338972211465,0.03275869397525457,0.02756257136133665,0.026327405660439875,0.028718429295194654,0.02713417446780587,0.03036320258277593,0.02819661667862777,0.027441174523288304,0.029220408013459608,0.02538441789826167,0.0242222330676987,0.026443271757981484,0.02408064530195854,0.028172924225341275,0.024254175895805825,0.022607482828729,0.025809259483759277,0.022723155487478122,0.024364691049304343,0.022649822938005267,0.02394592405253123,0.02275097761537884,0.021663703575713254,0.022175923909432974,0.021571270564201907,0.021078279328544253,0.021391293685989965,0.021127790350035976,0.020625673778534345,0.02025082999859581,0.019825086926983804,0.019280032297001058,0.01963975402992752,0.02013277890265227,0.019522851662878864,0.019508496224282808,0.019435605991371445,0.019154057162947193,0.018749289837626384,0.01869208268044604,0.018661794560574173,0.01855995704047595,0.018580028225901407,0.018390966751500707,0.018364499195693284,0.018275240960852045,0.018190003459993644,0.018201124853105686,0.018175715862646408],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(233, 142, 105)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8937584749408112,0.9381315892409421,0.9544619518322333,0.9614752911407831,0.9606024830659673,0.9655226160875698,0.967405607183276,0.9710506102427564,0.97358502516161,0.9755009626677609,0.9722016049524962,0.977182540567051,0.9762091513929257,0.9785521067629122,0.9779033422334495,0.9784638242386762,0.9754274111569752,0.9806336130285467,0.9774427883597229,0.9829388945023484,0.9833924090485028,0.9822253537210374,0.9827326466797965,0.9856177811011977,0.9822420485563406,0.9784228355959542,0.9803330014404921,0.9858083201084906,0.9868530740165775,0.983726476935376,0.986400100961093,0.9813821321150036,0.985481233447405,0.9846341492182965,0.983456342428307,0.986903739119442,0.9881321543311723,0.9847318856352936,0.9888017811471437,0.9840440415529227,0.9877005433903094,0.9894582608119102,0.9855207750632358,0.9893676802645347,0.9885101108267699,0.9895473744093536,0.988792336703263,0.9892655638902312,0.9909324150695195,0.989836936299805,0.9907226485306205,0.9917039223596462,0.9913437704782295,0.9912469955749995,0.9914347102835189,0.9925626706162809,0.9920488905196688,0.9927137900575892,0.9922249684789698,0.9915266705339448,0.9925158067749837,0.9918849828414227,0.9918190354204321,0.9923797070020298,0.9929621106434101,0.993290294373604,0.9933639290693649,0.9933783343519853,0.9926670922773639,0.9934158214743198,0.9930527715233879,0.9935652182700512,0.9935386579897473,0.9934389163541505,0.9935706945044813],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(233, 142, 105)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.0978984444514173,0.0741756007792204,0.06111302032196235,0.06725865388449115,0.06077525747805527,0.05675373832179918,0.052541912818180325,0.05101397322471609,0.05027002197309458,0.04148485672842596,0.04462509319135004,0.05504776640343912,0.041491243450604766,0.03978756025843194,0.04833164786555103,0.07327252121520617,0.04063581138900465,0.03862245116139605,0.044614380769610815,0.039212732639202136,0.058648364906458514,0.03586934678947803,0.04523936467328432,0.03588711622961608,0.03715132593698928,0.04352008738263776,0.03871733116120407,0.035943112458355235,0.036595995217254484,0.03898739684181115,0.03853384038990306,0.059085518572338666,0.040302963414040625,0.04032645286716956,0.03624512974134426,0.03260733442208202,0.03212077352650387,0.03306524916407988,0.03298345467786199,0.040213215333182374,0.03232280538510211,0.03285253233311512,0.041045362041475845,0.031150897812495117,0.03361018463918024,0.03207215113994182,0.031645340615838666,0.030351825644777404,0.031189322394808542,0.03201671174352931,0.030134458074352586,0.0390862997736513,0.0336050323988359,0.029985232472010084,0.03030912982127101,0.029535133386498054,0.03010491558264211,0.029400387846727143,0.029504857552829885,0.02964973234894759,0.030308328297539676,0.03207461934063033,0.029530166355809805,0.02976289533104274,0.029487392078457828,0.029205507127075262,0.02924953332983751,0.0291312685511571,0.029330760066451897,0.029168610131412847,0.029131769406836466,0.029158181466699875,0.029124712478049434,0.02913221701262743,0.02909733103364194],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(233, 142, 105)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9257940478394466,0.9556384728509326,0.9651798012214157,0.9502306139315269,0.9589020784060712,0.9684315815223731,0.9694368375763532,0.9666518710004269,0.9628413102274527,0.9789097436728494,0.9775202533615216,0.9629612835371002,0.9755180714004309,0.9808474660742083,0.9743285248106873,0.9461821114890294,0.9770205393466286,0.9801702244769713,0.9754794956534285,0.97524151890184,0.9605860326011689,0.9791868914203049,0.9691493604886107,0.9798626007116721,0.9799692284584278,0.97594092141025,0.9776256144209329,0.9809287896554146,0.9790465423751781,0.9781994476455577,0.9772886019359597,0.9623712303665182,0.9739719292853537,0.9775205407393155,0.98206718815657,0.9829902688612098,0.983361888084695,0.9825428957836785,0.9820733875133524,0.9776139060235768,0.9814666144744998,0.9821554189073334,0.9760637278538669,0.9834182808192935,0.9820973769321333,0.9826347421702367,0.9845632486948253,0.9841949523745159,0.983835224884842,0.9821525438716019,0.9843490751617527,0.9752853383741192,0.9830544310891062,0.9833582444894404,0.9830048319430763,0.9831878438205337,0.9832541956756811,0.9843737987804517,0.9834703167782924,0.983605816890825,0.983025966077833,0.9832883616981146,0.9839234962783648,0.9841509563438355,0.9839289572393124,0.9843994780817342,0.9845856510337839,0.9836234897975367,0.9833643752134409,0.9843665231318448,0.984837378452415,0.9841557193950031,0.9843451847803184,0.983911564352684,0.9841416472549155],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(66, 225, 173)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv5.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12953605704513066,0.07702992826631935,0.06223028193463711,0.05471536578420648,0.05129860583756798,0.04735029038182648,0.0443787270548827,0.04343660320544707,0.038379301724983175,0.03893430721770037,0.03804828039149648,0.037762373040631875,0.03780442235046168,0.03633921976795448,0.03331651495778155,0.03109781303851602,0.034089848644101195,0.0309792648138903,0.03198147932449431,0.02994744598850574,0.031071460854693103,0.03106400345079814,0.029571933355203747,0.03160506909775024,0.028210491553068436,0.029762742464736803,0.026571438590839148,0.02694750965187776,0.027960571924913653,0.028092425815197816,0.027331352292774468,0.026893257913003673,0.0255896651373807,0.025284408776245302,0.024770826452922712,0.024772383628107974,0.02479411088834855,0.02439364095742282,0.02325171448529313,0.02548768430316708,0.023370289292617488,0.023132474369711893,0.02412026752413753,0.023842922261439237,0.022394300725861512,0.02314262535734548,0.022980826904939623,0.022577523394889984,0.02180186708170797,0.02252873799978048,0.021480365608736684,0.02215988366520043,0.022021443325970728,0.02201636974429893,0.021205937698961397,0.020868809183447152,0.020634698760903784,0.020759288819863077,0.020904072036567696,0.020446243843774206,0.020244664217069115,0.020143194795550965,0.019953421627908502,0.019923540664770624,0.01983133892892494,0.019785358487518518,0.019762159201219456,0.01967636403571153,0.019587968766928265,0.01950524997985991,0.019468733575621795,0.019432913927192815,0.019383057345340682,0.019352018235108355,0.01933645226492334],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(66, 225, 173)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8961009162003805,0.9455351361624211,0.9558813856335401,0.963429941287008,0.9663476977306206,0.9690823176735184,0.9720001620776921,0.9727000613015739,0.978196007792989,0.9761670216816466,0.9768138084810799,0.9768110335606431,0.978107045103715,0.9786036899581552,0.981570501187967,0.9834274218026182,0.9813868742389177,0.9834558346533316,0.9822027615375718,0.9846550176546595,0.9833597992527143,0.9839910794891279,0.9846106073727826,0.9835040835137172,0.9865358142622818,0.9838763147880049,0.9878962936313673,0.9864090098164354,0.9850342020485714,0.9860323801216215,0.9854788244999629,0.9863192145587607,0.9871699552632076,0.9885428197671332,0.9887888030832102,0.9877878691102746,0.9887951156398742,0.9892011309524092,0.9897004492603166,0.9872492940706123,0.9906973448790923,0.9904984887049895,0.9888764693751535,0.9891638410524041,0.9908814888004194,0.990209812644875,0.9906719618515706,0.9906865242966573,0.9917089988920693,0.9913889045903863,0.9918139449147334,0.9909427667502488,0.9906541449630174,0.9917574184683683,0.9922260699632994,0.9925739194076111,0.9926234446544561,0.9925630160247254,0.992636448618791,0.993109250914964,0.9927849601943164,0.9927716052342705,0.9928272797371969,0.9934813438962571,0.9931832864093962,0.9932436025620243,0.9930136622801728,0.9930777063522851,0.9937545598955501,0.9933076228470195,0.9933483056289225,0.9934003554337917,0.993453035399664,0.9934091137183239,0.9932869896217585],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(66, 225, 173)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.08877442768451684,0.06525104097074659,0.06225525190944934,0.0556419434825989,0.05402778358137894,0.046697834646169264,0.044673138175838185,0.06252926607004965,0.043858815401894936,0.04046302140527165,0.03999967844676726,0.03970479317188673,0.058474873876858416,0.05726383363258388,0.036745785732347123,0.036892771533981635,0.03537748077327443,0.03808116847041137,0.05240282442459126,0.0370761835288346,0.033489689727624256,0.03796937433151445,0.03372743776727378,0.051998164374189276,0.03570070476159198,0.03591310408772881,0.03278754063693109,0.03267629704217321,0.049111560004990534,0.035961732634461625,0.03962280551336475,0.035377493262598196,0.03936984609697283,0.030560530181910163,0.03292431713840396,0.030312119616368382,0.030263444522709373,0.03167607628602752,0.034758909953009225,0.03407586678043264,0.031146906231789246,0.03125779176854186,0.02983335618096119,0.030832966931701934,0.0312284342458158,0.029768056138274594,0.0320360025051738,0.0319055050893133,0.029590734591086704,0.03138918853604916,0.02931335667769114,0.031156515908712375,0.030385131944495786,0.03136216405894338,0.03164294273736551,0.03027172197693402,0.030764817838201817,0.029194439502106498,0.0304744726526983,0.02988859559969394,0.029072569610224558,0.0290347848868452,0.028917448511955254,0.0295322464426973,0.028834585973487276,0.028842505825446643,0.028715168538781786,0.028767207198331445,0.028847517493049712,0.02867173720727262,0.02862193492013974,0.02865627078000213,0.02865524028277479,0.02866009313267531,0.0286596456344185],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(66, 225, 173)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.935329095658429,0.9594017660805316,0.9574261070870923,0.9660678128144552,0.965471595430493,0.9746633039827786,0.9784970369159499,0.9560939145436679,0.9717548539957116,0.9766191212132372,0.9748049285667066,0.9796752036458347,0.9554045691220263,0.962581885984785,0.9784538736613407,0.9818734243071153,0.9797783354948523,0.9788151425202615,0.9650904209147161,0.9760443596535132,0.9829168570949679,0.9774432151009845,0.9831134233901181,0.9658464987907722,0.9790934954178148,0.979917359133338,0.9827154174562199,0.9830992625695989,0.9648497464020901,0.9781634075535831,0.9763981530243969,0.9800664752187717,0.9791827729213441,0.985505614778408,0.9848561884926634,0.9862722930802797,0.9857644364784088,0.9843067185938628,0.978393484896224,0.982602515441252,0.9849664533430639,0.9851196130524412,0.9854973411450583,0.9860387146631608,0.985028570099058,0.9860357213026131,0.984435858785822,0.9818190169449681,0.9869792455311013,0.9860187350860917,0.9852729412645378,0.9825379661527281,0.9835229035513787,0.9813603010733641,0.9851636407852127,0.983060946891019,0.9853548555846334,0.9864968485019938,0.9858288613185829,0.98631059140272,0.9848163829855352,0.9859979662040159,0.985975691716019,0.986290748463105,0.9864947792898086,0.9855018092290997,0.9855257532099475,0.9865023027301523,0.986207266255819,0.9862325375411466,0.985768273240099,0.9860017183448948,0.985768273240099,0.985768273240099,0.985768273240099],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(110, 241, 145)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv2.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.14171615115623976,0.09219157607011916,0.08631788152531386,0.07590762293830361,0.07206299947521394,0.06805156830864674,0.07500535827834172,0.06592646544673325,0.060168544539300574,0.06050690081725825,0.06362157862586527,0.0554956540166649,0.055112438563167845,0.05679372928072224,0.055123502589582984,0.060319588705470606,0.051115019570818473,0.04883247063775789,0.04643646018731089,0.04753678438231298,0.04864705181536601,0.04686390587554355,0.04491265248869976,0.04555351226019832,0.045290206539159786,0.042952278237640515,0.04371916709248006,0.04298849131438331,0.04376709316809153,0.046839461432085006,0.04225842161072079,0.047405173397385106,0.04162521684565768,0.04584324731782744,0.040693117730225624,0.041266027826043473,0.041816105362441805,0.041644629148205256,0.039760108299783825,0.03801451927351788,0.038204648477992924,0.03883626380378438,0.03723112931315022,0.03712848368157363,0.039354519363821304,0.03692650048217959,0.036699692378373214,0.035838565736036124,0.03664034098506247,0.03578529932961841,0.035592024938927755,0.03488823129728283,0.034922351163277514,0.034140524287779576,0.033694786911260634,0.03396030060247116,0.03371546538920064,0.0337210894015032,0.033424972356611786,0.03398115745695459,0.033372071316254505,0.0328582229734935,0.03291133181506689,0.03247702378877796,0.033253696792143454,0.03237111579033654,0.03221050445651817,0.032122743414567716,0.03192219798371008,0.03197575176619036,0.0319092094864086,0.031813437062798905,0.031715131440932805,0.031691469673711684,0.03168041821983125],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(110, 241, 145)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8928376765985067,0.9362512041851022,0.9403571049312834,0.9523248957604293,0.9525946517695268,0.9539312812044773,0.9491204533749318,0.9570229210976666,0.9607665117061582,0.9608934600109332,0.9555885546145411,0.9662152230556126,0.9633950453088204,0.9621001954181748,0.9645800704982008,0.9604380236765637,0.9694160887026444,0.9691648817546382,0.9714781411853058,0.9706697200557922,0.9703791251470125,0.970282073676419,0.9726340808695161,0.9717261630849924,0.9735893277618618,0.9733020763345921,0.9739079801665962,0.9743632400777675,0.9718589232497133,0.9683700275823025,0.9738097076552003,0.9692246060975244,0.9764000477158143,0.9709468902234194,0.9771265926807506,0.9761795899072517,0.9752952576613656,0.9748100333802908,0.9765142175817952,0.9784409278051741,0.97759585070884,0.9785517971169871,0.9800898207931388,0.9800024930702408,0.9769311430640721,0.9803653071280027,0.9798416025192472,0.98037840888789,0.980019861004654,0.9809816252563668,0.9808411146607934,0.9824234285175856,0.9822129793075355,0.9820819574339424,0.9834795200046226,0.9831778487262114,0.982734715599347,0.9830563350868469,0.9829030500873142,0.9832544571033752,0.9837685735514856,0.9835828779152099,0.983845320816362,0.9838726731353936,0.9833281603698432,0.9835246686474806,0.9849511870895509,0.9842521806745134,0.9851580008007388,0.9843395608799883,0.9851921072899809,0.9848711122761873,0.9844850732557815,0.9850288178221766,0.9847595184432877],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(110, 241, 145)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12393287855530113,0.09837019264800442,0.08623433140237717,0.08164658517259912,0.08400120044389542,0.0826813500925028,0.07091173856323937,0.06513046973042472,0.06542430585090238,0.06343846118327269,0.07030179439131747,0.06917494248688426,0.059074156416445665,0.060978565951393233,0.06714824459089856,0.06443891979984401,0.05500102860667452,0.05538070466868656,0.057441540420874695,0.05747162459795827,0.05164966803953001,0.07318250003856482,0.051282513205640506,0.05416313154027634,0.05471522097325407,0.06130018174853112,0.05373964120483481,0.056839446349651955,0.04803000208489674,0.05555738322052759,0.051621993920442574,0.049144650324923066,0.052482962377907075,0.05024352550660212,0.0499288578008868,0.05805497558153782,0.04956902288079672,0.05008803270117114,0.05343473499378388,0.045864588319231146,0.04721802066998793,0.04825652387390022,0.047378556074452974,0.0478828394638304,0.05252488310292005,0.04675197009624484,0.045570527869708756,0.04524231391264401,0.045148498006805114,0.04508848983500012,0.04420924140876511,0.050491259216340546,0.045099862347456186,0.04463029103385624,0.04409328761840194,0.043965068989500554,0.043443994275278244,0.044578422011993184,0.04420570276703212,0.04371095866067303,0.04361043027842168,0.043292558011422864,0.0429453013484011,0.04265986018369288,0.042814931616545544,0.04315400643600631,0.042810771156012804,0.042343038256532954,0.04233403480851773,0.04227111443518773,0.04234252258418352,0.0422661672931971,0.042256436712041344,0.042286394969192155,0.04227857816311502],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(110, 241, 145)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9002417295893715,0.9321680995850032,0.9467442808992363,0.9455763728159065,0.947923384225681,0.9390139301269411,0.95969335570024,0.9622990323494188,0.9583808194217337,0.9593707143952774,0.9562034837774769,0.9528749681412044,0.9673152676529622,0.9627881473646281,0.9543693798009847,0.9614035368068196,0.9661702201554898,0.9713185141074496,0.9653359135554584,0.9652418766055156,0.9728724332507944,0.943522091515531,0.9741736503314055,0.9665985606649119,0.9649600879035645,0.9625717432708122,0.967313830803143,0.9592309178747452,0.9725980997916441,0.9628553808706133,0.9701665411090825,0.9743067688472669,0.9670297592640444,0.9736393372253982,0.9728686093614285,0.9623300394257253,0.9734200432142854,0.9673048562882064,0.969120451035015,0.9767713558120654,0.9765042762511725,0.9758812363235594,0.9775424247937592,0.9718730830391812,0.9697904875314705,0.9756162633291963,0.9764242775251444,0.9768579979523675,0.97659304211207,0.9756582430979953,0.9780680163717187,0.9673988852495083,0.9773142889016035,0.9752407683297973,0.9785947023670105,0.977345627356852,0.9794729512171804,0.977089518992678,0.9763972422848799,0.9778507097269012,0.9768087175951097,0.97931889943321,0.9777036908332805,0.978927348079715,0.9792400479751789,0.9797430418452011,0.9782850809339361,0.978253802216361,0.978493036267643,0.9792136918879136,0.978493036267643,0.9787339834971905,0.9787383240531906,0.9787383240531906,0.9787383240531906],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(85, 166, 129)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv3.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12533011656651383,0.08035532864747315,0.06672465307606044,0.05790575092822689,0.0519832382736064,0.05302542477231665,0.04670441453809181,0.0461615909740141,0.045621180157566533,0.046009049886349004,0.042897006490386226,0.05123567756303807,0.039855511606012695,0.037667798868381556,0.04104054410091654,0.03972580745850369,0.03675408927469865,0.036810818649103,0.0351599195999822,0.03510536861148282,0.036436601998743184,0.03358901822331435,0.038939252786790654,0.035968391078376,0.034699561649171755,0.03293525377525292,0.03252401191822305,0.030698508509826278,0.03512970939415154,0.031149700990657215,0.03209425439633523,0.030519309370241898,0.02803264824078255,0.028572048636458448,0.02961507566008644,0.031139167690092756,0.02933580174099095,0.02797461609523594,0.028225957746921534,0.027530416687534438,0.02950188083710727,0.028699158340317003,0.02948488729546126,0.02706712637169186,0.02718994396761111,0.02615911788845868,0.02681675826467121,0.026072758839029354,0.02563273568145871,0.026099513032882212,0.02589304803829013,0.025259836293321705,0.024942839479784377,0.02470139392970593,0.02434100160356035,0.024296602007227802,0.023787465023318518,0.02452483736554679,0.023565304157284663,0.024422613449420306,0.02358139834973752,0.02325655063454228,0.023097917645367425,0.023262073333884022,0.023016191926386896,0.02295057624450459,0.022572721040808732,0.022794518029191378,0.02258815756248991,0.022481125027564313,0.02258666616833565,0.02241047078205906,0.022378366447362158,0.022358895661170677,0.02232784209228873],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(85, 166, 129)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8988630338773834,0.9445370230786386,0.9571191137951417,0.9632073367468522,0.9675563844347285,0.9642865475061091,0.9713020510169235,0.9708547325546739,0.9707402021269304,0.9721409320858921,0.9740562583135164,0.967301890674168,0.9767902982163983,0.9793137944607422,0.9752685243247707,0.976368918390082,0.9786937161218184,0.978785879329211,0.981650666083551,0.979493693858874,0.978033347050228,0.9821249372419064,0.9758767316834652,0.9783397353567757,0.9808347960148873,0.9825071495769131,0.9812321362603585,0.9845131206053668,0.97913380383766,0.9829169530408838,0.9822811334732829,0.9836195469069983,0.9869319930342487,0.9857318010585369,0.9850909156959804,0.9823100103196205,0.9843635355338981,0.9855292721108632,0.985903425564965,0.9869955211567049,0.9842995415203553,0.9850845966632604,0.9842718504324626,0.9868250728333897,0.9867697176159209,0.9883238099658906,0.9865975233215393,0.9881062830434864,0.9886831192832931,0.9871861433680906,0.9869219806874632,0.9883252709113175,0.9888557483422323,0.9889911474488476,0.9892674141061222,0.9889271433265894,0.9898277286951732,0.9887107305559033,0.9899744526832942,0.989316354733855,0.989661573761289,0.9906609934120192,0.9909052927149073,0.9898401605809567,0.990904173392894,0.9898984893362532,0.991081018901951,0.990690743590876,0.9907379869504124,0.9910774799591302,0.9910760970103607,0.99104088524114,0.9909514921501538,0.9910082562344136,0.9909485806822614],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(85, 166, 129)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09714864714141563,0.08912711361122296,0.0637578217946377,0.05762870823804456,0.05783040666395856,0.05760907609642986,0.0494503210116293,0.050857699283517105,0.052488840376490986,0.04789409380649373,0.06671005089053583,0.0486805195020851,0.04752623722264447,0.049836851054757735,0.04664315922102567,0.043879593309667925,0.044871801094398464,0.043989133765709766,0.044118369255483764,0.04279106049910444,0.04317510319534446,0.039784283703033045,0.05345031218891291,0.04108896886872262,0.046890968483747894,0.041884323411790776,0.03882085106934059,0.037452140371107154,0.0411309630984498,0.041767907211768256,0.03892510174117547,0.03627298607961419,0.03842228802874736,0.04243674817722278,0.042916651676917814,0.03558266116018148,0.03582369982940225,0.06058780102385688,0.03580512207807954,0.03532633257844194,0.03691673242451809,0.05296508740518511,0.04310273801287015,0.03417161811411995,0.03409891486731182,0.03408032161314872,0.03333732326262186,0.034878666985690386,0.03751930142800833,0.03237285312405976,0.032185179847724656,0.034406393996833526,0.031735684341171765,0.03196430548615882,0.03302746614844529,0.0322361203377804,0.03635794552126291,0.031352189487915266,0.031602665239183356,0.033614638158443455,0.03202731058615999,0.03152341421219901,0.03135269328542182,0.03230602458836287,0.03170970673976895,0.031186512706615672,0.03164725473912311,0.031077341195131905,0.03101129557719755,0.03210345707142476,0.031080072304022682,0.031045132335211403,0.03112766238831982,0.030998773857490305,0.0309834693177664],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(85, 166, 129)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.921765090273169,0.9338991851476864,0.9621138292107295,0.9679354961676644,0.9677200722002232,0.9678718196297252,0.9720111742463847,0.9747585209314414,0.9686046866355711,0.9754611187776331,0.9488469714510331,0.9734103242005007,0.9738113008735819,0.9672082367539491,0.9718757337819044,0.9745135693270578,0.9738462393641163,0.9768328899505166,0.9724959177778174,0.9775715479053376,0.9724621783384272,0.9799048063096321,0.9581365968329079,0.9798533090860705,0.9725485193081503,0.9737718119876512,0.9765385596465755,0.9782624365681547,0.9779850276500454,0.976954855821301,0.9784377951776834,0.9811065535167918,0.9788265874478855,0.9745572946346516,0.969605580264563,0.9787853404886566,0.9789298508900438,0.9490580545572802,0.9803267173181349,0.978942479970532,0.9783039677093227,0.9594090929741886,0.9717036284957432,0.980735672520117,0.9789749083263646,0.9799984761739876,0.9812924168786524,0.9806651570336804,0.9790978518051338,0.9814791264431371,0.9814786571442501,0.9780028304190946,0.9833759284616425,0.9814427543176882,0.9796750973874759,0.9812304261313717,0.9795812099345379,0.9821922702899919,0.983163745854683,0.9818401686018557,0.9814416473750704,0.9826442587546963,0.9831756008401435,0.9830032062544166,0.981424927699544,0.9822299116795205,0.9814206247575563,0.9822051427878372,0.9828940485170028,0.9822907376211892,0.9827059401343073,0.9829360230365388,0.9829434802297736,0.9826945776260607,0.9824500585192824],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(177, 6, 100)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv4.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12964545144077566,0.08283401713594538,0.06924958649301584,0.05959269659762257,0.054896979387273497,0.04843765216253331,0.046426848078067635,0.04297109961612118,0.04128196647759839,0.038256253201390734,0.04750984688753525,0.03563767755478791,0.036408969407894766,0.037095964701690486,0.03327639181452791,0.03472970131174464,0.0316077718244522,0.031688701090875494,0.03186082311682958,0.034044353404047156,0.02931412789556715,0.03072471723199437,0.033751184234495905,0.029690792056959108,0.02895989256472522,0.028737252970297587,0.02929398931183117,0.029930802503811943,0.027082046628518954,0.02733829836153083,0.028483439505181327,0.029027005603359874,0.026165709969718842,0.02747823604257758,0.025929613285116587,0.02731828577453772,0.025147039914356475,0.024167508583298254,0.02384768094854666,0.0255184858209557,0.02428212411304827,0.024092822897441062,0.023594850491420852,0.02462980087296045,0.023230295410680606,0.023617890523906632,0.022802426186598248,0.024389590474457948,0.02373173930698244,0.022466506567684442,0.0240656756590049,0.02287756660473101,0.021839112468183312,0.022100961387567095,0.022138435758419873,0.021964888909633218,0.02158828675661573,0.02114130620442476,0.02134597902650276,0.021209857669602417,0.02116366336111487,0.02136860666209821,0.02074834667188878,0.0208831085803207,0.020716930150063997,0.020444605257980874,0.020327417412719435,0.02042470161823336,0.02030407991068265,0.020296860184149234,0.020230603832346126,0.02021494245536429,0.020168849294162426,0.02015071690662893,0.020135058040590903],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(177, 6, 100)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8994280231445334,0.9426995008489105,0.9534532126263955,0.9610064520817551,0.9648058430351051,0.9711214637216173,0.9692877268522618,0.9731819083443918,0.9742828013259507,0.9780528311469705,0.9710586270450451,0.9821306311475335,0.9794517416353659,0.9771377810761104,0.982843756052484,0.9809810463518273,0.9845153941561128,0.9843562651799945,0.9838328268398422,0.9804828922448661,0.9861463316712047,0.9835684487546861,0.9805419516312774,0.9849049023537144,0.9857493459676905,0.9859242029635352,0.9847418815834904,0.9841898253346516,0.9875271710076063,0.9867070509071075,0.9855290210680223,0.9852139324865703,0.9877407887107083,0.987606582745616,0.9882527600913211,0.9860224461542253,0.9875638411042977,0.9902254675102186,0.9885516417514058,0.9883037303502213,0.9894647380390691,0.9903116157801786,0.9905111941410653,0.988793314562735,0.9885826257794527,0.9887643854121461,0.9903440302810056,0.9891240287883978,0.9893564896219224,0.9900567189738932,0.9889403477807456,0.9894976247981929,0.9910215429025208,0.9906544542946507,0.9905968075026955,0.9907951323949854,0.990746182853887,0.9918088722471188,0.9906440831249442,0.9913960242297088,0.9915037053001542,0.9905886733301335,0.99126964044962,0.9919561261569486,0.9915555589162344,0.9918726251525171,0.9913370087156054,0.9917779477195635,0.9915953600969238,0.9914939830719621,0.9917402093931127,0.991643826437914,0.9919363591240637,0.9916882294874466,0.9918728595405438],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(177, 6, 100)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09894571035485906,0.0749948572354628,0.07166348999923038,0.06978612565707505,0.055619906242360774,0.053228547754361456,0.04942888808516702,0.04687624823852503,0.049228120034502956,0.06502334743840588,0.050506232171943506,0.04597288015013708,0.04458345084665567,0.049123228128833046,0.04161509403709284,0.05093533390832111,0.048936042773354914,0.04018982767649123,0.04563456374857434,0.042488044910004866,0.038768247586652586,0.038822681318546076,0.04589223764401531,0.038509374163097534,0.03951177473381622,0.039937068995638814,0.04351969295248543,0.03942649478662465,0.035937865608439,0.036151211324733555,0.0738681020810432,0.03694604799612281,0.04083093923238135,0.03520653754011872,0.04993992341599104,0.035189566052041925,0.034112928841019824,0.03469003691654844,0.03558296169509593,0.03803451097861598,0.034559504080986236,0.03423107386761924,0.03457128722028634,0.04072202657864676,0.036563493060492155,0.03426095212704128,0.034851371755509855,0.03481643431887184,0.03226833547052649,0.03510989024467075,0.033852084060407586,0.03449854170487509,0.03275194056460128,0.03560841828584671,0.032879261564962646,0.031988156607368146,0.03245240247894808,0.03218603797026516,0.032346028642556104,0.03208317067922186,0.031915924984872134,0.03211210318088941,0.03182964683142315,0.031590974105592445,0.0323123875198905,0.032579544964934544,0.03200127296277747,0.03170680093908638,0.031727301088702635,0.03174594486599526,0.03182649442164349,0.03172387734721207,0.03167472061832336,0.03169660604338056,0.03169738376109871],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(177, 6, 100)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9299782343942704,0.9568364259977997,0.951606717776506,0.9558460944657632,0.9689244042104226,0.9689109227555652,0.9733956943551408,0.9741952619382276,0.9748705652184565,0.9583231328472388,0.9676372030037956,0.9729820399727079,0.9778164844709223,0.9704371174350894,0.9755443066834251,0.9708576880334396,0.9684806997105528,0.9792468882504174,0.9707475499748426,0.9744797850914585,0.9791740729190763,0.9768907822717334,0.969418247541221,0.9800969032427682,0.9784383744332479,0.9795265211357114,0.9759480835669668,0.9764416268981191,0.9803800984076051,0.9797109843052497,0.9482422096244513,0.9793408005668611,0.9777656453887307,0.9792137715708541,0.9735075857036465,0.9803700621247304,0.982422313332667,0.9816548348935413,0.9812900286930076,0.9786360152408582,0.9803066463953558,0.982235455408525,0.9805699057919859,0.9782455758468599,0.9809387851283418,0.9809085393784179,0.9789207103693164,0.9797097720687775,0.9824295669459715,0.9817957578985025,0.9812651866727534,0.9803788615736534,0.9827009571329016,0.9798470087060374,0.9819272618211758,0.9824365758151419,0.9819150219292079,0.982470491746862,0.9817657555559296,0.9819845227252226,0.9819486073553302,0.981698200477241,0.9824017826426547,0.9821985464453956,0.9821156652047631,0.9815530483151811,0.9819162278605791,0.9821912219151294,0.9819767444824972,0.9828878527093105,0.9819767444824972,0.9821953145948809,0.9819545298430664,0.9819545298430664,0.9819545298430664],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(180, 17, 96)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv5.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.16763978945806127,0.09067321377384976,0.07549039766209505,0.06123950955930581,0.05605981952200333,0.050833785964041645,0.047397858848771825,0.047363132607930035,0.04250545710120414,0.04204429396025981,0.03927294572074935,0.036714231190437034,0.03615358314409699,0.03397392678839439,0.03409772310256617,0.03329267027998025,0.03413091160075361,0.03417773704894766,0.0333113498213399,0.03219832742345634,0.029779652077683332,0.03072564039690866,0.030520069847077575,0.029415949294503613,0.027936143306471227,0.02813128323079794,0.029727945571502868,0.02743983468273388,0.028347194762761787,0.032797572520248945,0.026842543909503285,0.025313088877333537,0.026071554061371027,0.027132396398694653,0.0279116116220906,0.02666375793704394,0.024971290559156926,0.023766870251755397,0.026083483265335206,0.024408599155883542,0.02538338591689233,0.023650231106123562,0.023271039854585784,0.024206713006915983,0.023250314356483955,0.02284122991321218,0.02282938667117508,0.022846164793322313,0.022631695402096978,0.02221120628832337,0.02207117223171229,0.02253865318165885,0.022351071121595706,0.021794550856500693,0.021565123686283178,0.020952973048813973,0.021059099344999655,0.021186613773698796,0.021079142760796917,0.021371846306825832,0.02084460090995176,0.020482951202104454,0.020573963820592917,0.020405082292812113,0.020406437290134707,0.02032113777258074,0.02026364082323317,0.020221272217193012,0.020156170204623184,0.020141028293663422,0.020047332546198986,0.020019413426492837,0.020016174508260834,0.019978222282515903,0.01996090312500112],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(180, 17, 96)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8563188093582383,0.9314347366383607,0.9471775501579754,0.9598974274352091,0.9630528391108705,0.9665668212744373,0.9699851776943728,0.9694239402501835,0.9744141661231245,0.97447101409843,0.9773797258415751,0.9797624218373729,0.9798215478385109,0.9827071188626154,0.9821834997559538,0.9811568016856563,0.9817686640761267,0.9803049844436723,0.982053579914678,0.9850492746274492,0.9850227865649309,0.9830783780769661,0.9840251940032995,0.9849184895571341,0.985736947875253,0.9861087247376503,0.9845054289813249,0.9863824149877845,0.9854372339693689,0.9818707266226735,0.9867001032931775,0.9890326448652683,0.9872335367590832,0.9868373388247171,0.9851509134228749,0.9867779545507613,0.990122730010826,0.9906947898680196,0.9871374383834882,0.9901301219225954,0.9889054217404487,0.9903183052157081,0.9906384599698739,0.9897722190668012,0.9910159059434656,0.9911905571400818,0.9904157694882629,0.9912156297540122,0.9905948490537927,0.9924950960627817,0.9914352746320909,0.9913301821020916,0.9917232501788261,0.9915397277366326,0.9915267536163838,0.9929762662401538,0.9928894028735016,0.9925626198311995,0.9923623959530234,0.9921153374484619,0.992728373348067,0.993014367652512,0.9929289110554416,0.9925663798806603,0.9932908275025076,0.9930772371419867,0.9933768859010971,0.9930044293275204,0.9933489643044184,0.9929859686086535,0.9927435497866451,0.9929675295785976,0.993068046397369,0.9933000036790728,0.993178868748371],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(180, 17, 96)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10527603919665839,0.09347403487798685,0.06975246340650873,0.06370465905498393,0.05637492645544694,0.05593995993694489,0.050951643477600464,0.04740936370547285,0.04679781030799515,0.04849162245636544,0.04685122199433366,0.04719772558288066,0.0441033850958462,0.04412883151949886,0.04029569799957407,0.038561224317837414,0.03856935973224771,0.038808615129325805,0.039175918813526014,0.03674130744182367,0.05873565569878444,0.04513273031748447,0.03536589556850519,0.04026551757174259,0.04561048328774082,0.03787171612797734,0.036820608879282714,0.03882620093441501,0.041606916484656614,0.034705732135531006,0.03342947893587175,0.03384689382466254,0.036025846987655485,0.03883855578415992,0.038417953460179656,0.033100660790487665,0.032953247200070375,0.03436126878222649,0.03972260111437221,0.03471777803453383,0.03145403345067477,0.03268769493012903,0.031711863503628174,0.03182798527257959,0.031201451676305628,0.03071095074984626,0.032270930561515476,0.03242871705250642,0.03476845843787865,0.0317275115227986,0.03088616303510682,0.030193608594719076,0.030325902729612036,0.03072995836894537,0.029992228917956762,0.03058668877353373,0.0307781307916461,0.03153441342752414,0.0298190726780195,0.03020810696882071,0.030321850998909612,0.02981806215654124,0.029798186042976543,0.029854581604913336,0.029725743843005695,0.030120679789904466,0.029663933257978805,0.029655504846286117,0.02992086640696755,0.02983162076151658,0.02971495924536715,0.02964016157378446,0.029642270833151445,0.029653488849753776,0.029650176802963734],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(180, 17, 96)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0009144615755940233,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.91962293835562,0.9280627779080582,0.9597892750184884,0.9596507268219949,0.9696074859829342,0.9655252808015719,0.9665609567699375,0.9737506231854347,0.9725799457952875,0.9670862635000658,0.9742934054569394,0.9693745475196026,0.9748416721268954,0.9735326016569853,0.9788733346967841,0.9782027095094915,0.9801560380592468,0.9792321733239522,0.9782498731001248,0.9802098667078923,0.9599098123836821,0.9721324867794758,0.9820099232592638,0.9762064662751456,0.9736585126149047,0.9766606871907217,0.9825066132318133,0.9773491817920479,0.9723160975081157,0.9825161697423214,0.9827571306677817,0.9839151753155703,0.9804542570769658,0.9753385940252871,0.9735981362033698,0.9816942523637469,0.9841605123929705,0.982774829015224,0.9785092243170319,0.9823723835114365,0.9850893195285035,0.9843511472320398,0.9830760070869184,0.983769077057823,0.9850906102827857,0.9850757455663254,0.9835245599357224,0.9822620201617571,0.982156759324333,0.9825100653050327,0.9855256711476315,0.9864770342261149,0.9855566901342219,0.9857880151039197,0.9865105580142827,0.9857491943521653,0.9849109517433747,0.985607886311177,0.9862428724396316,0.986507356939933,0.9858072986024432,0.9867112868070214,0.9864628909118457,0.9867334766026743,0.9869672414952848,0.986280281230212,0.9864859743445595,0.9864859743445595,0.9862702391148596,0.9862629087198623,0.9864859743445595,0.9867230864529657,0.9864732868324235,0.9864859743445595,0.9864859743445595],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(208, 227, 200)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv2.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.11566625294645765,0.09445477041830858,0.09054782856063745,0.08436026222244297,0.08365206269361719,0.07975852452858617,0.07283006950888574,0.07210756177413367,0.07392550284237114,0.07153148786479528,0.06558114081399274,0.0656574735987637,0.06179684350119833,0.06431753676542575,0.05730467154756445,0.05828190834559116,0.05520853148037216,0.05684511283197081,0.055680529106694264,0.05383273650553901,0.05189216413619455,0.05343335491412011,0.052280061613482434,0.04737022845938648,0.05328158129258391,0.04593376367174746,0.049420519944233444,0.04987522641032559,0.04988536849225373,0.04613535337977939,0.04457573182326139,0.04441371892257791,0.04747757021516971,0.04607132576738846,0.04700047610688182,0.04294892337945592,0.0426726210448477,0.04350130244671343,0.04207482335988735,0.04244948171872863,0.03987142411738054,0.04112296822776363,0.03898099336905441,0.03892775318688817,0.0391449526379062,0.03808431735119323,0.03771163116914369,0.038839296569870646,0.037361198640668516,0.03796498129082436,0.03780609516838709,0.03662598165091371,0.03684169812232495,0.03498195499941314,0.035150091925727676,0.03565163340799314,0.03469422832643453,0.035243302682694155,0.03406137461344128,0.033708458933189835,0.033621312295943326,0.033388989339587956,0.03338706780135427,0.03289700056077381,0.03245769255546019,0.03228825143317586,0.03221506222928922,0.03221664100049015,0.03203433208628414,0.03183822595777858,0.03178384379661916,0.03173925218696173,0.03162053003181775,0.031582582785501516,0.031547866647322696],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(208, 227, 200)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9156536475655903,0.937157412876916,0.9373345906586538,0.9438713970996244,0.9435575661003739,0.9484131611597526,0.9525425098564803,0.9497123009669327,0.9466578897903712,0.95141367645567,0.9553162337963327,0.9559049705906671,0.9570656357030459,0.9553649333528513,0.9617714091469605,0.9606634459155692,0.9630935267468101,0.9605847588531636,0.9633168557245833,0.963195797751819,0.9669138669524113,0.965680090680877,0.9653410290422325,0.970263440456383,0.9647068509719993,0.9709982626113945,0.9681708218633959,0.968656137983703,0.9687118386337721,0.9712485648791302,0.9729109089502316,0.9722573754107653,0.9704531969337611,0.9718339624667451,0.9712129800351006,0.9729774341690137,0.9746608097518541,0.9740105732840016,0.9748035161890894,0.9739748363685395,0.976003522097631,0.9750896002973344,0.977822616559023,0.9767911889434213,0.9771519265809218,0.978965414367927,0.9789457612167427,0.976703691879415,0.9779734177175374,0.9795702352136473,0.9781470260965917,0.9793441575676695,0.9794762660351771,0.9811825229798511,0.9817376009664718,0.9806629183142699,0.9814946812418565,0.9800211707477366,0.9815609728092548,0.9824625840595671,0.9826886257371125,0.9824527524267178,0.9832154465210888,0.9820291859309469,0.9825901471994202,0.9826217208625107,0.9829994122810651,0.9842119873978254,0.9834023221664391,0.983354749281823,0.9836355746424731,0.9837151445969321,0.9835702559628575,0.9838208310972691,0.9839797088268433],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(208, 227, 200)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10243376555414134,0.09558378337687233,0.09563682777775112,0.10468304731284629,0.07899624549646148,0.08855713993515756,0.0827762695675863,0.07797783656730685,0.07416681903334418,0.08906779161228756,0.07987362759834303,0.06742126290331182,0.0777609592246026,0.06308690125180275,0.06770556170710992,0.059265808325862555,0.05605185955148382,0.05535179014058457,0.06600358068328543,0.06537547312753716,0.057055136156860496,0.0750990714096941,0.05855579908258726,0.058610978666244914,0.05782884034196945,0.05262156668090329,0.05486134299400336,0.05558379524454628,0.05332048408610304,0.05309955415754384,0.05071976338721223,0.05127864400904203,0.06640212080118173,0.0773518508754645,0.0514279536518854,0.048495597187493675,0.057236303957467226,0.05932535478441986,0.050265374147298,0.04722711966465839,0.04916804452327519,0.05096446422674402,0.04896367704335767,0.04631151721598357,0.04849567292482173,0.05003029528198783,0.049130757951859345,0.04905854556773536,0.04548213039220813,0.05134181277858433,0.05169706107773322,0.046333001233970175,0.04488750814725853,0.04395275169989907,0.04463027643225447,0.044179796158652945,0.04534164314059048,0.04446946608600338,0.04378864712424295,0.04324521058050218,0.04402532622064512,0.04295954623890087,0.04450756645796635,0.043789013301076755,0.04319828523137315,0.043591210561007565,0.04278700173003567,0.04332438200404964,0.04268282784014633,0.04319930257614945,0.04311791034447369,0.042663133769100886,0.042616824134928254,0.04263957869709562,0.042643429323570015],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(208, 227, 200)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9356567985847224,0.9354204496896639,0.9426249598370438,0.9293629244927327,0.9524756006397174,0.9374429342663413,0.9445538759390035,0.9423284555077615,0.9535439761918955,0.9407931859217263,0.9388956142593,0.9519252086836335,0.9452177742784277,0.9651504444271503,0.9471285557116284,0.9630779513270837,0.9676801286971357,0.9685347068675348,0.952155201745086,0.9607311443332428,0.9631289364391874,0.9417408599066361,0.9598661005782435,0.9618005122504629,0.9651149370524578,0.9675901687764294,0.9718926596369484,0.9699086825885763,0.9662703564590451,0.9718204550719683,0.9739673682903597,0.9648697359854771,0.9556137406844409,0.947843874131868,0.9732426006105328,0.9765658240049011,0.9618653888146723,0.9607181919068009,0.9676679141900459,0.9742324460515921,0.9695366427475239,0.9673293983774218,0.9717731722829768,0.9747109020238466,0.9736323936775065,0.969856506868171,0.9708977229330934,0.9745951262343686,0.9758314081443383,0.9696204486025911,0.9745759804559312,0.9725091239309466,0.9761782957968552,0.9772370892853166,0.9780390702517701,0.9745938771859558,0.9753755150876406,0.975196192814899,0.977220275195533,0.9771631239978462,0.9773508366766307,0.9771893807324662,0.9737181508644449,0.9764346412192649,0.9787087725461148,0.9784917831483823,0.9789031576324083,0.9778078078469076,0.9789038449789327,0.9785037430573348,0.9780344067142596,0.9796422776519915,0.9781938043889374,0.9794041286810301,0.9794041286810301],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(190, 46, 70)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv3.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10977186155073422,0.07832908722201846,0.06627799235070966,0.07012739148404863,0.0616671689221676,0.05697184034195416,0.054412197645109274,0.05286259101565078,0.050931238764544945,0.0482419245875305,0.04818328750647289,0.04584155947243088,0.049480915329027556,0.049328273551843695,0.045326285872006745,0.04271166676149745,0.04013086614995887,0.04025977567555159,0.04128961968283678,0.047935269918945644,0.038706347122703086,0.03692390352897218,0.03873583609312944,0.03995614791414855,0.03630515582142622,0.03923114660662201,0.0376103115288321,0.036746746916131875,0.03420229881344724,0.036467287507663475,0.03296984228618093,0.03582723841267421,0.03227735285616276,0.03299158890171559,0.033024337666721554,0.0321304864899809,0.03315108372805318,0.032468598849328935,0.02998178250498788,0.033679313711258756,0.03113060957343581,0.030372948120993573,0.031222395466390957,0.028964902897966004,0.02775040118947608,0.028110532114391063,0.030075613873555215,0.028926131332384898,0.02696774200390158,0.027071917597268454,0.026883062125873866,0.02694840032312878,0.026408937983086838,0.02672057431383231,0.025871481289415015,0.02544374499312108,0.025206029187260625,0.025009768590638318,0.025258308707097415,0.024381656758700587,0.02416179368586065,0.02376770594147677,0.02438762842465239,0.024058280228066418,0.023768065742431434,0.023635515268520802,0.023386250665029435,0.023355476731884657,0.02314857793811875,0.0230444078144851,0.02297634545282673,0.022873348144351376,0.022835410565120794,0.022814424598817835,0.022779887589494795],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(190, 46, 70)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9193339955025464,0.9476337977816701,0.9558021435950879,0.9522572296821175,0.9579200197967102,0.9623331524778539,0.9605661983138025,0.9644717246632168,0.9656736007580092,0.9687442369548718,0.9684851033928055,0.9705929966500125,0.9653819328527238,0.9675696661520012,0.9708288109537228,0.9725928056607284,0.9753472732615316,0.9745812240551006,0.9736630332735638,0.9666893784098456,0.9763524231625395,0.9784676475622394,0.9751373277664964,0.9759113070960432,0.9779030875447607,0.9756348654633907,0.9764311128214366,0.9781149200574895,0.9812872006994053,0.97690554055906,0.9809886387005732,0.978793172619651,0.9824488191062325,0.9810363819393042,0.9810613853538795,0.9824595423148912,0.981737284705908,0.981637511854945,0.985014915446113,0.979189603091386,0.9833413996980632,0.9834577850728956,0.9826822355788395,0.98448191415652,0.9859402095249554,0.9859048475532127,0.9828459925430588,0.9837719354525487,0.9878043246527268,0.9863035838376217,0.9865272897049211,0.9863405551538306,0.9865728326812381,0.9847566224187441,0.9869086182673225,0.9875175319439147,0.9882981137186989,0.9879145593936918,0.9872608359489147,0.9893215531750761,0.9884662225827985,0.9886998054868829,0.989186335135652,0.9889659021225572,0.989562670215791,0.9896565532776433,0.9896610986780255,0.9894177780144598,0.989768240158312,0.9898930581074304,0.9894850464873938,0.9898635813119359,0.9901612568792602,0.9899829202270688,0.9901862138847339],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(190, 46, 70)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.083620548232929,0.07605899833014741,0.06821206362591577,0.0731792271034824,0.0692570780509526,0.06925985580662272,0.06059486016477506,0.05937147510215589,0.07840289855535908,0.05610781646112806,0.05436891342202822,0.05230193304288428,0.05684083739097176,0.04979088639782876,0.049177257499027086,0.04589382155807977,0.052214276388645994,0.04458983293718489,0.045496115252324394,0.05853496278888991,0.042624890607144826,0.04752903309525903,0.04220002270799732,0.04216320453486901,0.03921655294360574,0.05171925855410058,0.03948719047803649,0.0400133845655574,0.04560759417277431,0.042155860093041383,0.03653917133398482,0.039463539662230054,0.03711255099304353,0.044607782223073066,0.04185753527683081,0.03930671091649131,0.04074589831824974,0.03837007837197215,0.035945380184658614,0.04266427022587393,0.036473867299732884,0.036477617947608745,0.03555930192579109,0.03468797426299541,0.04015676971921806,0.0345635509393674,0.04502948250147895,0.034759046257668756,0.034032050873508156,0.0386770641450415,0.03607199971925762,0.03535767752690004,0.03386683694224587,0.036681809842176866,0.034820189316760224,0.03393283570857392,0.034106578790547515,0.03272378478212045,0.03279233586900832,0.033249786772678806,0.032440257333603104,0.033593218353708175,0.03240532332781664,0.03218648030944297,0.031962489815512064,0.03187868621494762,0.03273146660979261,0.03176665709344382,0.032061759810062616,0.03173328178240262,0.031848026535560174,0.03175301866740295,0.03180353492498398,0.0317502981890313,0.03175695154470267],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(190, 46, 70)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9458050490111042,0.9574659328644468,0.959340000261955,0.9427388798468485,0.9515719366112285,0.9546501397068983,0.959826677184448,0.9579230450936048,0.9343653090071525,0.9676748131540205,0.9655709428729332,0.9696883560773636,0.9684780602535532,0.9667187853231236,0.9696654713814808,0.9749683675442111,0.9626149329043632,0.9781256649570286,0.9777466267801351,0.9548189885853654,0.9735720447644598,0.9754513830937598,0.9725134892519628,0.9757125195800623,0.9779870422287826,0.9622140383203944,0.9767417723212521,0.9759101410651669,0.9666230614495468,0.9742776008479064,0.9788258963332144,0.9771217911429041,0.9801339618567064,0.9701002663296647,0.976783727629726,0.973842521707912,0.9797616834873545,0.9787313275627748,0.9792124470350676,0.9774582773028926,0.9777946027615168,0.981989621156294,0.980137367503659,0.9796310847101262,0.9708136024904976,0.980908759952133,0.9753170338854777,0.9802060957325278,0.979901891466971,0.9807328121173476,0.9763124593414362,0.9819687611150222,0.9787805539790475,0.9795579646272086,0.9822166142507724,0.9819842217605562,0.9813570862562043,0.9814929254683574,0.9834265901894969,0.9829340350756006,0.9813350106006336,0.9804171855451377,0.9838764502083478,0.9816791271079387,0.9836141667192166,0.9833978556894575,0.9818073462172299,0.9829551779613525,0.9836630384788295,0.9838603836335363,0.9831907487364306,0.9834202897106064,0.9834214194817015,0.9834202897106064,0.9834202897106064],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(3, 157, 205)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv4.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.1044839691639561,0.0758243547733297,0.06107478924223648,0.05629677797783964,0.04852453396599078,0.04403624736958899,0.04452708085309735,0.047500120214307155,0.043993747352119986,0.03614762648428019,0.03970486067815813,0.03717467843854687,0.038769065851573956,0.03424425137603713,0.03362182992332341,0.032785889802912395,0.0348865760718595,0.032543156609229344,0.031731754655758816,0.03409432309497263,0.03530148452687646,0.03130236860420149,0.02898622615068707,0.029789534584967952,0.031205856738631257,0.030726713361591954,0.02843534114246789,0.028921023845877435,0.031793281367690836,0.027309956596172143,0.028706165346586897,0.026989415760780393,0.02911193194667908,0.028325702277314076,0.02743438498756047,0.02896672273086723,0.026428010302328026,0.02650222905398371,0.024816874108295396,0.02592638894096273,0.0239015126731012,0.025180972832652984,0.025163516823846724,0.02351672836874888,0.02439678039184141,0.023461298167893295,0.023388390506554783,0.023377086710769424,0.02273937501023947,0.021656060415103536,0.022051411694745702,0.02163816971399257,0.02151300727954094,0.021247145572820206,0.020884233647007987,0.021257882232416127,0.020963381666941876,0.02093853968678881,0.02059771298961063,0.020167201394289902,0.0200376975543174,0.02005397433250767,0.020268763891524738,0.0196828010463223,0.019360781910552943,0.01953150437630602,0.01919650969107263,0.019291479762041895,0.019241268048436302,0.01907847253232341,0.018959658360802156,0.01888879258014936,0.01884919736120436,0.018807821835626987,0.018789978594629077],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(3, 157, 205)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9226250925843401,0.9462895778557,0.9591769301296498,0.9619762530717642,0.9675440987079301,0.9723144142472998,0.9721104382470626,0.9662116362284345,0.9736592590876354,0.9790187846370879,0.9740276277551614,0.9769005323831191,0.9766338595231426,0.9818801097546507,0.9812851488425419,0.9821124829386658,0.9783724145516179,0.9810341299288413,0.9824922348858379,0.9798720105891745,0.978186110676892,0.98250848208981,0.9848425540837428,0.9831256955129614,0.9830952856964117,0.9824623714447035,0.9848246345165139,0.9836549492459621,0.9809038979118014,0.9858939083324324,0.9849177522114139,0.9868991446458544,0.9831846465933897,0.9850082186372039,0.9850607561053647,0.9847332158565434,0.9860640846756682,0.9859782455987338,0.9886298491822715,0.985910777420866,0.9888230336714834,0.9869531717301396,0.9877541631313724,0.988981586578186,0.9881277610797827,0.9889915261463371,0.9884079581587313,0.9891865159207021,0.988801750959333,0.990567485558742,0.9898440360786427,0.9908326322529752,0.9912106784346614,0.9911965266457411,0.9906873191768476,0.9915243980610038,0.9911652925159042,0.9907218178831387,0.9910264874432968,0.9919431843888102,0.991543580817954,0.9918018784780084,0.991388290789555,0.9924790232026273,0.9922065394852991,0.9927442509936443,0.9923455749117772,0.9923502231733895,0.9927125577133104,0.9929244393423434,0.9927674619283808,0.9924933786870498,0.9929118396228036,0.992878025603884,0.9927506183976983],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(3, 157, 205)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.08267755101329273,0.06847837495639972,0.06140549525977,0.07034794073743919,0.05642891896190922,0.05026819727469965,0.04661845166197757,0.10432455505394854,0.04471705288872686,0.04372660975224783,0.041985065315904485,0.04426678177417349,0.05286230278230205,0.06512404996197659,0.053351759301223295,0.03936669625009048,0.03780546241353468,0.03894076572916762,0.058103456063983365,0.04215082453749434,0.03750515780907726,0.04263582635683702,0.0371993713772174,0.038554803619986956,0.03709915778839711,0.03913953160655867,0.035990711045531476,0.035538633346967274,0.04591025828393464,0.04370678038531562,0.03933446390419891,0.037306773109534355,0.04214000962290567,0.052976702985792226,0.036006258571475644,0.035646235479419586,0.036206346778423104,0.03778496179537675,0.038509377563225033,0.033917389848490354,0.039010834458357686,0.03390091820988049,0.035138031791678,0.03735787759941468,0.03302531498404303,0.0333467501263643,0.03271312197715146,0.032298768088989654,0.03142692716312163,0.03301675690152391,0.03382028644181199,0.032511965405592806,0.03312293683847611,0.03095496575703326,0.032045083654295536,0.03293634409859418,0.031213243761423117,0.03185000458789855,0.03233504834556088,0.0323398387155582,0.031390378357925776,0.031180356737367066,0.031053717442087292,0.030836281728293887,0.030793195978267907,0.030992760286503232,0.030718707999925023,0.031226594776836866,0.03116102963173922,0.03080224533568543,0.030741474393409553,0.03072551382878392,0.030697746528792626,0.030733824377924306,0.03070732791911286],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(3, 157, 205)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9496981931099412,0.956486976485156,0.9628719751971143,0.953138908580563,0.9607401434341238,0.9720810059471702,0.9752795004736078,0.9281544220202508,0.9737810429222612,0.9778353788882059,0.9784224823171007,0.976372509978054,0.9695478808692173,0.9467830396702928,0.9673275232375205,0.9797940257956306,0.9788364840743231,0.9778539109891516,0.9590685622139216,0.9794204036458145,0.9795800597837002,0.9755730605288022,0.9762703846146468,0.9768346178914873,0.9821524565727221,0.9799574120098499,0.9795928088939649,0.9808065213625561,0.9768598178574315,0.9743852781430415,0.9782609632263213,0.9806261404258115,0.9786003302019689,0.966880262824778,0.9813392433024759,0.9796911376125086,0.9799334958036238,0.9789870403248206,0.9742274228962011,0.9809331483416269,0.9754535841465818,0.9816567687125938,0.9786937254806536,0.9762004230088321,0.9809517051658001,0.9827764192688304,0.9831247945690744,0.9823934163522907,0.9829217096280503,0.9816379760228945,0.9799161788167555,0.982199590180011,0.9821268024157259,0.982450051699283,0.9840812187213824,0.9821022185835407,0.9829263325385056,0.9812075151167807,0.982970821793248,0.9816058498236848,0.982656258635815,0.9841052827247668,0.9831777371205775,0.983169483944481,0.983169483944481,0.9843379482829835,0.983168263696295,0.9830804966940866,0.9831836728556986,0.983876129464424,0.983876129464424,0.9843457438411622,0.9843457438411622,0.983876129464424,0.9841086441646583],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(135, 129, 184)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv5.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.11882113451724882,0.07304152457847901,0.058972938920350965,0.05296855551390167,0.045221862598468485,0.04603123346435382,0.04307145439643084,0.04330847826951694,0.0375511449743553,0.03556208477439818,0.04176391946730062,0.034169365392244695,0.03469638109232757,0.03152805647096956,0.034926798275724175,0.03261293843473902,0.03437517960209139,0.03275698531431841,0.03237772843622328,0.02983491112993345,0.030273130072658413,0.028831751415034478,0.03087994538990083,0.02887907867391358,0.028451786258120306,0.028607201447362614,0.028388847362459182,0.030642239638056953,0.029304268843956285,0.027994919341710424,0.026888804584058835,0.026620139201883258,0.025010205825144276,0.02970551145983111,0.026248090619910486,0.028188555587386074,0.025361810007558244,0.024302500610550243,0.025049396875695264,0.025176127874137748,0.02452094160331859,0.02326132655911839,0.023180200194967707,0.02352772775913705,0.022769828511557764,0.023064784249898905,0.022150784174037964,0.023324120089087425,0.022505046578949875,0.02257675945263821,0.021576888832815187,0.021874778768963012,0.021688352611858957,0.02108979565043425,0.020556420512403546,0.020391036983139873,0.020305531195451303,0.02049044120506323,0.020146641289689425,0.019993155649335115,0.020068407115469956,0.019902396621555945,0.01961218236101725,0.01964749131044469,0.019451921647851957,0.01930667262221393,0.019077438653744373,0.019074923021319874,0.019060400203467782,0.01899293471545937,0.01889831388085432,0.01880517828087285,0.018790853687296414,0.018741993002491103,0.018723826722070184],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(135, 129, 184)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9051026881483292,0.9469763273326844,0.9588021783828475,0.9653687223500892,0.9708129998160704,0.9699884933269648,0.9704190020926324,0.9727822102931936,0.9768426603483978,0.9787766151431041,0.9731354237935174,0.9811894954307001,0.9791971936427387,0.9818930517772372,0.9795803586978404,0.982662732365329,0.9798150338644211,0.9818048834449118,0.9829929672203597,0.9843315265552919,0.9833868517427878,0.9850483023592208,0.9822894895892648,0.9856474379365644,0.9854361398459827,0.9851909319465227,0.9840757452414541,0.9822791368014526,0.984354947485625,0.9863450159181012,0.9860325190841334,0.9873219229674136,0.9872918833442819,0.9817808725050021,0.9858219835066,0.9848391390350826,0.9879441862572744,0.9878975171419992,0.9878256584617863,0.9874960762096264,0.9884563836967568,0.9889146625031857,0.9896807176821271,0.9890577840810544,0.989158819902352,0.9897862147377947,0.9904752522810186,0.9885925848720759,0.9898935983008581,0.9901271736463675,0.9919032207083204,0.9907861543234463,0.9910291711199413,0.9922133275312568,0.9917254732961517,0.9926591684724202,0.9929253964597209,0.9917040683341583,0.9924702459817254,0.9918200382637049,0.9925929166415113,0.9920721108058784,0.9929344448458376,0.9927557841278678,0.9931169326870691,0.9929969851281971,0.99355625634925,0.9933238065390286,0.9936300572471062,0.9935121772099069,0.9933855028430729,0.9939015150965643,0.9942819407149952,0.9940388846202334,0.9942089712760613],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(135, 129, 184)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.0832569173060332,0.07073229002891128,0.08374325945410122,0.05088889689226331,0.04858287373154434,0.047708745213718345,0.04177418716789521,0.04089240742866526,0.043978782679207136,0.05270007672844474,0.05315327558576856,0.04295905534549267,0.04292036979059174,0.039128452221962186,0.03737112054556506,0.0369208461845044,0.04002093583140586,0.04388311298698494,0.03700556532060567,0.03626290231021409,0.056982362539190604,0.04080054896189175,0.034210810533811135,0.03420034917871567,0.04397295307611272,0.03643531484650992,0.050548571076794585,0.04008316820634599,0.03316344554532844,0.03650274859466094,0.03309685343114781,0.03211872101956626,0.04460208334054324,0.03527976567904974,0.031349336798862905,0.04914270071024747,0.045769612767647225,0.031171033682487265,0.03425930911271843,0.03660834604471,0.03130212780536245,0.03406560121942632,0.04178707259883176,0.02969302786994226,0.029653794233946457,0.03069152630788764,0.03050864358333378,0.038937807065207525,0.03050348476036308,0.03038636486196436,0.03106492527622947,0.02925433009043592,0.029797078292701663,0.03045323346181424,0.03066764041730219,0.029319543094467052,0.029230990204512048,0.029952651792273077,0.02917065589442286,0.03163107753414469,0.028812540801325205,0.028763522660916613,0.028713678658213403,0.02887491679324727,0.02875984899981325,0.028677735242134926,0.028580704316035987,0.02959692209600583,0.0291403784963888,0.028643802673751138,0.028645489188199192,0.02860472501348384,0.028575505323938488,0.028583648555057566,0.02857957939102068],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(135, 129, 184)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.936935894098009,0.9511947548579869,0.9390843001650278,0.9662128400750796,0.9731385880449355,0.973995718114799,0.9762117460979627,0.9798891723323887,0.975695927278337,0.9670016090617108,0.9656575235029403,0.9753828196231179,0.9771154880909932,0.9798778544298672,0.9814578516129612,0.980920261164105,0.9744271515188135,0.969409794873389,0.9795419301585403,0.9762211095871463,0.9561952150641183,0.979562567539192,0.982483171587744,0.9820551943882,0.9696401061590358,0.982843939686299,0.9613113183054615,0.9793965844424175,0.9828905636595032,0.9802474127181867,0.9812141542702051,0.9826402653116787,0.966877259053747,0.9800874531949687,0.9836161276505154,0.9705945035671308,0.9723525388387896,0.9843637317985346,0.9827890696908,0.9812154731416723,0.983965842069743,0.9793586226214385,0.9719040375216216,0.9845009524952195,0.983839018904892,0.9831068654792314,0.9836982611430923,0.9718833523590285,0.9849877418787426,0.983549434683099,0.9823267240812075,0.9853081917640943,0.9855537520040729,0.9830725574168495,0.9865511771519465,0.9840272354873814,0.9862507109251386,0.9848799496210553,0.9850043904814336,0.9849111159782798,0.9857469586269803,0.9840792326623745,0.9862364846862787,0.9855557256370406,0.9855305003084301,0.9850197314363399,0.9855344009021799,0.9828280822991411,0.9842710181880189,0.9855344009021799,0.9855344009021799,0.9857718988724387,0.9852823840246396,0.9852823840246396,0.9852823840246396],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(128, 83, 9)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv2.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.13820897953889896,0.10112504553890447,0.08729179563493664,0.07977411032815433,0.08082303208942265,0.07800029592347446,0.07219022336114041,0.0700132192529627,0.06703559880028612,0.07012742082295002,0.06330522161749494,0.06417538653824474,0.061893677085484426,0.06418539639327671,0.059868017788058975,0.05878257858145551,0.05824794903192733,0.05690009057487682,0.058832389233209284,0.055972709074906604,0.05155386620938334,0.05241276475708646,0.05472055568284049,0.05349792615261957,0.0505216205413672,0.04784906698222286,0.05116589916496615,0.04706355876631753,0.04695714263486562,0.05075418214481175,0.04597739085324646,0.05101160165370535,0.044362572740597,0.0433205014284602,0.043795423064581444,0.04672036863274591,0.04819605893456513,0.043447685252794015,0.04403596231532534,0.04150465656357805,0.04116631347391614,0.04410728363219952,0.04207219766263017,0.040561998307380616,0.0404972819075893,0.04296312729944342,0.03902518260340715,0.039510744903983396,0.04039327426422025,0.03838517107794921,0.039009199973370746,0.03691097374787855,0.0368679878044818,0.03722501148012086,0.036061427001025256,0.036528610284274864,0.03560199913273289,0.03601659191996371,0.03513918973863944,0.03476314994400924,0.03516880835663583,0.03433385791186615,0.03421614647022569,0.03411258166700704,0.03473759739652259,0.033596690511170944,0.03346573862866984,0.03335705410906744,0.03301840920901654,0.03293515366180997,0.03294725703461712,0.032954031946766965,0.03279060645099358,0.03273050480555286,0.03270449507448135],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(128, 83, 9)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8985413446682593,0.9316427131050488,0.9439790363373488,0.9492982753526322,0.9485950766923994,0.9505104635336928,0.9552211708598788,0.9560641490294483,0.9578145397453307,0.9556604963905079,0.9601963422704045,0.9596320356766311,0.961390138574855,0.958480713279334,0.9631238196266853,0.9633258852746656,0.9633274754924549,0.9644741112716501,0.9609190350625789,0.9648716367009013,0.965674179695418,0.9642710435715325,0.9644062457284094,0.9675112121356076,0.9684633628991203,0.9717943583129033,0.966416616307761,0.9703908919592148,0.9702895737511626,0.9664755697645556,0.9701790248266917,0.9678541826217678,0.972555926770756,0.974250486184079,0.9736663632476609,0.9701572850293948,0.9696135417370141,0.9716903153386632,0.9722416600175542,0.9745488602902364,0.9754493887454031,0.9722121151143913,0.9734496237653261,0.9751665490486688,0.9752223596137088,0.9739978513801277,0.9778960552484186,0.9752901032982516,0.9761169322971507,0.978607466236816,0.9770776636811436,0.978590622696034,0.9793868256985154,0.9785249531112024,0.9793135457011268,0.9790745740601747,0.979499481316065,0.9791265452613288,0.9805031524825384,0.980541138534421,0.9802602749032403,0.9815647006731251,0.982094461233988,0.9814389820034254,0.9806424882015856,0.9824387580250991,0.9825681443459738,0.9823751364535345,0.9830693540482943,0.9835058658584196,0.9827925940185185,0.9825386623161069,0.9834905701208561,0.9830702188363656,0.9833136399342338],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(128, 83, 9)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12354758120689195,0.09352352441995825,0.08838803465014061,0.08034072911104385,0.0817962050489134,0.0771597793114554,0.07350598967976586,0.07218037261483595,0.08055750900220215,0.07354021208085555,0.08091008154694566,0.09137333988426477,0.06599170805252705,0.07585407684451526,0.0666319230987444,0.06958687883062462,0.060477432547156346,0.059864798797569736,0.05961017495987751,0.06805305486589773,0.05689817060207583,0.06609163888541283,0.06639821263113382,0.05881670523447679,0.05599783845885922,0.05455131718946486,0.0525054271662563,0.06422169611523651,0.0647120039948483,0.06084155178664066,0.05487963739283306,0.0548732173734719,0.04972483691890625,0.057683879551813776,0.05223825875784933,0.049759525662332875,0.05069264844059944,0.04846732464596578,0.049434169404899954,0.04817596159337722,0.048545286087543285,0.04957726169697608,0.05049547562023619,0.050637694100026824,0.048435367738565625,0.04742597720211314,0.047350265705298725,0.05448020324879086,0.0570443496722536,0.0488902650133441,0.04590936272414689,0.045787122571693664,0.046376109957899835,0.04774917991147008,0.04470523887944385,0.043823618743939904,0.04451549603664588,0.04418125274971998,0.04428058703433197,0.04329842749688634,0.044339457704746434,0.0431954734462643,0.0448092284455537,0.04364175307740461,0.04251978426045159,0.043184624724986215,0.04320987996059595,0.04264182039501331,0.04246130404910681,0.042835109529831154,0.042919973778151155,0.04241429925016112,0.04229600616695545,0.04232504772617645,0.04231651988635768],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(128, 83, 9)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9002070099842586,0.9422286506484517,0.9394200482118353,0.9545199471504119,0.9494782005909717,0.9605013656529214,0.9573938211868929,0.9572018107768689,0.9484545316066472,0.9576598739650171,0.9400018997725613,0.9331824082585828,0.956569070021644,0.9518791363799263,0.955953785862331,0.9524854892012464,0.9634539032304417,0.9631029345623839,0.9633233287413832,0.9528621818905774,0.9629748309274326,0.9548812824560834,0.959302318552199,0.9639470134434117,0.9662351858833507,0.9684655396702585,0.9651263804107048,0.9547523657881447,0.9565597529265593,0.9622501779281747,0.9620763633694893,0.9659628524296203,0.9692030861729188,0.9636577301509616,0.970471558732933,0.9755266268136779,0.9717063489541583,0.9717417751760483,0.9679133483918618,0.9711120721193106,0.9693495511119692,0.9705400313817683,0.9712642088219324,0.9721226061862781,0.9745835659134429,0.9762855346985462,0.9751650538974295,0.9659283148089701,0.9655543883944302,0.9724104608542312,0.974765569633804,0.9762064957254766,0.9758241534286103,0.9722212893072302,0.9785347412992031,0.9746381198506843,0.9769315704403001,0.9757480010768063,0.975355770454565,0.9762352670854154,0.976014316189433,0.9763761720020161,0.9754001420160953,0.9760348789853981,0.9779790224929821,0.9774341384075521,0.9769291333655469,0.9787546813620862,0.9772607974272594,0.9773839174748474,0.978570091763977,0.9787374391354446,0.9780011880179369,0.9787374391354446,0.9782527481948583],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(144, 10, 113)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv3.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12340498409422812,0.08490360143155987,0.06886210795257236,0.06072609799595087,0.05605486189265909,0.050734459317768286,0.0521684339865274,0.04870357767839885,0.04534556381276383,0.046790856023970887,0.0527330804283369,0.04480540866175195,0.04413776213531505,0.040769178504796375,0.04090608616294457,0.04219434178921843,0.03863312030661284,0.03827710524628116,0.044233983119294676,0.036154522042708706,0.038335281358722426,0.03570237351327554,0.03571940061059126,0.038436988237413136,0.03519823122108174,0.03738073626701706,0.03603756486823332,0.03451200269102914,0.03406087523336946,0.03398630705417773,0.03165003110282336,0.03596575027129017,0.0321318528522126,0.03371632762022854,0.02967782897228228,0.03045658827988689,0.03249077959432088,0.030541689385833883,0.030802591266227888,0.028155153109996077,0.02989032342304547,0.03032508335222425,0.028291525355490076,0.027281699079981785,0.028263537427847873,0.02852058814367,0.027346649825069434,0.02723100293355163,0.02725080730647429,0.03124567865941328,0.027263110573536888,0.0266937304215606,0.025555942401093646,0.025309016427530865,0.02510316397617107,0.024670020224183013,0.025261819763333117,0.024253701465577742,0.024553658474633578,0.024473178899182352,0.023902675042618046,0.024014509327598726,0.024004061308018392,0.023660130753157337,0.023444911443285924,0.023427021351748522,0.023418960682749065,0.023392363207975886,0.023209499276508786,0.02310053130989427,0.023063278458969837,0.022999683416364256,0.022949899451536172,0.022919054010206618,0.022897833210223047],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(144, 10, 113)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9035011845250208,0.9402621042669006,0.9565446451395061,0.9595526471840642,0.9620193411942634,0.9682625378537828,0.9667191908581372,0.9679128082882595,0.9729966415299772,0.9695970260375366,0.9649164087955969,0.9722686025653104,0.971613277530774,0.9756481971997331,0.9746472855909546,0.9740996947447303,0.9775382308379945,0.9776454295570812,0.9705568305148659,0.9797898339553777,0.9774046882213616,0.9799454932430489,0.9796735279844847,0.9765385815062485,0.9800122314786388,0.9770905402694159,0.9807706328593162,0.9801430024597161,0.9801389728077418,0.9812502033023177,0.9834109851763568,0.9798756271285982,0.981630180763693,0.9818298390825735,0.9847925589657301,0.9842850169905157,0.9818145397021597,0.9842046290931339,0.9837602930598413,0.9864828907265228,0.9841215515625801,0.9842005773153125,0.9855887801146967,0.9875728291443121,0.9859480923400478,0.9845409107985663,0.9864781634554045,0.9866637972160501,0.9876257336106995,0.9819483901339254,0.9865057939994587,0.9883361253255011,0.9885856232735715,0.9882422914456374,0.9885131825272128,0.9888259522631572,0.9882213005148381,0.9894991216511078,0.9895586680035284,0.9890952217221872,0.9893947838590429,0.9904226036179128,0.9896455041878447,0.9898168815873553,0.9906840749630417,0.9905199831610499,0.9905516075825279,0.9901900275707083,0.9905525273281524,0.9906676788848928,0.9907296341154667,0.9911740903416197,0.9907680205984728,0.9909717945001603,0.9907943393328689],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(144, 10, 113)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10418343737567823,0.11200771713789386,0.07073725387710067,0.0711366109473189,0.05721912689737438,0.06135684917984959,0.05949513838463223,0.057989175735470355,0.051992375091281545,0.06480721835930323,0.05006611525910007,0.05097188206831205,0.051370924163315304,0.04596298781457226,0.04654379221530714,0.04776271368629744,0.041842854478719715,0.04576237728095956,0.04408081648839298,0.042396161700134834,0.04581073906618295,0.044076195799608,0.04159082396282363,0.055137234598500624,0.04004918822820244,0.06113468405000123,0.040527757454369075,0.055507752545101124,0.04305358879857047,0.03904001780135935,0.04793169572949409,0.03952680507169147,0.03774046321044263,0.040677668924901086,0.036758331701005856,0.036560674339430436,0.03611473827222778,0.04517246837570905,0.038638531037724716,0.03494343933524545,0.04450083236462882,0.035982542825523524,0.035895399073350064,0.04050595182733437,0.03475167804567265,0.036787624767248575,0.03775556576518259,0.03483471436445246,0.03542528980734832,0.03524829895584444,0.03351231538450595,0.03405992033094475,0.036459700619846686,0.03359931161262326,0.033344375764586265,0.03274420203826681,0.03323419678610625,0.03271052372670665,0.03233007579478611,0.03278156275192077,0.03240435614004168,0.0328284466235908,0.03225344778694648,0.03232176991314003,0.03273097172481907,0.03208469806104591,0.032707247132902705,0.032469420263038055,0.032014197047121336,0.03186364836760403,0.03191841391916947,0.0319214413460997,0.031877572168804116,0.03188927869514092,0.03187299189749862],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(144, 10, 113)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9231922827786094,0.9148690379119466,0.9571244170374409,0.9548535812412918,0.9652388156659847,0.9586555440230149,0.9590466128791114,0.9647645592201414,0.9689010202431976,0.96019383275287,0.9698250094708163,0.969700199399731,0.9702030617522899,0.9746599133554054,0.9734142243211598,0.9740614525582556,0.9769413261804238,0.9724948759266742,0.9757534818648073,0.9752495620897432,0.9714104251030595,0.9738264862564375,0.9766210600521498,0.9690253740111344,0.9781672487065366,0.9540422626385098,0.9744243007264712,0.9587648811030374,0.97269467565843,0.9764926744700546,0.9726306494004232,0.9763582662872393,0.9776466579211583,0.9792648216755632,0.9803518258429889,0.979859079717139,0.9786511135048229,0.968940124638139,0.9792889723514014,0.9794737011007892,0.9782785392213614,0.9816994930823638,0.9813508874766824,0.9760772280935405,0.9812424654160685,0.9789789523379776,0.9811636275783512,0.9782627409588972,0.9811670834297785,0.9801579780727254,0.980304101291366,0.9819638948781064,0.9818729186154954,0.9809943525980781,0.9816686234053158,0.9822200014900788,0.9832085063804747,0.9825007764357084,0.9819306728591093,0.9816829956898042,0.9823933701678392,0.9810477478743185,0.9819195700824093,0.9817338276740254,0.9820077585895179,0.9819459124554019,0.9827384694950003,0.9822489259292385,0.981507702726088,0.982416874702602,0.9819669415427985,0.9819669415427985,0.9819669415427985,0.9819669415427985,0.9819669415427985],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(223, 44, 194)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv4.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12153485903488674,0.07643301734809613,0.06283395658977936,0.055274978918871516,0.04768688127479568,0.04744946518816899,0.04196518080784829,0.04021857956196059,0.03831534373647569,0.035314397906451425,0.03678404287544996,0.04274113564778712,0.03301476977961853,0.033796131685546855,0.033291570764175805,0.03197552103032907,0.03013508406820849,0.03419172569622903,0.03489631639211038,0.0447917288013511,0.029028903285013716,0.02933883327662194,0.03178857407422751,0.030316818418474133,0.033437067883052916,0.026577582078445,0.029951043901079572,0.027733173002867626,0.027353311313864564,0.0274451007524597,0.02467774689658401,0.026613349263969157,0.026099087177580575,0.025985188434969417,0.025706800397694587,0.025670027698924656,0.02532323457019026,0.0234408166984956,0.02487419868701,0.022497781362909426,0.02721453665586067,0.023589982960121398,0.0239783366623613,0.02431692304335031,0.02239118160344959,0.021714372615429134,0.02421538883229165,0.023081281668328405,0.02152899505670811,0.021598412874842838,0.021009226012996315,0.021699554361360316,0.021197099922884165,0.022174874084648535,0.020743662181213277,0.02028303620968769,0.020164505667122506,0.021250365637370813,0.0201581319013156,0.019868698277492568,0.019693986897197854,0.019943868055862863,0.019432744113909762,0.019126542499086224,0.01930893656625951,0.019173633099831257,0.018993664897961237,0.01904839636824112,0.01888858271978224,0.018980280695032895,0.018859265682384593,0.01866982265611319,0.01863889193411955,0.01861355609782168,0.01859140934706383],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(223, 44, 194)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9046982887846401,0.9461617807488109,0.9569642098072656,0.9624566019731193,0.9699925494561996,0.9690100425278744,0.974840283460185,0.9754503403551222,0.9778112719993938,0.9806185264787272,0.9778816191782777,0.9723618107802489,0.9829884609740969,0.9815236438431716,0.981076224098097,0.9829343993468839,0.9833968554430063,0.9786574096551895,0.9799548074222072,0.9693797988277408,0.9856553387186947,0.9847735799314941,0.9806840835414998,0.9842821842684959,0.9800808303504777,0.986493403100923,0.9844675236689991,0.9866005513544127,0.9864402813989241,0.9864390290238801,0.988845718435459,0.9862890900046412,0.9862699509139473,0.9883595946201502,0.9879349736922273,0.9868828618876764,0.9874964613870058,0.9888543487855026,0.9887452415709341,0.9899886965892487,0.9849847555817534,0.9888133517918026,0.9884326670076858,0.9883263122474045,0.9905081231750341,0.991821875696306,0.9878406454033591,0.9893616582787134,0.9901064564159988,0.991520558183447,0.9911336989360223,0.9908789664947302,0.9911894602133892,0.9890808278214089,0.9923814564468519,0.9922716239005511,0.9922524456300651,0.9906484020683504,0.9921934155736316,0.9930049058682929,0.9923808159831358,0.9920439659944232,0.9927014394331706,0.9924920697996794,0.9932203654971344,0.9930026021031139,0.993590997701847,0.9926579785193663,0.9928919386322598,0.9929468159901896,0.9927265824456385,0.9935940118491441,0.9934803330346349,0.9932755485520824,0.9933174662480883],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(223, 44, 194)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.08557525741070816,0.07507128554725975,0.062078465347232686,0.051725076246507386,0.05941836188236872,0.05623557549418043,0.05098503147767172,0.046912650779350515,0.06741032899962258,0.040175233233425624,0.051453398659671704,0.046449764111300106,0.03856932601517009,0.04071134126319508,0.04800492597046177,0.041772497097139094,0.036303247280956547,0.09864386471686085,0.0850370970122593,0.04117077558310991,0.03732147878131915,0.04750938933483514,0.03668766391185141,0.037411641746861825,0.03601784287500627,0.04665953038945231,0.04592493969396627,0.03495541276903087,0.0339278218698051,0.036719360054051346,0.034537605831201135,0.03359600713418931,0.03434201902437865,0.034208937357995926,0.035674542751918545,0.031908551124772665,0.0374417432625474,0.031977053821291714,0.03797943979962585,0.03217587433115313,0.0337952482997347,0.03112790829844491,0.031342321401301936,0.03278864177385556,0.03395044046783775,0.03172878192872116,0.03857737017661026,0.03234533988118581,0.03112556835527682,0.030438412234340746,0.03221120049351269,0.03134575065263768,0.02969956854006269,0.030635258880575087,0.030113026210430154,0.02978871864141877,0.03030242533054958,0.03009870981996002,0.03265622326803371,0.030592996929212125,0.03016707398944704,0.03147466489693144,0.029400162718857276,0.030804936505879732,0.03059646064012321,0.029393669125652805,0.029971622294167062,0.029177450245291097,0.02948110608761663,0.0293702399454166,0.02945766429003981,0.02927591197474306,0.02938191778061726,0.029312681004763468,0.029309951618979476],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(223, 44, 194)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9454625103649669,0.9491311764426534,0.9589342301746892,0.9733716447729138,0.9571423777391131,0.9625975451341046,0.9709198900669703,0.975169645146684,0.9471743336269182,0.9783392723203542,0.9646810865090769,0.9777486872989335,0.9805713985071977,0.9748447958271759,0.9744789242763322,0.9770292164381604,0.9829673837783203,0.923052898250285,0.9277335912437206,0.9784426942261265,0.9809094939292414,0.9746094008122188,0.9836308126736452,0.9819710254521244,0.982064610091708,0.9736326935308147,0.9703781492474457,0.981852537671784,0.9801035603323061,0.9812177881777201,0.9816344725636239,0.9823585995304684,0.9792171296183578,0.9834252339489205,0.9835318134245957,0.982747298197346,0.9779189901902923,0.9822258677792143,0.9777495003366444,0.9859597109839189,0.9811108487884904,0.9826039072470717,0.9844214534518116,0.9826844560907029,0.9815714263812529,0.9838842839326527,0.9791551300309395,0.9828138973752487,0.9829956226592436,0.985557676949708,0.9808831577673377,0.9827970989570309,0.9841150479976697,0.9838451858597166,0.9823604863794847,0.9839426523948889,0.9838294318905669,0.9848420489925132,0.9827608272878324,0.9833253388187456,0.9842955807796013,0.9841959431788904,0.9843774981186293,0.9832475282063527,0.9834936751708732,0.9840817492725136,0.9830700170393585,0.9838566243372459,0.9843570297660813,0.9845974908824766,0.9845974908824766,0.9845806424350824,0.9845974908824766,0.9845974908824766,0.9845974908824766],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(56, 235, 115)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv5.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12239110082183097,0.08329814281933359,0.0709492122099435,0.059474248594400675,0.05761086618760334,0.05398109051370129,0.0518649319782085,0.04690493813460908,0.04293284343442488,0.04007757996109753,0.040257001254293656,0.037202944904821844,0.03491836418051012,0.034685123678113314,0.03950411827865676,0.035552725965265965,0.02972145803008156,0.029822079380728807,0.03570401234742156,0.03174204003035954,0.030620292868179282,0.02838619772775818,0.026948940215412557,0.02712708150654314,0.02722055376224638,0.02504125937034311,0.02648440557281313,0.027256417703758265,0.03154248561868007,0.02614102675414133,0.0246353872664452,0.02405134736211439,0.023258790354599657,0.0243414320555203,0.023257064765568863,0.022874237170905934,0.023484941191740872,0.023989870070233378,0.02538581317267194,0.022882456863715067,0.024827413195221422,0.025516356925511005,0.021748853891763627,0.02389906339842361,0.024299588755737532,0.021620554644251742,0.021614198676840682,0.02123319634875413,0.020579879951128845,0.020518567404983926,0.020787558180812366,0.020546324908272507,0.021009626265568286,0.019896206090958598,0.02042797292067423,0.020037328147938537,0.019965405513416176,0.020246087242971854,0.01973643075559557,0.020267332044834396,0.019545512093190543,0.019534064999611928,0.01953436073688161,0.019660363141511313,0.0193402193439751,0.019445413256539854,0.01900488099068369,0.019063930205663694,0.019061958582438143,0.01886060402215483,0.018828586223647255,0.01883042488259958,0.018798011384759297,0.018778590921564337,0.01877134010528576],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(56, 235, 115)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9064383623669914,0.9424252284481662,0.9521205902790398,0.9601765993051864,0.960386199531204,0.9649948071610553,0.9651366518987257,0.9715526452542923,0.9749833358406285,0.9764439808350945,0.9772267563062957,0.9775048292944677,0.9816113619869621,0.9814618236124086,0.974111715186971,0.9801712896874116,0.984326721462221,0.9851016842364129,0.9774628576918575,0.9833265817464282,0.9833977954792056,0.986107520953564,0.986614231709992,0.9864232257163866,0.9862202995121171,0.988940731796448,0.9868900888819472,0.9850649943109454,0.9809969159319849,0.9872425479410957,0.9885091352021758,0.9894796155403867,0.9897803591763696,0.988437696918204,0.989734981025582,0.9893537834515774,0.9898373623664106,0.9885731927672498,0.9871389426221453,0.9892682270116119,0.9874910294362003,0.9873019900768408,0.9912182922982111,0.9881386146963972,0.9880158366428481,0.9914685894821348,0.9907682991470491,0.9910565260904595,0.9913394038904081,0.9910687849431513,0.9917821412024368,0.9921270972784524,0.991787731309774,0.9922903995844521,0.9914601303740496,0.9919076061521636,0.9923614072462669,0.9919566288554887,0.9928083741663156,0.9912171869993894,0.9924916946160521,0.9923848897036388,0.992697605775438,0.9925512007537084,0.9926964915158704,0.9919239615998157,0.9929944163160068,0.9932878000148903,0.9932279751198851,0.9933186853568655,0.9931130551083395,0.9933732229467781,0.9931762576597736,0.9933650387440908,0.9932162143744715],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(56, 235, 115)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09763490843916267,0.07606978357043054,0.06715860915757536,0.10698448998719147,0.06568364448768577,0.0595289787373592,0.06026443463215713,0.05936324453026159,0.04910272263835386,0.06083154804313306,0.04963621663320106,0.041940849858153724,0.040316455908554934,0.039452034762430026,0.05026113551455675,0.038981362040509886,0.03734356733839127,0.05348815559521573,0.058589936388317256,0.035855353301026155,0.036511080676235284,0.03773680211035247,0.03647247479133999,0.03837415436442775,0.033615877205358746,0.03420102293651128,0.039301525266309784,0.0383412057745088,0.0386608658915328,0.031838751940178295,0.0336955270099476,0.03219490117326225,0.03185981019972935,0.03322900599476808,0.031655784084420026,0.03447694783050989,0.03157689219590315,0.033796413680634546,0.03449619005584635,0.030417569294008604,0.04109296972296902,0.030807585515005072,0.032948496039073495,0.03532852129018593,0.030307542409786243,0.03038507283399605,0.029340611674736457,0.02961961931430597,0.029248905361108354,0.029670738119030326,0.031909888294042184,0.030212395172246134,0.030061837060447412,0.03044257095384434,0.030047292958727406,0.030321453137905737,0.03260537184442032,0.030966111124073926,0.029274352711602995,0.029225830349725545,0.029285638264774046,0.02930665403297267,0.02926630487095859,0.029233376850786898,0.02900372762553061,0.029237117736097874,0.028835897454895926,0.029558685352814567,0.028889498631773945,0.028902108291375267,0.02887663342493916,0.028856860608169714,0.028849080857849613,0.028847549958122554,0.028849740165103343],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(56, 235, 115)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0012742905281420724,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9363652883066694,0.9520947234943103,0.9581534273034905,0.9208117662082758,0.9599903084048144,0.9643014203096364,0.9556869278879627,0.9556190211491635,0.9691795230170601,0.9514985489263492,0.9664463274324263,0.9777404665337529,0.9796994800997701,0.9775575436235417,0.963946409606954,0.9803367134596418,0.9830216177662175,0.9671898299351981,0.9509824037945533,0.9825207924089321,0.9811283922774642,0.9814723806506898,0.9779378715201588,0.9787157422263015,0.9824558849531527,0.982782811129325,0.9701813487735361,0.9794115313283562,0.9787319657855048,0.9848339212946656,0.9829528710276255,0.9844186568106102,0.9845730818246281,0.984156712638222,0.9843047779480799,0.9788487154886795,0.9823463139531471,0.9834879919938874,0.9816844729949649,0.9857218675141214,0.9769084076506075,0.9848813057447461,0.9835048375475866,0.9823830915619388,0.9860601581320368,0.9855818050427225,0.9859701461363388,0.9840572535822786,0.9862110999896183,0.9853086162097386,0.9827514323888321,0.9854951733240507,0.9830801671193669,0.9857808732013066,0.9860736144645605,0.9832801487761456,0.9815373513072025,0.9839929720060687,0.9842798944252771,0.9855590172432849,0.9855231470804517,0.9872133704601325,0.9847743379313484,0.9845460917788108,0.9867377151699956,0.9833328928796948,0.9857564880857423,0.9872270600185536,0.9864936864910728,0.9850379788962731,0.9850379788962731,0.9864931688535703,0.9862546285088113,0.9860075942036711,0.9860075942036711],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(196, 227, 61)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv2.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12903224421496517,0.10870212991759266,0.10353091277549084,0.09644653194104956,0.09442956663864448,0.0921444990534553,0.0864836275202029,0.08938081398998886,0.0850886481904765,0.07851313391757994,0.0795369720745742,0.07635815698410228,0.07739584653852731,0.07482874069580507,0.0788793255902511,0.08177334482163907,0.07314276306250934,0.0729139808450163,0.07106974274090748,0.07042461906315467,0.07049638596955443,0.06830051032674818,0.07135766618346362,0.06942797767815584,0.06812457501939483,0.06739266878345988,0.06667402690961394,0.06588686060454838,0.06435526022420852,0.06525914447482373,0.06601929811745848,0.06387085584413692,0.06339155531148047,0.06337706145381326,0.06216741711891258,0.060538017906820375,0.06185479352735026,0.06038172577869441,0.06097231924721602,0.05980620714329089,0.058658499339806665,0.05800051405121234,0.057660421968052614,0.055701138898474244,0.06011925951448639,0.05545066245932896,0.053380432575599705,0.05398752533863773,0.05384488974581606,0.05216222623574365,0.05049965519027066,0.05033156312788748,0.049380747903082924,0.05166293220575323,0.048786714398232796,0.04887139595626829,0.0480138827315857,0.04703598033641076,0.04616783459298526,0.04660278081825645,0.045577686982397905,0.04585927304772304,0.04534885802083272,0.04495168296793073,0.04450202149777478,0.04389317140222142,0.04405425144498154,0.04364436880897684,0.043486907316920134,0.0434672785242262,0.043320456383598355,0.04307244685729208,0.04302707571975827,0.0429463877676552,0.042907025118839565],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(196, 227, 61)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9031304749298377,0.9244024789165898,0.9278431187311935,0.9359433761940146,0.9359013849655392,0.9383625393739881,0.9448391323368555,0.9407543852309019,0.9447564488890087,0.9491247558337513,0.9495010255796914,0.9506262014275988,0.9503673444735201,0.952315087790389,0.9484010586165194,0.9462912578542914,0.953471746275445,0.9549872306871063,0.9560501668650112,0.9560212568637104,0.9545465773864403,0.9561569012916854,0.9540654059300582,0.9541306304835461,0.9563654255654543,0.9563709024800264,0.9570470667981903,0.9584062965837883,0.9575932093365548,0.9592953900750717,0.9575101636109998,0.9597116127687246,0.9594564438482697,0.957865635053173,0.9586325665069103,0.9620891995294025,0.9589875056671702,0.9612641611553859,0.9615847303384253,0.9624283506256761,0.9617153109831444,0.9627656078762442,0.9620151681973977,0.9641306394524164,0.9593394425597678,0.9651456273623684,0.9644740642473197,0.9642722790966874,0.9644830785774835,0.9647061741650347,0.967412444317822,0.9679881996248935,0.9678742087133468,0.9642756318391823,0.9669396282727389,0.9657751881738782,0.9682050794375188,0.9676401902199341,0.9700100456643862,0.968693621836755,0.9686926079389309,0.969501789839686,0.968638225010225,0.9681710246983215,0.9692813725775963,0.9700010269213861,0.970348629541663,0.9698364813538064,0.9708789407972072,0.9707308797138929,0.970586586736,0.9704128224631444,0.9707975792906823,0.9709100067546762,0.9707317449075126],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(196, 227, 61)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.13147242914155585,0.10368941173315868,0.10037683127159924,0.12001297964468036,0.09904400903949213,0.08977607100485117,0.10272830502478937,0.08676401861139998,0.08540774714905781,0.08087233539504284,0.09323057804218272,0.08062944536151755,0.08270144567047198,0.0756593206289298,0.08395901191787622,0.07739795474559581,0.07472537229048837,0.07567526356563535,0.10179486741212634,0.07549388572522454,0.07384954307906816,0.07596688662300405,0.0700668720342859,0.08766822521526789,0.07506202047121074,0.07552486255099274,0.0861088484628094,0.0764603025263937,0.07119669953162727,0.069597148654592,0.07642610973816147,0.0700884489310566,0.06887821500346424,0.06711665002750777,0.06731386887248848,0.0668099730387586,0.06896778839253069,0.06562367817482997,0.06697847207182461,0.0628182109995806,0.06494705992465986,0.06512624467259011,0.06649116092838372,0.06240769670284081,0.06386109720595513,0.06171102399371334,0.07074788821112249,0.06005992576531118,0.06485012347243496,0.05796384269121996,0.05838079292749621,0.05818210236497761,0.05638942790195295,0.059166281119859505,0.05509327544840341,0.05559728083639211,0.05729329660278825,0.05429710233744067,0.06011812371486651,0.05514524086439323,0.055510869407162225,0.05393173531517131,0.05341086092683458,0.05253869782864433,0.05277884824527908,0.052706942325809976,0.052539234672420214,0.05226008934304886,0.052718912505407105,0.05205701377844483,0.05200413989344823,0.0521123243130974,0.0518458513126955,0.051810485677620796,0.05182008752247312],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(196, 227, 61)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8938565323543187,0.9329328155072799,0.9383979907582978,0.9151550357545337,0.9305859256430814,0.9447369670906188,0.9310424778225849,0.9452125397748539,0.9424181023129568,0.9491013302021464,0.9452888251465827,0.9523009273469059,0.9443017634220121,0.954633045891393,0.9434416820177963,0.9492406000864914,0.9551227724026816,0.9544807304246059,0.9312711484476119,0.9508336110866267,0.9572563715477581,0.9490709518215995,0.9562911423469456,0.9475151579171797,0.9487020938574711,0.9504984766067418,0.9320610675225328,0.9483164824800375,0.9545875957933387,0.9589298015439048,0.9481157818898657,0.9585116837993485,0.9577296264347045,0.9592363051974326,0.9565165555596353,0.9559848218404855,0.9560574303479386,0.9622748106370189,0.9604870775790088,0.9616670787395152,0.9613758864821771,0.9557853264786516,0.9619921598415057,0.9588778634022492,0.9568843248513595,0.9575329493830259,0.9459838316400646,0.961412183596393,0.9597548278588184,0.9609205154038569,0.9597190944490958,0.9587053214511676,0.965561392766706,0.9640553749491946,0.9645048574103525,0.9659999557725483,0.9657231535159617,0.9642527773287561,0.9630834547546047,0.9674196811284415,0.9630153984032613,0.9647556308255213,0.9674790844112419,0.9673056747919327,0.9667693272039863,0.9692944512161932,0.9675568907253855,0.9685343689482779,0.9653012299147581,0.9689237251776448,0.9694640106067494,0.9704189261368995,0.9694286415694181,0.9689291380110581,0.969415782859863],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(191, 229, 119)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv3.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12050084249294638,0.09700519032733683,0.08830717799658082,0.07711025806483124,0.07632852744264154,0.07346429320954785,0.0683282687952838,0.07035681113412973,0.0675971159137538,0.06460082381579679,0.06694504225718333,0.05916741205825292,0.057927791199050546,0.05729375112490283,0.055419647693634035,0.0595392450045918,0.053809753996570765,0.05804994305999009,0.049390962739546546,0.048707285342484134,0.05650999818726913,0.0503933324078514,0.05051832119214166,0.04641014417592876,0.048044063338640766,0.04749912971807783,0.04749229648166506,0.04621773097762264,0.04507635651457146,0.045995442316635916,0.04451973072817508,0.045148217451053525,0.04583713829619778,0.04480981520309891,0.04522522030076074,0.04536703078189325,0.04181933371949715,0.04135088956779358,0.03919340747601661,0.03884738412902254,0.041327647035269394,0.03743897934848569,0.03669740703951179,0.038461655349324525,0.036656863282499716,0.03702657665387393,0.03532346463724886,0.03517176310306699,0.03591198451552467,0.03514754399870611,0.03509052988880006,0.03350389879730286,0.03329313813889013,0.031775463823088254,0.03131745195513397,0.03117659854417814,0.031249569327851067,0.03115159846150998,0.030040605852694035,0.030193171556292257,0.029399292005432977,0.02976366673155612,0.028909303512923496,0.029509450208485333,0.028923226946715227,0.028534730319851178,0.02836764554097344,0.028278653152534507,0.028110531916818393,0.027800837793693647,0.027691936784798338,0.027600043526459124,0.027560612871786336,0.027493988783370792,0.027464633122223078],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(191, 229, 119)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9114273036276964,0.9322801867958721,0.9387191474693241,0.9484427270690372,0.9493332162190902,0.9505324451760894,0.9529427881515992,0.9525685570289273,0.9546233580129846,0.9564676919825268,0.9536643122825074,0.9613175189712325,0.9612564191489035,0.9605524606594759,0.9617276151704964,0.9594709226685066,0.9629942713879757,0.9597143690087727,0.9670596816559359,0.9676354028735487,0.9605703280861254,0.9667372384410149,0.9650199390094046,0.9693683366360616,0.968171219181945,0.9687177544943525,0.9665976937807716,0.9710157144942674,0.9699613680050309,0.9691166335996366,0.9702331771090947,0.9708133300621882,0.9687427769946663,0.9693763306872935,0.9688856114630777,0.970463085815021,0.9740823675355308,0.97289056309454,0.974645582630485,0.976056635249386,0.9732461204568258,0.9773589955893386,0.9782920334545657,0.9752790666667989,0.977751848815995,0.976056917131715,0.9795355913008201,0.9793063525253594,0.9793000980221374,0.9791957303726564,0.9789396163831643,0.9809450468566986,0.9823160982034723,0.9830887646998212,0.9822737715792355,0.9828294114787177,0.9837182192402536,0.9841258641786724,0.9841632944622041,0.983845558161,0.9848413505212754,0.9850446894222075,0.985267220586722,0.9846912449709345,0.9858470081520503,0.9860719814227196,0.9865435551992771,0.9865335670778976,0.9862088665122338,0.9869862541691408,0.9870903393917522,0.9871865871021877,0.9871342445398378,0.9874653687331347,0.9873348080517164],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(191, 229, 119)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09799005760974491,0.09714634090764417,0.09698720421782883,0.11326990787618349,0.08663835824560054,0.07952590789581902,0.10023830364557476,0.07659258454833244,0.06888554029653163,0.076573264998259,0.06633017836362635,0.07216835635429396,0.06489112307525582,0.0687155964243453,0.06517269437767796,0.05981139455669115,0.05705339973120345,0.059395961827019235,0.06808281836333553,0.06087784790193912,0.06895425764146129,0.05760331654671541,0.05195380171498482,0.053865520738039636,0.05168230281150628,0.053612304996071815,0.061989553810394914,0.051207707676383635,0.05662002096983166,0.04630082978057288,0.051895926045285874,0.051996570843499144,0.05163563492986345,0.054341728396432094,0.04861759227985369,0.05078212241690183,0.04750251923895783,0.04592103360547233,0.06418838630529614,0.043468869820595606,0.04216915842696154,0.04414026049762657,0.05090933520206061,0.043982244687494135,0.04382182143859028,0.04403242991142666,0.04623835165424855,0.04241208311464778,0.044575811479611906,0.04411266750537653,0.04119598617156347,0.04269743445249358,0.03884612149798993,0.039918059061995075,0.03759599699289938,0.039877788213008046,0.03921506994778348,0.0391874110038133,0.03793496515896312,0.03746960799616227,0.039419521537330963,0.037264916188118793,0.03904981949075391,0.036769648963950345,0.03676872298735933,0.03703894072637935,0.03696120829414256,0.036874355910877184,0.036558669385631466,0.03635725308580907,0.03624729279595142,0.03633729656742201,0.03633348616384149,0.03632449764003049,0.03631508517050251],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(191, 229, 119)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9419662373784196,0.9398226173216372,0.944688222348421,0.9128413062154997,0.9405739307324141,0.9515144377505301,0.92034972068309,0.951654269789537,0.9600466570780325,0.9392854041121939,0.9553603160949772,0.9455887891120246,0.9636052084480402,0.9447963192539028,0.9562796256741378,0.9550157937291124,0.9644262613456235,0.9607579598918208,0.9533850323242848,0.9601022032366655,0.9490940286596478,0.962429803239572,0.9675484423341856,0.9686751448327453,0.9680831974096737,0.9670383777168272,0.9642260532189634,0.9695377371318856,0.9669861457027388,0.9727233914750933,0.9611422251424332,0.9669594238390714,0.9690600108617392,0.9631581109878333,0.9726137925168099,0.9700325747808373,0.9686126983958895,0.9736930224526058,0.9529673000684812,0.9755349552684512,0.9757148440623591,0.9756026862066932,0.9671220079982152,0.9720151573074961,0.9734416470321515,0.97534753024642,0.9744526571027571,0.9758756147739995,0.9775826913282224,0.9774955303533367,0.9755791343513491,0.979356515351813,0.9792962727680369,0.9755642409299027,0.9810144874639614,0.9790716583061315,0.9791991885062595,0.9795454357796354,0.9829893603788756,0.9802508865239091,0.976591705465076,0.9817126308217866,0.9811976335560226,0.9820357197943613,0.9812116433663521,0.9797573188140085,0.9805280374130887,0.9809484324921878,0.9819500512732038,0.9822296460117415,0.982667015445137,0.9822496997205926,0.9827064514864042,0.9827064514864042,0.9827064514864042],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(191, 212, 164)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv4.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.146506715179309,0.08963045385694995,0.07613939874360652,0.0661831195650778,0.06458119304780288,0.053647247731753916,0.051556529120071644,0.0517941524303451,0.049696056549901405,0.04901366762279235,0.04757147911778157,0.04719302338662153,0.04215289794360922,0.04251269493401392,0.040501287700725724,0.037089103236495766,0.03817624296109701,0.03950483210017864,0.04120591772380973,0.03547087692893521,0.03681314540909601,0.03912134523645368,0.03719111015638262,0.038034966822291155,0.037066604846497445,0.035733726738876356,0.033726968437877036,0.03595302923105632,0.030894506042492484,0.032196656900446305,0.035102299504400254,0.031938545359565286,0.034187361207563445,0.029533713961282546,0.029738029822246315,0.029435023464868433,0.029773342287656504,0.029452317569059194,0.027971689952889463,0.02761566567267766,0.02780435972325376,0.026796005909914686,0.03127932081602829,0.025893186156948408,0.025853055043045733,0.025342344257336235,0.02579029868703596,0.027444502470624407,0.0242728779474348,0.024215755535488073,0.024175444902064875,0.024041201682486076,0.023200713688633558,0.023196541190036453,0.02292717238216056,0.022323768968263987,0.021610850749061553,0.021118150725277975,0.02121380721245554,0.02087202674164122,0.0212232705935375,0.021194620091651586,0.020663202466936047,0.02020199313879286,0.02010651635985399,0.02037529755439204,0.019811801623898683,0.019827140994766835,0.01947353778648076,0.01950425608403648,0.01928538397991473,0.01928024868238696,0.019161404990713792,0.019168477041597767,0.019115652767397285],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(191, 212, 164)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8764348895144449,0.9349775257609175,0.9462428114079129,0.9540543725299006,0.9552438804386728,0.9651664815771427,0.9663481402408857,0.9656558145477929,0.9683284734678023,0.9677659177436411,0.9693803824490697,0.9700497360248986,0.9731789728939744,0.9722207892558957,0.9755391066497012,0.97718912116005,0.9752697673700745,0.9767381961103445,0.9737542559461222,0.9792783416222972,0.9788224453906746,0.9758379300295836,0.9773445477785603,0.9768912842853222,0.9781632489337619,0.9780897846385488,0.9803899214480247,0.9781507557360538,0.983266051802475,0.982288253765905,0.9800530655828474,0.9817723269363712,0.9808133886617425,0.9840224946442766,0.984161108143519,0.984407288387348,0.98354976920114,0.9834557436395029,0.9844210989004255,0.9856098155284221,0.9850725337710888,0.9854196429200903,0.982698191360608,0.9872734331727743,0.9867318899046881,0.9865516816318238,0.9865296293354345,0.9852472586247243,0.9891394100742371,0.9894969680038791,0.9887451403587115,0.9893777798824142,0.9894625535111976,0.9899725507074345,0.9892784062833145,0.9905481622282001,0.9905380666916075,0.9907626121106013,0.9904714964989904,0.9910713801247343,0.9907752695906025,0.9910542509177869,0.9918861953865687,0.9921128395356459,0.9915067976514791,0.9918768268397788,0.9918762810557972,0.9912377582415886,0.9920828913755969,0.9923612775297269,0.9923605042973589,0.9926968788188476,0.9924357139832898,0.9925817547741689,0.9926712914182325],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(191, 212, 164)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10227658463917237,0.07696555883101991,0.07403686636501981,0.06563123072852793,0.060389016153886144,0.0636602197981782,0.05211763041689224,0.05535627793507887,0.0555787233603779,0.046758294310356746,0.04736553029096413,0.048301643865419824,0.05079651291120503,0.05289342229052917,0.04553175924467467,0.042767485154351005,0.054311393042610276,0.04503907299123679,0.04219732643761176,0.05157048575759344,0.061005296524857325,0.04417040219524063,0.041116022013921505,0.06290135426107551,0.0406050449051603,0.03900892907862401,0.053625161131632695,0.04553412032598483,0.03921881216703002,0.03943961607068265,0.04467652286194854,0.04633147277168392,0.03772672088080665,0.037207922091086705,0.04626165499392244,0.03676807993824539,0.03497440490577229,0.03547631403322482,0.0370440354385122,0.04063213051287169,0.03834305516223317,0.03664015860031151,0.03510900023569356,0.04374817177602106,0.03427500506139703,0.035559487916349955,0.03587414708078113,0.03463428067587495,0.033947181087179285,0.033075246125254844,0.03269362616579967,0.03226170698084782,0.03300466460409443,0.03357491832213713,0.03289505561807311,0.032774662195714475,0.03318892515094829,0.032572147444761085,0.03177607105052758,0.034275135769159935,0.03076928783323347,0.03660499440486898,0.03203401298764645,0.031141826564196458,0.03675609024888052,0.030561050601124354,0.03275308313545902,0.03033261718772531,0.030819157874563716,0.030670938889185588,0.03070985871491973,0.030710425916611123,0.030711931847932004,0.030561050718899856,0.030563566728760698],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(191, 212, 164)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9236156704992758,0.9545592408428613,0.9533387972872699,0.9598135959667994,0.9591506453434544,0.9506523194756701,0.9685090882684287,0.9699686926401759,0.9649154481551369,0.9749924656800001,0.9758802908294656,0.973982083118459,0.973106460892233,0.9742962173540255,0.9761455595830593,0.975080658316349,0.9604756940519541,0.9698297736476121,0.9786989200624276,0.9626149699921538,0.950046422924233,0.9751718798495727,0.9776647352149268,0.9595423043453373,0.9768530307475654,0.9779174762872153,0.9607598735873871,0.9755255307219115,0.9780244718198408,0.9772161362867204,0.9705861570787674,0.9685586903700283,0.9792902106971286,0.9799477951334472,0.9689901568381056,0.9799972736905568,0.979593471490281,0.9826797384842882,0.9789749083263646,0.977761642057215,0.9779664577368158,0.9790364162446926,0.982537017877704,0.9694613950256118,0.9806729361152084,0.9814075434283757,0.9802075091839575,0.9795698027583275,0.9823673004781629,0.9841351151385545,0.984294198410827,0.9836316689517208,0.981866987663449,0.9828714128867463,0.9836792410122931,0.9843732931368873,0.9830295728904703,0.983295399646479,0.9843160840230564,0.9798601472065119,0.9850012489944295,0.977367613795563,0.9841180095442887,0.9843339461581035,0.9780272143762245,0.9857474267978517,0.9809621327075835,0.9859833607687015,0.9862372847437672,0.9850021672638573,0.9857618438638661,0.9862356563982599,0.9860052470689029,0.9857462610921975,0.9857462610921975],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(84, 159, 86)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv5.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.11789332119255132,0.07810970223663462,0.06411233683546658,0.06232253696816416,0.05681772665606059,0.05225101213855295,0.04574509929579695,0.04550380987229353,0.04449727378299146,0.048315194861073676,0.04154004250681892,0.03889052282681717,0.038152261715028155,0.04005759464404018,0.038112646424961524,0.03578202735978304,0.03531323818474974,0.037938015848261654,0.03770839694294255,0.03362840458426415,0.03321909747915375,0.035990095901530225,0.03727335014294103,0.033711434913562335,0.031660135593884045,0.031324288286567414,0.03182663494861874,0.03417611204574198,0.031168136473442135,0.02931776557966879,0.029865493218735596,0.028031811398909538,0.03220913522158838,0.0317348000076082,0.03201571202496619,0.02665574262676097,0.02602132245906781,0.028815705540834288,0.025805623342900753,0.024699041987731967,0.02761369298755235,0.02671043395159717,0.028276506313288163,0.025502946390583685,0.02512011969193202,0.024962771768849194,0.02373017915625889,0.025364154119808375,0.023481710940036166,0.02282928792036481,0.02326237333752855,0.02412088579174989,0.02316002602617492,0.0227208539529918,0.02316396276873002,0.021788920553236824,0.022158984558279162,0.02232778982492964,0.021349030469820535,0.0213269975808751,0.020970082287073886,0.020722889089434145,0.021080174214310115,0.020509096989056088,0.020442666373677542,0.020329152064062952,0.02014693288902783,0.020152547719535275,0.020006326047057137,0.019864357190306926,0.019795069416701144,0.019767029261278397,0.0197067961758184,0.019693176417194418,0.019658403620465243],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(84, 159, 86)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9075300616573446,0.945869981459353,0.955394226363879,0.9571874646545786,0.9615678114114093,0.9648051059334114,0.9698346831722064,0.970379693356262,0.971242434275471,0.9666219572024229,0.9746830216228018,0.9750786495341919,0.9765645869873519,0.974235568210617,0.9763043880823007,0.9788157300092253,0.9788945202649203,0.9769371550418262,0.9765561159637272,0.9806441964352668,0.9793823477132568,0.9780365755083488,0.9759061757051873,0.9790114074576542,0.982582272312708,0.9818134971775461,0.9817391096919629,0.9800054259052385,0.9822934713674532,0.9845849256356729,0.9837007357269025,0.986063086214425,0.9807787400968618,0.9825278024292606,0.9811736669348167,0.986557790024055,0.987981441784323,0.9842857253189042,0.9874231787330038,0.9876860615750062,0.9849932682645348,0.9867102186314787,0.9838869996299916,0.9870038913725953,0.9884730355590313,0.9892770520829622,0.9892097672968918,0.9879985010938866,0.9889282325376868,0.9895618220286667,0.9890798202406184,0.9879064979614717,0.9891969370092862,0.9899689732942281,0.9891384457214853,0.9907006318306607,0.9891042119921397,0.9901866021256329,0.9912161908770688,0.9913771692787224,0.9911159924706924,0.991626387399345,0.991284304126724,0.9910197572139793,0.9912594759691965,0.991248409526507,0.9912397748445344,0.9913555967259909,0.9920334096962163,0.9916289299009473,0.9919120250677361,0.9923057587281047,0.9920878094362312,0.9922521566309002,0.9922176496286002],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(84, 159, 86)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.11704227121425248,0.06699576393230675,0.06107952984952435,0.06778119408695149,0.052346042195443844,0.053120121032102,0.04894596111426239,0.0663342788098604,0.04748870612112517,0.048803227028691075,0.04453747400508304,0.046828877221780134,0.04136136758214829,0.0584521551875724,0.047731532742784605,0.04609824009623724,0.04835854675556786,0.055202833068944336,0.03721846219702685,0.049674562037093535,0.03518602394105233,0.04169340358976646,0.03659409002186507,0.04793140503395464,0.038073389068297096,0.04755227613131615,0.04001725552878839,0.036131354700453915,0.03514880924341605,0.03215560611734276,0.04468877164461359,0.03200827392259824,0.03470259676479392,0.051000858036820425,0.0329161064705693,0.03409385999146196,0.030746513295317024,0.031201667522953957,0.03313448524864269,0.03396488599914456,0.032706977462850485,0.03373641199160277,0.038925584976615774,0.03052110490213145,0.03075319810216779,0.03035178816400443,0.03558044301480362,0.03203887138067652,0.031003193918269934,0.030760202398107634,0.030327345361721884,0.03245924640050049,0.0315116336895633,0.033750527646533406,0.030163287642792736,0.03061305490095181,0.030960114284888984,0.029507689216395013,0.029906831822546897,0.02974741625826793,0.02918148305766361,0.029435958984688794,0.028917267094158226,0.029445637163427688,0.02932783129186565,0.029344372284883485,0.029042107834643924,0.029156495809964707,0.029170509317691384,0.02938517928123474,0.029030802904842647,0.029092131350253455,0.02907659145411347,0.02906884964030633,0.029068382678367838],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(84, 159, 86)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=64, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9066568636818043,0.9632800954393813,0.9650692816458135,0.9605654217358773,0.9673997776658072,0.9647442298641243,0.9701713223009094,0.9572975204065632,0.9695684708794682,0.9742090774870834,0.9735460336764795,0.9751361230656147,0.9728151390452339,0.9613711553211327,0.9732646652714805,0.9768548660689356,0.9728049978150505,0.9647490248013528,0.981376912034773,0.9700236141367479,0.9804063355788851,0.9735431117076434,0.9820463620588717,0.9679420586708447,0.980956328896495,0.9712494683417799,0.9806951764433298,0.983504149509204,0.9813947377249618,0.9833803918358278,0.9744019582835015,0.9839360580912576,0.985310836804639,0.9623266387743795,0.983153807883204,0.9840078634643045,0.9857470851492478,0.9843215313434667,0.9830634798780707,0.9837661887528362,0.9837267484396692,0.9788194953490736,0.9780127214956142,0.9860493718188172,0.9840545876463206,0.9864938614810251,0.9776341077028391,0.9807071633552293,0.9837008427425947,0.9850563230619401,0.9850463219496305,0.9825219590161793,0.9847437965805752,0.9786046836500398,0.9855702984524002,0.9869945325044537,0.985124569260439,0.984045379355281,0.9859956915518533,0.9852387634660514,0.9855367362231927,0.9864645322546404,0.9864651049950458,0.9871915890617262,0.9862096601178779,0.9864966402306389,0.9867304492337742,0.9867364735624696,0.9864529910560882,0.9862662176979838,0.9871981981102582,0.9869614649045282,0.9869614649045282,0.9869614649045282,0.9869614649045282],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(65, 49, 191)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv2.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12267610412965799,0.0956172324196443,0.09543065090940034,0.09138624977922112,0.08420924114539179,0.08152675110961972,0.07964950392677748,0.08277649962922552,0.07589640979231564,0.07623744915492346,0.07692477269629433,0.07566392548203059,0.06969133424151258,0.07246510559588432,0.06946258220527181,0.06927679258847838,0.07360271094590938,0.06796849779656662,0.06922813774537795,0.06781080277190722,0.06909085800455199,0.06838258095298982,0.06511275330803785,0.0641863332632481,0.06627815689417914,0.0628902903998021,0.06214449307011848,0.06469469653081922,0.06675888390236567,0.06570630284177752,0.0623125446109018,0.06113791749017891,0.058116088792220834,0.05861890669848092,0.06034816123912151,0.05989003590428679,0.05615404650466549,0.05828351978925222,0.055558912884465336,0.057153009522582524,0.05646190441385304,0.054642388830548165,0.057293913358948345,0.052769765800626825,0.051841805873167744,0.05319605327690317,0.05524990009175953,0.05194143271084367,0.05377185645364951,0.05257347501379108,0.050838016885047924,0.04937190223412689,0.049889273595017936,0.04861381278210079,0.04832182930268236,0.04796177400585439,0.04714131494226598,0.04794129463849335,0.046963484054441985,0.04615199230312619,0.04558137355613408,0.04568301996414604,0.04528130138589481,0.0447082985205339,0.044196654664606434,0.044376680015971434,0.04429001283980181,0.043721206787064724,0.043677380440366224,0.043488624362959895,0.04340925055970987,0.04335180600104429,0.04320190293247143,0.043187967545319804,0.0431404609570252],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(65, 49, 191)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9060672991650317,0.9373078878756845,0.9378546950783229,0.940599038433627,0.9478537107955181,0.9479614333014781,0.9491959785141092,0.943908244100962,0.9530432246883022,0.95288548112108,0.9502788678245917,0.9508314834519493,0.9568974333964557,0.9548541570336175,0.9570544064350742,0.9561092471388069,0.9528787538554905,0.9569560113828798,0.9570298724467112,0.9558752068999908,0.9550341165658476,0.9555046268400325,0.9582069379529513,0.9606188021028411,0.95887371143334,0.9612547497366579,0.9616413294304031,0.9598593229764263,0.9562564920427098,0.9570728867616423,0.9591067704659796,0.962101969815863,0.9635069626305032,0.9639401001600696,0.9621162063634388,0.9615504350597202,0.9655620577250552,0.9627993251558395,0.9648559777235486,0.9629637394848927,0.9651582178092781,0.9652693469320468,0.9629710284994799,0.9679376309518041,0.9673913971959558,0.9665436635406377,0.9655652082919045,0.9683021517246885,0.966485239467445,0.9673193738480483,0.9683788601422341,0.9699903489873093,0.9684020287173246,0.969499882719829,0.9690001772686185,0.9698667685671045,0.9702472868396667,0.9692710436650254,0.9693759575598789,0.9712688197591639,0.9716513070900032,0.9711141994388914,0.9714909007908138,0.971842548920422,0.9722590206638096,0.9726699276745351,0.9718253280537015,0.9727315315877391,0.9724880301251908,0.9724707576443441,0.9725579464186287,0.9725756478829531,0.9730573092932098,0.9729453736656173,0.9730080101380992],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(65, 49, 191)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10493946843028479,0.1017082720482882,0.1155128641994958,0.08801866501979402,0.09127330422094188,0.0809677711876807,0.09444869507833854,0.0910171999912901,0.08057607909555697,0.07738881417361322,0.09740777692127064,0.07716260744431584,0.07786248951535864,0.07209763380977297,0.07328254646042369,0.09540448950225955,0.07333996689708781,0.07353230047574158,0.0779019505143985,0.07192877534636108,0.07522033764836715,0.0887875286420596,0.0720100870298356,0.08194488412428558,0.0712674413913304,0.06733266620599117,0.06744108705995829,0.0680522358243408,0.08089827424574554,0.06616713766687105,0.06298591681464841,0.06973816370943568,0.06342208150428595,0.06261878481640439,0.06189638000378494,0.07498689006079513,0.0683395504388203,0.06451741002781694,0.0633513395524107,0.06167568636616481,0.06152561991587537,0.06552421793188017,0.06423331745916216,0.0576052429084106,0.057877085283654664,0.06344811301870444,0.05945523554824062,0.06134936670257463,0.05816591013338148,0.06244730878224487,0.05663657819282558,0.05553661067666057,0.06039388921764708,0.05516158579704688,0.056293471608170116,0.0542494922594721,0.06168905781818829,0.05581184612311858,0.054186074850485495,0.05369285876091403,0.0550164552386274,0.052020952207935636,0.05259957268987734,0.05206192827357869,0.05212154005145289,0.052731635618660455,0.05169269252581285,0.051468710040616004,0.05196979852374067,0.051077704922747366,0.051812456714635864,0.051017980948346586,0.0511671198704808,0.05102508480811037,0.05103815776580797],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv2.1_weight)\",\"line\":{\"color\":\"rgb(65, 49, 191)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv2.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9342499193414456,0.9334333873281648,0.9100772557620793,0.9423213721354854,0.9307154275657408,0.954893635970447,0.9361421926438199,0.9428016717512822,0.9511027603143111,0.9511747097177391,0.9228065359508497,0.9540499325817666,0.9491256673244416,0.9532143303535653,0.9563168082332559,0.9351278521258312,0.9564464729829637,0.9556071748336847,0.9452469120392111,0.9580244328114819,0.9479714672879468,0.9438464158395371,0.9551534051014561,0.9418871736669644,0.9557007032413142,0.956034827076034,0.9593774681962782,0.9532461248610921,0.9480235612113076,0.9581744037948644,0.9606792242920981,0.9539888469251653,0.9602501663377148,0.9605129886653542,0.960460941676637,0.9575094861745472,0.9605525532453456,0.9575471259424543,0.9577344369186749,0.9640372943726528,0.9610407146535674,0.9564103745268898,0.9593551952079622,0.9644164101520776,0.9659586751635236,0.9586409351762197,0.9599561991456755,0.9646247657455234,0.9629570244687546,0.9607218024482003,0.9643789862854665,0.9653841338477913,0.9647401922369491,0.9669022016054247,0.9681849011773642,0.9671377157314428,0.9617475216316614,0.9689569991848874,0.9682041510461692,0.9681681913350872,0.9680047473166243,0.9674520359999992,0.9683300363154415,0.968451173529983,0.9689175530663707,0.9691271935353845,0.9699262893706408,0.9701544912050144,0.9690628377963836,0.9696374350738511,0.9695301552390985,0.9693462141017466,0.9698810845241175,0.9693744278467283,0.9698627096704234],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(75, 175, 234)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv3.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.14460273780363941,0.0917934668227979,0.08051859489540737,0.06958516770451066,0.0665252441017625,0.07576243977605408,0.06102351103178551,0.05843598548694165,0.059312186185334556,0.051083781313309005,0.05228095472644148,0.053990410230755126,0.052676258384242634,0.0443388991697957,0.05017327031276412,0.05131314410684035,0.045244009675882664,0.041958558160005155,0.046637238224996054,0.046158421173145545,0.04297030354761449,0.03936246165329375,0.04432453465139129,0.0422530676900726,0.040602429058372906,0.03893311755669827,0.03918500845784584,0.04509080288097484,0.03769796136361286,0.03880633685955477,0.037552714925622066,0.04095882629679104,0.03746354947333893,0.035979381699503356,0.03532483144689944,0.03359720352333026,0.0344569409178636,0.03350268059366416,0.033636670049284195,0.03242819314696671,0.03364943842118753,0.03251244408960741,0.03179480062385878,0.03263339345414887,0.03085291839726397,0.03148829475992529,0.030921753790370377,0.031001305665910448,0.029644793630038885,0.029748671279125605,0.028963306698978016,0.030382073567785484,0.029074297680541924,0.028328875028117553,0.029531803460874917,0.02780118789267806,0.028001023042959854,0.027313922549185066,0.02748104236072691,0.02712876830049465,0.026785511185947426,0.02810664323562882,0.026854136143677695,0.026477844012476597,0.026605630899008196,0.026220213904345296,0.02603077142235886,0.025804068032920974,0.025702254552253334,0.025705002871729256,0.025532545395332035,0.025515122239276305,0.02546509963685363,0.02542432183815851,0.02539857072629811],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(75, 175, 234)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8802909762615585,0.9380434194146358,0.9474104063385795,0.9554524130765919,0.9569419337480292,0.948128962648468,0.9593748437500677,0.9619430892684608,0.9586661132533798,0.965168955717868,0.9654646638168551,0.9638271431106031,0.9639396058375467,0.9720563298395751,0.9663537976289795,0.9652614079336614,0.9705932243724752,0.9730808154081427,0.9698442448141136,0.970756066212537,0.9719371016391927,0.9752051480373792,0.971557073275707,0.9739091053018788,0.9741362605863695,0.9760062758280932,0.975006383119592,0.9701970018060874,0.9763252856446547,0.9752423843918894,0.977398796800705,0.9746071189488408,0.9773528176837872,0.9779060120313438,0.9787917992986302,0.9804054117776054,0.9797736344000882,0.98056311366684,0.980111637912272,0.9808932746865306,0.9800055116405293,0.9814308919371919,0.9813207273548785,0.980865122003338,0.9820436171490249,0.9818756430534231,0.9829092971433739,0.9823441440898975,0.9828276087354405,0.9826906600914451,0.9842533604834728,0.9824824074109672,0.9833252208040536,0.9846655127359121,0.9839492368638816,0.9856811401812163,0.9846595027189694,0.9853682218290556,0.9858437763440379,0.9863332110508591,0.9858320539161882,0.9851886799279914,0.9851229495629815,0.985648570217583,0.9865294538687547,0.9854402435487868,0.9865665112289955,0.9869039815568112,0.9865163764817775,0.986667736973836,0.9870529595026571,0.9866247017096226,0.9869953055878956,0.9870283616520302,0.987018392935702],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(75, 175, 234)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.10452700818527196,0.08565631701876618,0.0750974296448157,0.11035591321405266,0.10404178487885858,0.07248084403804897,0.0612811113858141,0.07499736214626286,0.055907147953805233,0.05562543818067849,0.05480348031545423,0.06915162709570423,0.051901945817409104,0.05484348329942661,0.07718547187310314,0.05778030931335135,0.047712504287663195,0.04919498526712054,0.059898155046902164,0.049668643303548345,0.04578830897296007,0.04727495041807083,0.06486790172525288,0.04333218360684581,0.045005566895622566,0.04355109546756007,0.04206656058834181,0.061967593139594365,0.04891186290231767,0.04279875666722399,0.04348673992806284,0.041228172825681385,0.04356552201396821,0.04178230927879458,0.03921894188394252,0.0395648267644992,0.04439646664917264,0.0480050297639624,0.0425751867214429,0.03740455996488378,0.04297739748948628,0.04138443721938379,0.03658355173376417,0.0369391298017551,0.03644516448077467,0.04089882100877893,0.037801280907032005,0.04485189787966689,0.03965718350869274,0.03613945421893982,0.03544022501078258,0.03624607460810147,0.03579670303978052,0.0351991136126297,0.03452606356584329,0.03594451677502226,0.036252562612090325,0.0366498986495934,0.035885958401710306,0.03506233612798743,0.035323869570116816,0.03436157405632468,0.03382266032480702,0.0352039636564009,0.03367804138060288,0.034031517519471574,0.03466811748765588,0.03354390452254269,0.033553917332203524,0.03350794599023472,0.03347399084479948,0.03339765700431624,0.033386417270935685,0.033404303843930006,0.0333926066653835],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv3.1_weight)\",\"line\":{\"color\":\"rgb(75, 175, 234)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv3.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.937564099871849,0.9511030448010096,0.9598622690435361,0.9177490261294584,0.908785667997727,0.9556856663245132,0.9664121209834826,0.9446482853166829,0.9694987536860067,0.9653728474162603,0.967959576953784,0.9529346011062456,0.9704550713331155,0.9634224630983511,0.9349744817666946,0.9631275970012317,0.9665826891724187,0.9723654315543115,0.9612072051364118,0.9650020590216062,0.9719120861203299,0.9733314377697686,0.9558526740734602,0.9745973854302397,0.9693122627657487,0.9751091731146129,0.9756351181367261,0.9510213019701326,0.9721569452754081,0.9741807027948878,0.9786147197750972,0.9747065235865819,0.9722428746187458,0.9767785140868271,0.9776443685111189,0.9776918298608854,0.9734133555006497,0.9699313846337076,0.9779502868891645,0.9769654795327815,0.9690846104891528,0.9739468223188352,0.9792640164392958,0.9771844347263015,0.9784998889695162,0.9770895766843588,0.9759295492848072,0.9692977269795726,0.9781568313141598,0.9794377050285109,0.9781912796966613,0.9772471037852566,0.9798267797558051,0.9779147633786084,0.9808656755083547,0.979571020898505,0.9804709823973632,0.9779402400313014,0.9785244681212386,0.9804375761960162,0.9770689476590808,0.9802780089754041,0.9801147109203585,0.982331143481537,0.9805751099203628,0.980372097726767,0.9818515678251607,0.9807808443231275,0.9807741191004752,0.980290029914187,0.9805828983270636,0.9803277153152212,0.9800925339133251,0.979868000652396,0.9803316404045904],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(48, 123, 139)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv4.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.14194820848083442,0.0834480060514101,0.0679423770420296,0.0647587810185357,0.06009983289112067,0.05908784904994096,0.04895992989972968,0.05240212207636609,0.04839132251676689,0.048342696689111946,0.04426961000816724,0.03916233417675121,0.03822334802155847,0.03870232454826469,0.040818669769874555,0.03878770720252057,0.03960109657020162,0.03640030186755619,0.0353893637422468,0.03170047593383035,0.03746247836959389,0.03688909329286214,0.03224813247191264,0.04058925495200277,0.034028990099042276,0.03365451597266181,0.030752692945549033,0.02876453946087454,0.033456394503923625,0.030454066655207064,0.033470566733340215,0.029637971453428405,0.02825072955975964,0.027985059596248234,0.031717216307969434,0.02827235926945458,0.026736489182630423,0.026272216013377748,0.026117288608867823,0.024798950894745354,0.024286768656279486,0.026632288911242256,0.02645548374049647,0.025328377743954647,0.025051977315582633,0.02486311041953774,0.02326200970545633,0.024633993578377048,0.023654300663172173,0.02293267855367368,0.022207350856523197,0.022895999981852128,0.02333449436925599,0.022367256066259444,0.021905397720643472,0.021589244730591366,0.02237292809320138,0.02089517021686146,0.021590955238383933,0.020620837531992387,0.020308224582684067,0.020100824217837973,0.02039378054166919,0.019966775726821415,0.019971424804622776,0.019787243008186864,0.019458977887481622,0.01958475599229711,0.01944509790986264,0.019322307226498785,0.019258540194323725,0.01925368402089245,0.01918943324867734,0.019156204621048317,0.01913270829533486],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(48, 123, 139)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8892501300066665,0.9429605106159722,0.9557243725633734,0.9554753310003384,0.9593289912234666,0.9604333948078215,0.9678865031585373,0.963646344329618,0.9670987527602635,0.9667301753972514,0.9720897683188161,0.9747838020094199,0.9777979892875851,0.9745655251568915,0.972248630659083,0.9748741889104842,0.9744063684963232,0.9775088891595445,0.9785258188076743,0.9827213460678669,0.9759898744115134,0.9768720678511749,0.9813011260290844,0.9727466266996703,0.9794811634135453,0.9810265781590636,0.9821361280330452,0.986339461213759,0.9788828245307837,0.983029808071681,0.9804239240520265,0.9836587056272391,0.984202281208292,0.9861073482310362,0.9808241731455425,0.9842371706958093,0.9865027370614057,0.9876142419628297,0.9867609009459615,0.9881148567340959,0.9889782777968009,0.9863096638873752,0.9871092437166444,0.987370465970021,0.9873533132498061,0.9880618616810718,0.9884045674249693,0.9874599563765497,0.9889990142795789,0.9899680645341874,0.9910084998912433,0.9894630379005273,0.989561605921583,0.9894245251283564,0.9898453701451251,0.9902427897987809,0.9898663124819814,0.9916115205540463,0.9899251046459311,0.9918899677633111,0.991689635376545,0.9919117681258398,0.9924562681938501,0.9925993322473061,0.9925465265773115,0.9921250985779055,0.9929578206187107,0.9926762907514094,0.992759765848255,0.9930317150111883,0.9928289807223842,0.9926330514192615,0.9930675684962117,0.9932157378775841,0.9931485397880496],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(48, 123, 139)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.09881569582879339,0.0818957923767493,0.07643414529430907,0.06349894158106899,0.0564294971935323,0.08232085153306883,0.057114930141115516,0.06538614254329622,0.05842351891945318,0.05693780056287333,0.044768219594795676,0.04868576884832988,0.05754644790875543,0.05217057908165086,0.05259399096836749,0.06458321032450372,0.055069818214554964,0.03922668715676491,0.03876549482499201,0.08036856603171817,0.03999362962762105,0.04454167073022869,0.04824300785500979,0.04299284373106006,0.03772067136524879,0.03820073686770557,0.037038256376469667,0.06689690887108701,0.04136587652246567,0.05178949879002325,0.03837370300006211,0.03373822532466187,0.03420587469570825,0.041320890270147945,0.051274442583117696,0.0347207150713275,0.033344283231447655,0.033619219377073635,0.033267798200505704,0.03539958093891439,0.037158013521805665,0.03275535962067519,0.06028674081787211,0.03557100707210626,0.04391131873700217,0.03174130926324739,0.03453341698165202,0.03550555003979772,0.03387876421827631,0.030903501414351445,0.036379810486667344,0.03246770517010869,0.0331125895412722,0.03520638687196876,0.033268311545508834,0.035581358629710895,0.032129865969272,0.03204458572764167,0.03090455448863023,0.031226327902672626,0.030734605613545452,0.033896337979540385,0.03070748577412871,0.030435461338443034,0.02998048467785632,0.029880262299399194,0.03011886373520717,0.03009499602301424,0.030121284339231315,0.03006935267872417,0.030259406267060446,0.029991064891475173,0.02996890151623598,0.02998336070694055,0.0299768406705758],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv4.1_weight)\",\"line\":{\"color\":\"rgb(48, 123, 139)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv4.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9336590204344489,0.9520221940954751,0.9547967969261804,0.9585082390136449,0.9660392960485359,0.941181035264483,0.9621067823910198,0.9584096835552123,0.9621834435315787,0.9663064442582352,0.9726444420723298,0.9693353724890693,0.963337820213718,0.967351522742508,0.9695294981362714,0.9516723995463139,0.9605738841468425,0.9778879906886528,0.9795972838022419,0.9430355957799427,0.979061730050812,0.9715311070652437,0.9671211411450655,0.9720377200813333,0.9794134544374821,0.9766164332164772,0.9788147898897763,0.9518095959790372,0.9776513378211117,0.9655932700512992,0.9772031529149103,0.9807865029253059,0.9812529859272509,0.9726265273725909,0.9658054388237598,0.9814501217292829,0.9792541705462758,0.9805644781397623,0.9821561471203085,0.9789878332339365,0.9791097711673263,0.9787233996862745,0.9537681004467096,0.979948913943097,0.9738473282993803,0.9799860087881541,0.9806975305087792,0.9807804959625087,0.9808315636579596,0.9822299145662856,0.9793276131510841,0.9818855787191997,0.9823219767117366,0.979805282601711,0.981691139272607,0.9812942173592476,0.9808341361764974,0.9817924763811942,0.9831939954063904,0.9830576348433778,0.9832288486074382,0.9818978470572964,0.9827369212254721,0.982705044289763,0.9824393244111582,0.9826979520140587,0.9842780795098807,0.9831630383563024,0.9833926279801845,0.9829032386843881,0.9831898769423398,0.9833893797771265,0.9831440234362027,0.9833893797771265,0.9833893797771265],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(2, 49, 117)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv5.1_weight)\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.12435028691779297,0.08379301323479667,0.07577567854117419,0.07066921150527322,0.06393583201443753,0.04836931182893008,0.05162403546828448,0.045465708709613153,0.04905350544454715,0.038537173841750295,0.0375115381803402,0.03948125688642912,0.035385931268726974,0.037086110805318116,0.03744555662161023,0.032695961406482184,0.03236231007970622,0.029581070886786177,0.031184111825720687,0.030527831320858904,0.0320308221378209,0.032838467676683254,0.030733001020919415,0.030520978363450586,0.03585372633169504,0.027185969660645362,0.02967047028601989,0.028697408732765047,0.026390539807825426,0.030844837912749862,0.02773878955690858,0.024490031654618997,0.02930993295412293,0.025521905439450297,0.026283440525554983,0.026565739770397494,0.024942975277070463,0.023102106552950842,0.02590956061839511,0.027257613942692778,0.024635647447675227,0.02521690316092893,0.022173360943418735,0.022306732329425805,0.021761651623051055,0.02345196861637178,0.022515399142766943,0.02220626249888065,0.021967790138407398,0.021456163823775274,0.021752819129162364,0.023545109513050364,0.020657491535673776,0.020860379043313646,0.020714930433220948,0.020427936515456144,0.021871688990146434,0.021129552060940898,0.02101908803395799,0.019773093065650192,0.020085959558941654,0.019647994349878132,0.01972807132034111,0.01932574328385029,0.01938836055360777,0.019234583677696267,0.019146515736260365,0.01917659066604687,0.019102105408488747,0.019018535461502933,0.018912617123994085,0.018838243129292987,0.01882224531304454,0.018812184059994567,0.018782946417686387],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(2, 49, 117)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.8984190669433233,0.9388556657000746,0.9444360546897094,0.9502249612375451,0.9548738596089692,0.9696813925178199,0.9661450841538668,0.9706428117364991,0.9677735375913459,0.9753840639486449,0.9789385521838679,0.9755324532812306,0.9807171003177132,0.9789170995002471,0.9758942443924205,0.9817729278109443,0.9806958917661942,0.9847396292433421,0.9830290924815396,0.9841933828949154,0.9798968423542866,0.9812084340572185,0.9829683632583164,0.9831033290239203,0.9776282543106344,0.9867460631814297,0.9831670098395305,0.9849039991825138,0.9870206005362673,0.9830953929528506,0.9853543991221871,0.9894787390049259,0.983427262181323,0.9874269701312405,0.987632207189451,0.9864837378984493,0.9880831039030935,0.9899909926054173,0.9871483936692774,0.9852468303568428,0.9880620921263404,0.9882135183139807,0.990305023691135,0.9913192009134385,0.9905640568964714,0.9897815343507543,0.9890580658860026,0.9909850168446999,0.9906141163456395,0.9903094757713686,0.9899858011174801,0.9886068901071434,0.9920173448571622,0.9926364250387875,0.9917615530257443,0.9920048082674339,0.9902834259131941,0.9912522574114065,0.9907905856202164,0.9918816040526381,0.9924575445431945,0.9929521535097453,0.9925913147920815,0.9930616614350386,0.9926030468891133,0.9930842473610999,0.9930165815607527,0.9930425144212638,0.9929793499580563,0.9932795178450179,0.9932327837367992,0.9932613015579213,0.9934567054837025,0.9932183757992801,0.9933094615538998],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(2, 49, 117)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.11374917000122496,0.0835949994137197,0.07441136624497646,0.08751719748646124,0.064908729713807,0.10452652151641977,0.09072517232898994,0.05021171794365772,0.04976535007306391,0.041332232399085136,0.0400316910056519,0.04375265133647165,0.04323638405433226,0.06888270323833649,0.038727039362454335,0.03811927355595471,0.04330002026971673,0.03515438263102905,0.040814507709131205,0.03588254643520948,0.04000126825984811,0.034374113009660104,0.035027608610817655,0.04514782207784374,0.03762955425940838,0.03316181932169547,0.03347106451533504,0.03557142614806231,0.06966571975307366,0.035873879492282865,0.03322577377416424,0.03059814020069604,0.03211958326936997,0.03644096451168208,0.032060033541262356,0.030587366138844146,0.03127114363398749,0.0318401456058435,0.06857750383541757,0.034173014939240985,0.030218195357068708,0.03048896728870795,0.03138158162639723,0.030870400607278668,0.030375643020745406,0.03623851121905743,0.030658984299480302,0.031204772023708142,0.031796687951509896,0.03139954156584756,0.03313781359965859,0.029076346606528226,0.029469888047971265,0.031112885710709692,0.029776096984078383,0.02985151707357967,0.0289751376278212,0.029637298082670394,0.029713091497466326,0.02913933243743333,0.030103067518611953,0.029902966620074106,0.02895904868277897,0.02943203378583967,0.02881271987129323,0.02852117513975327,0.02862507011933425,0.028616336889283354,0.02854715543463058,0.02856897298771491,0.028616649744539327,0.02855763310419325,0.028514392991451053,0.028518747433354356,0.02852009507184176],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"legendgroup\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv5.1_weight)\",\"line\":{\"color\":\"rgb(2, 49, 117)\"},\"mode\":\"lines\",\"name\":\"deep_rescnn(pretrain=True,weight_decay=0.01,learning_rate=0.0023290654300914477,batch_size=128, start_layer = conv5.1_weight)\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74],\"y\":[0.9112749606843962,0.9351881988118688,0.946411571365022,0.9345118173791513,0.9596366972063508,0.9001534503384454,0.9268753292838385,0.9744914892600217,0.9752818078687443,0.9773442969194356,0.9798217189902712,0.9776468199984303,0.979052525375691,0.9401701560197391,0.9765108015464545,0.9796410263742223,0.9704390257696689,0.9816581651956815,0.9724604757970643,0.9782766988790845,0.9784015070248834,0.9829296540192664,0.9817378079508404,0.9686352841442564,0.9830631014848807,0.9853080851407817,0.9809073841927272,0.9782350376754276,0.9400618172909178,0.9759557626931419,0.9819606622168451,0.9850735188021134,0.9824614412902445,0.9752318533326387,0.9811513610563305,0.9832039796438264,0.9836683039352775,0.9792008583701998,0.9523470037868867,0.9813835486314174,0.9860002375374498,0.9828544609364122,0.9849019347333958,0.9846231655989944,0.9838289634200181,0.9741455819673448,0.9835413863399745,0.9806195248885348,0.9863491967956257,0.9860860883199876,0.9837637794334584,0.9835745962056468,0.9855531925277312,0.9861202968281326,0.9843116685406532,0.9825714062801469,0.9874535278566593,0.9853028850534801,0.9867848567475024,0.9852439360483604,0.9872521780724381,0.9874734184320075,0.9874319455087202,0.9862962234928612,0.9845222778372005,0.9867119141833844,0.9857417033930929,0.986947898638531,0.9864631317912317,0.9867029610479991,0.9864596936806729,0.9855013078954202,0.985983233311057,0.9862237092255263,0.9862237092255263],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0],\"title\":{\"text\":\"Train Loss\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.625,1.0],\"title\":{\"text\":\"Train F1\"}},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Epoch\"}},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375],\"title\":{\"text\":\"Val Loss\"}},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,0.375],\"title\":{\"text\":\"Val F1\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Train Loss\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Train F1\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Val Loss\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Val F1\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Metrics for Different Models for experiment 3 (40 models)\"},\"height\":800,\"width\":1600,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('21c47c13-ce36-4dc1-b254-4d9c6de263a1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2623
        },
        "id": "Fr8ehsGOXI5Z",
        "outputId": "7ec10bc5-a645-45d7-8e7b-90e462769c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       model_name                                     hyperparameter  \\\n",
              "0   zero_baseline                                            seed=42   \n",
              "1             gru  {'weight_decay': 0.01, 'learning_rate': 0.0007...   \n",
              "2             gru  {'weight_decay': 0.01, 'learning_rate': 0.0005...   \n",
              "3             gru  {'weight_decay': 0.01, 'learning_rate': 0.0001...   \n",
              "4             gru  {'weight_decay': 0.01, 'learning_rate': 0.001,...   \n",
              "..            ...                                                ...   \n",
              "91    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              "92    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              "93    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              "94    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              "95    deep_rescnn  {'weight_decay': 0.01, 'learning_rate': 0.0023...   \n",
              "\n",
              "    best_epoch best_train_cost best_val_cost  best_train_recall  \\\n",
              "0            1            None          None           1.000000   \n",
              "1            1        0.237551      0.228324           1.000000   \n",
              "2            1        0.247199      0.235428           1.000000   \n",
              "3            2        0.268154      0.262874           0.384359   \n",
              "4            1        0.237496        0.2285           1.000000   \n",
              "..         ...             ...           ...                ...   \n",
              "91          71        0.019795      0.029031           0.993196   \n",
              "92          68        0.043721      0.051469           0.979579   \n",
              "93          64        0.026478      0.035204           0.987589   \n",
              "94          67        0.019459      0.030119           0.994543   \n",
              "95          62        0.019648      0.029903           0.994247   \n",
              "\n",
              "    best_train_precision  best_train_f1  best_val_recall  best_val_precision  \\\n",
              "0               0.721993       0.838555         1.000000            0.721993   \n",
              "1               0.721993       0.837994         1.000000            0.721993   \n",
              "2               0.721993       0.837963         1.000000            0.721993   \n",
              "3               0.293013       0.322972         1.000000            0.721993   \n",
              "4               0.721993       0.837926         1.000000            0.721993   \n",
              "..                   ...            ...              ...                 ...   \n",
              "91              0.990810       0.991912         0.991923            0.982528   \n",
              "92              0.966299       0.972732         0.982887            0.957762   \n",
              "93              0.983912       0.985649         0.991955            0.972927   \n",
              "94              0.991456       0.992958         0.982439            0.986138   \n",
              "95              0.991731       0.992952         0.995702            0.979381   \n",
              "\n",
              "    best_val_f1 best_test_recall best_test_precision best_test_f1  \n",
              "0      0.838555              1.0            0.721841     0.838452  \n",
              "1      0.838520             None                None         None  \n",
              "2      0.838520             None                None         None  \n",
              "3      0.838520             None                None         None  \n",
              "4      0.838520             None                None         None  \n",
              "..          ...              ...                 ...          ...  \n",
              "91     0.987198             None                None         None  \n",
              "92     0.970154             None                None         None  \n",
              "93     0.982331             None                None         None  \n",
              "94     0.984278             None                None         None  \n",
              "95     0.987473             None                None         None  \n",
              "\n",
              "[96 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd68d697-aeb2-4261-b744-e5885d8a8687\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>hyperparameter</th>\n",
              "      <th>best_epoch</th>\n",
              "      <th>best_train_cost</th>\n",
              "      <th>best_val_cost</th>\n",
              "      <th>best_train_recall</th>\n",
              "      <th>best_train_precision</th>\n",
              "      <th>best_train_f1</th>\n",
              "      <th>best_val_recall</th>\n",
              "      <th>best_val_precision</th>\n",
              "      <th>best_val_f1</th>\n",
              "      <th>best_test_recall</th>\n",
              "      <th>best_test_precision</th>\n",
              "      <th>best_test_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_baseline</td>\n",
              "      <td>seed=42</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838555</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.721841</td>\n",
              "      <td>0.838452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0007...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237551</td>\n",
              "      <td>0.228324</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837994</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0005...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.247199</td>\n",
              "      <td>0.235428</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837963</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0001...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.268154</td>\n",
              "      <td>0.262874</td>\n",
              "      <td>0.384359</td>\n",
              "      <td>0.293013</td>\n",
              "      <td>0.322972</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gru</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.001,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237496</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.837926</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721993</td>\n",
              "      <td>0.838520</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0023...</td>\n",
              "      <td>71</td>\n",
              "      <td>0.019795</td>\n",
              "      <td>0.029031</td>\n",
              "      <td>0.993196</td>\n",
              "      <td>0.990810</td>\n",
              "      <td>0.991912</td>\n",
              "      <td>0.991923</td>\n",
              "      <td>0.982528</td>\n",
              "      <td>0.987198</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0023...</td>\n",
              "      <td>68</td>\n",
              "      <td>0.043721</td>\n",
              "      <td>0.051469</td>\n",
              "      <td>0.979579</td>\n",
              "      <td>0.966299</td>\n",
              "      <td>0.972732</td>\n",
              "      <td>0.982887</td>\n",
              "      <td>0.957762</td>\n",
              "      <td>0.970154</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0023...</td>\n",
              "      <td>64</td>\n",
              "      <td>0.026478</td>\n",
              "      <td>0.035204</td>\n",
              "      <td>0.987589</td>\n",
              "      <td>0.983912</td>\n",
              "      <td>0.985649</td>\n",
              "      <td>0.991955</td>\n",
              "      <td>0.972927</td>\n",
              "      <td>0.982331</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0023...</td>\n",
              "      <td>67</td>\n",
              "      <td>0.019459</td>\n",
              "      <td>0.030119</td>\n",
              "      <td>0.994543</td>\n",
              "      <td>0.991456</td>\n",
              "      <td>0.992958</td>\n",
              "      <td>0.982439</td>\n",
              "      <td>0.986138</td>\n",
              "      <td>0.984278</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>deep_rescnn</td>\n",
              "      <td>{'weight_decay': 0.01, 'learning_rate': 0.0023...</td>\n",
              "      <td>62</td>\n",
              "      <td>0.019648</td>\n",
              "      <td>0.029903</td>\n",
              "      <td>0.994247</td>\n",
              "      <td>0.991731</td>\n",
              "      <td>0.992952</td>\n",
              "      <td>0.995702</td>\n",
              "      <td>0.979381</td>\n",
              "      <td>0.987473</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>96 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd68d697-aeb2-4261-b744-e5885d8a8687')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd68d697-aeb2-4261-b744-e5885d8a8687 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd68d697-aeb2-4261-b744-e5885d8a8687');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/99dac6621f6ae8c4/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_baseline\",\n\"seed=42\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385551786070644,\n            'f': \"0.8385551786070644\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385551786070644,\n            'f': \"0.8385551786070644\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7218406593406593,\n            'f': \"0.7218406593406593\",\n        },\n{\n            'v': 0.8384523334662944,\n            'f': \"0.8384523334662944\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23755095152852188,\n            'f': \"0.23755095152852188\",\n        },\n{\n            'v': 0.2283238747890053,\n            'f': \"0.2283238747890053\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379937470948515,\n            'f': \"0.8379937470948515\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0005, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24719946554777686,\n            'f': \"0.24719946554777686\",\n        },\n{\n            'v': 0.235427502913983,\n            'f': \"0.235427502913983\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379629078268853,\n            'f': \"0.8379629078268853\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0.26815437865694225,\n            'f': \"0.26815437865694225\",\n        },\n{\n            'v': 0.2628738533180604,\n            'f': \"0.2628738533180604\",\n        },\n{\n            'v': 0.38435905446214724,\n            'f': \"0.38435905446214724\",\n        },\n{\n            'v': 0.2930126002290951,\n            'f': \"0.2930126002290951\",\n        },\n{\n            'v': 0.3229722700122437,\n            'f': \"0.3229722700122437\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23749594646426,\n            'f': \"0.23749594646426\",\n        },\n{\n            'v': 0.22849985904914816,\n            'f': \"0.22849985904914816\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379264789299304,\n            'f': \"0.8379264789299304\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23769533075008195,\n            'f': \"0.23769533075008195\",\n        },\n{\n            'v': 0.22830794468982932,\n            'f': \"0.22830794468982932\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6705612829324169,\n            'f': \"0.6705612829324169\",\n        },\n{\n            'v': 0.777512641968225,\n            'f': \"0.777512641968225\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 6,\n            'f': \"6\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23270269353092057,\n            'f': \"0.23270269353092057\",\n        },\n{\n            'v': 0.2275912654051666,\n            'f': \"0.2275912654051666\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.7120274914089347,\n            'f': \"0.7120274914089347\",\n        },\n{\n            'v': 0.8261833872750113,\n            'f': \"0.8261833872750113\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 7,\n            'f': \"7\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23086243700940176,\n            'f': \"0.23086243700940176\",\n        },\n{\n            'v': 0.22798991973457466,\n            'f': \"0.22798991973457466\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.711340206185567,\n            'f': \"0.711340206185567\",\n        },\n{\n            'v': 0.8256718950601964,\n            'f': \"0.8256718950601964\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 8,\n            'f': \"8\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24596911503550387,\n            'f': \"0.24596911503550387\",\n        },\n{\n            'v': 0.22848816736252447,\n            'f': \"0.22848816736252447\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5950744558991982,\n            'f': \"0.5950744558991982\",\n        },\n{\n            'v': 0.6906552425563023,\n            'f': \"0.6906552425563023\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 9,\n            'f': \"9\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23872165301684525,\n            'f': \"0.23872165301684525\",\n        },\n{\n            'v': 0.22900927653632214,\n            'f': \"0.22900927653632214\",\n        },\n{\n            'v': 0.8680412371134021,\n            'f': \"0.8680412371134021\",\n        },\n{\n            'v': 0.6282932416953035,\n            'f': \"0.6282932416953035\",\n        },\n{\n            'v': 0.7283860077496157,\n            'f': \"0.7283860077496157\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 10,\n            'f': \"10\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23140935585124658,\n            'f': \"0.23140935585124658\",\n        },\n{\n            'v': 0.2278759803661366,\n            'f': \"0.2278759803661366\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378469602887191,\n            'f': \"0.8378469602887191\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 11,\n            'f': \"11\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23049640620696996,\n            'f': \"0.23049640620696996\",\n        },\n{\n            'v': 0.2280344991647091,\n            'f': \"0.2280344991647091\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377898005373192,\n            'f': \"0.8377898005373192\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 12,\n            'f': \"12\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2582674642540472,\n            'f': \"0.2582674642540472\",\n        },\n{\n            'v': 0.2318031070261067,\n            'f': \"0.2318031070261067\",\n        },\n{\n            'v': 0.677434135166094,\n            'f': \"0.677434135166094\",\n        },\n{\n            'v': 0.4879725085910653,\n            'f': \"0.4879725085910653\",\n        },\n{\n            'v': 0.5667718616335917,\n            'f': \"0.5667718616335917\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 13,\n            'f': \"13\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23063589629438735,\n            'f': \"0.23063589629438735\",\n        },\n{\n            'v': 0.22779080836019155,\n            'f': \"0.22779080836019155\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379650916960658,\n            'f': \"0.8379650916960658\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 14,\n            'f': \"14\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24043467447656672,\n            'f': \"0.24043467447656672\",\n        },\n{\n            'v': 0.22798836162614658,\n            'f': \"0.22798836162614658\",\n        },\n{\n            'v': 0.9120274914089347,\n            'f': \"0.9120274914089347\",\n        },\n{\n            'v': 0.6577319587628866,\n            'f': \"0.6577319587628866\",\n        },\n{\n            'v': 0.7636797948350783,\n            'f': \"0.7636797948350783\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 15,\n            'f': \"15\",\n        },\n\"gru\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.22989786069281185,\n            'f': \"0.22989786069281185\",\n        },\n{\n            'v': 0.22800785562221948,\n            'f': \"0.22800785562221948\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379435108647284,\n            'f': \"0.8379435108647284\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 16,\n            'f': \"16\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23187371032549345,\n            'f': \"0.23187371032549345\",\n        },\n{\n            'v': 0.22782516483588727,\n            'f': \"0.22782516483588727\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378981891847632,\n            'f': \"0.8378981891847632\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 17,\n            'f': \"17\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23513470497259706,\n            'f': \"0.23513470497259706\",\n        },\n{\n            'v': 0.22760502922371081,\n            'f': \"0.22760502922371081\",\n        },\n{\n            'v': 0.9120274914089347,\n            'f': \"0.9120274914089347\",\n        },\n{\n            'v': 0.656930126002291,\n            'f': \"0.656930126002291\",\n        },\n{\n            'v': 0.7633690624041659,\n            'f': \"0.7633690624041659\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 18,\n            'f': \"18\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23658089066630514,\n            'f': \"0.23658089066630514\",\n        },\n{\n            'v': 0.22805524370104996,\n            'f': \"0.22805524370104996\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6675830469644902,\n            'f': \"0.6675830469644902\",\n        },\n{\n            'v': 0.775601528856061,\n            'f': \"0.775601528856061\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 19,\n            'f': \"19\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23012308565406864,\n            'f': \"0.23012308565406864\",\n        },\n{\n            'v': 0.2281344659754501,\n            'f': \"0.2281344659754501\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8380585970171258,\n            'f': \"0.8380585970171258\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 20,\n            'f': \"20\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24781396114539445,\n            'f': \"0.24781396114539445\",\n        },\n{\n            'v': 0.22780490974380388,\n            'f': \"0.22780490974380388\",\n        },\n{\n            'v': 0.7802280990089148,\n            'f': \"0.7802280990089148\",\n        },\n{\n            'v': 0.5706758304696449,\n            'f': \"0.5706758304696449\",\n        },\n{\n            'v': 0.6541961675540556,\n            'f': \"0.6541961675540556\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 21,\n            'f': \"21\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2344763796422353,\n            'f': \"0.2344763796422353\",\n        },\n{\n            'v': 0.22773042042640476,\n            'f': \"0.22773042042640476\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6696449026345933,\n            'f': \"0.6696449026345933\",\n        },\n{\n            'v': 0.7769669688757592,\n            'f': \"0.7769669688757592\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 22,\n            'f': \"22\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23270658465253802,\n            'f': \"0.23270658465253802\",\n        },\n{\n            'v': 0.22759577791715405,\n            'f': \"0.22759577791715405\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379593694587404,\n            'f': \"0.8379593694587404\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 23,\n            'f': \"23\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23113079530540065,\n            'f': \"0.23113079530540065\",\n        },\n{\n            'v': 0.22782862943267496,\n            'f': \"0.22782862943267496\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7010309278350515,\n            'f': \"0.7010309278350515\",\n        },\n{\n            'v': 0.8134646001971478,\n            'f': \"0.8134646001971478\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 24,\n            'f': \"24\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24239094583370976,\n            'f': \"0.24239094583370976\",\n        },\n{\n            'v': 0.22818102553947683,\n            'f': \"0.22818102553947683\",\n        },\n{\n            'v': 0.8680412371134021,\n            'f': \"0.8680412371134021\",\n        },\n{\n            'v': 0.6262313860252005,\n            'f': \"0.6262313860252005\",\n        },\n{\n            'v': 0.7268249415921961,\n            'f': \"0.7268249415921961\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 25,\n            'f': \"25\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2518623998826039,\n            'f': \"0.2518623998826039\",\n        },\n{\n            'v': 0.22761076536170397,\n            'f': \"0.22761076536170397\",\n        },\n{\n            'v': 0.7360824742268042,\n            'f': \"0.7360824742268042\",\n        },\n{\n            'v': 0.5312714776632302,\n            'f': \"0.5312714776632302\",\n        },\n{\n            'v': 0.6165380489730845,\n            'f': \"0.6165380489730845\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 26,\n            'f': \"26\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.232329122092306,\n            'f': \"0.232329122092306\",\n        },\n{\n            'v': 0.2277810290507025,\n            'f': \"0.2277810290507025\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8381023417873185,\n            'f': \"0.8381023417873185\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 27,\n            'f': \"27\",\n        },\n\"gru\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23091984057003143,\n            'f': \"0.23091984057003143\",\n        },\n{\n            'v': 0.22761146157877551,\n            'f': \"0.22761146157877551\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.711340206185567,\n            'f': \"0.711340206185567\",\n        },\n{\n            'v': 0.8256305016925937,\n            'f': \"0.8256305016925937\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 28,\n            'f': \"28\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24659692590452414,\n            'f': \"0.24659692590452414\",\n        },\n{\n            'v': 0.2301524313557189,\n            'f': \"0.2301524313557189\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.6688430698739977,\n            'f': \"0.6688430698739977\",\n        },\n{\n            'v': 0.7764289639170969,\n            'f': \"0.7764289639170969\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 29,\n            'f': \"29\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24808622859802443,\n            'f': \"0.24808622859802443\",\n        },\n{\n            'v': 0.22897008621610726,\n            'f': \"0.22897008621610726\",\n        },\n{\n            'v': 0.8093928980526919,\n            'f': \"0.8093928980526919\",\n        },\n{\n            'v': 0.5835051546391753,\n            'f': \"0.5835051546391753\",\n        },\n{\n            'v': 0.6777182478215532,\n            'f': \"0.6777182478215532\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 30,\n            'f': \"30\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24036915460676125,\n            'f': \"0.24036915460676125\",\n        },\n{\n            'v': 0.22881030144355552,\n            'f': \"0.22881030144355552\",\n        },\n{\n            'v': 0.9413516609392898,\n            'f': \"0.9413516609392898\",\n        },\n{\n            'v': 0.6799541809851088,\n            'f': \"0.6799541809851088\",\n        },\n{\n            'v': 0.7890973790044737,\n            'f': \"0.7890973790044737\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 31,\n            'f': \"31\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.233591164599033,\n            'f': \"0.233591164599033\",\n        },\n{\n            'v': 0.2288613215549705,\n            'f': \"0.2288613215549705\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379850303243254,\n            'f': \"0.8379850303243254\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 32,\n            'f': \"32\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24404066760105775,\n            'f': \"0.24404066760105775\",\n        },\n{\n            'v': 0.22964662380030065,\n            'f': \"0.22964662380030065\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.7116838487972509,\n            'f': \"0.7116838487972509\",\n        },\n{\n            'v': 0.8257649556000859,\n            'f': \"0.8257649556000859\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 33,\n            'f': \"33\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24605310336011382,\n            'f': \"0.24605310336011382\",\n        },\n{\n            'v': 0.22872167988536285,\n            'f': \"0.22872167988536285\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5956471935853379,\n            'f': \"0.5956471935853379\",\n        },\n{\n            'v': 0.6910118096782196,\n            'f': \"0.6910118096782196\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 34,\n            'f': \"34\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2334126355790054,\n            'f': \"0.2334126355790054\",\n        },\n{\n            'v': 0.2294187471321768,\n            'f': \"0.2294187471321768\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8379098005498145,\n            'f': \"0.8379098005498145\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 35,\n            'f': \"35\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23930862564742905,\n            'f': \"0.23930862564742905\",\n        },\n{\n            'v': 0.22784182982346446,\n            'f': \"0.22784182982346446\",\n        },\n{\n            'v': 0.8973654066437572,\n            'f': \"0.8973654066437572\",\n        },\n{\n            'v': 0.6483390607101948,\n            'f': \"0.6483390607101948\",\n        },\n{\n            'v': 0.7523462888410456,\n            'f': \"0.7523462888410456\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 36,\n            'f': \"36\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24829804776460446,\n            'f': \"0.24829804776460446\",\n        },\n{\n            'v': 0.23096789029455678,\n            'f': \"0.23096789029455678\",\n        },\n{\n            'v': 0.9120274914089347,\n            'f': \"0.9120274914089347\",\n        },\n{\n            'v': 0.6575028636884307,\n            'f': \"0.6575028636884307\",\n        },\n{\n            'v': 0.7637263087855324,\n            'f': \"0.7637263087855324\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 37,\n            'f': \"37\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23854704901046905,\n            'f': \"0.23854704901046905\",\n        },\n{\n            'v': 0.2289582008860775,\n            'f': \"0.2289582008860775\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.837937023496059,\n            'f': \"0.837937023496059\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 38,\n            'f': \"38\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24648009746208496,\n            'f': \"0.24648009746208496\",\n        },\n{\n            'v': 0.22909368780470385,\n            'f': \"0.22909368780470385\",\n        },\n{\n            'v': 0.8973654066437572,\n            'f': \"0.8973654066437572\",\n        },\n{\n            'v': 0.6467353951890035,\n            'f': \"0.6467353951890035\",\n        },\n{\n            'v': 0.7511256735881737,\n            'f': \"0.7511256735881737\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 39,\n            'f': \"39\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24160942262868565,\n            'f': \"0.24160942262868565\",\n        },\n{\n            'v': 0.23032080584375308,\n            'f': \"0.23032080584375308\",\n        },\n{\n            'v': 0.9560137457044674,\n            'f': \"0.9560137457044674\",\n        },\n{\n            'v': 0.6904925544100802,\n            'f': \"0.6904925544100802\",\n        },\n{\n            'v': 0.8012523489553404,\n            'f': \"0.8012523489553404\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 40,\n            'f': \"40\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23757384251483118,\n            'f': \"0.23757384251483118\",\n        },\n{\n            'v': 0.2279223526159103,\n            'f': \"0.2279223526159103\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7012600229095074,\n            'f': \"0.7012600229095074\",\n        },\n{\n            'v': 0.8137792079514921,\n            'f': \"0.8137792079514921\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 41,\n            'f': \"41\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23180127218621702,\n            'f': \"0.23180127218621702\",\n        },\n{\n            'v': 0.22847400504289214,\n            'f': \"0.22847400504289214\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378703062107288,\n            'f': \"0.8378703062107288\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 42,\n            'f': \"42\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23798671251788037,\n            'f': \"0.23798671251788037\",\n        },\n{\n            'v': 0.22795328791813343,\n            'f': \"0.22795328791813343\",\n        },\n{\n            'v': 0.9266895761741123,\n            'f': \"0.9266895761741123\",\n        },\n{\n            'v': 0.66815578465063,\n            'f': \"0.66815578465063\",\n        },\n{\n            'v': 0.7758350982211567,\n            'f': \"0.7758350982211567\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 43,\n            'f': \"43\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.001, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23300043067981288,\n            'f': \"0.23300043067981288\",\n        },\n{\n            'v': 0.22760143634379934,\n            'f': \"0.22760143634379934\",\n        },\n{\n            'v': 0.9853379152348225,\n            'f': \"0.9853379152348225\",\n        },\n{\n            'v': 0.7112256586483391,\n            'f': \"0.7112256586483391\",\n        },\n{\n            'v': 0.8255556394557227,\n            'f': \"0.8255556394557227\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 44,\n            'f': \"44\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23394868339050268,\n            'f': \"0.23394868339050268\",\n        },\n{\n            'v': 0.22764059617552151,\n            'f': \"0.22764059617552151\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8380657319957353,\n            'f': \"0.8380657319957353\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 45,\n            'f': \"45\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.24325722402723385,\n            'f': \"0.24325722402723385\",\n        },\n{\n            'v': 0.22764322254870764,\n            'f': \"0.22764322254870764\",\n        },\n{\n            'v': 0.8240549828178694,\n            'f': \"0.8240549828178694\",\n        },\n{\n            'v': 0.5942726231386025,\n            'f': \"0.5942726231386025\",\n        },\n{\n            'v': 0.6899781748662641,\n            'f': \"0.6899781748662641\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 46,\n            'f': \"46\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23448481141223668,\n            'f': \"0.23448481141223668\",\n        },\n{\n            'v': 0.22770088012890308,\n            'f': \"0.22770088012890308\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7005727376861397,\n            'f': \"0.7005727376861397\",\n        },\n{\n            'v': 0.8132708898924303,\n            'f': \"0.8132708898924303\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 47,\n            'f': \"47\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0009, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2330947979892106,\n            'f': \"0.2330947979892106\",\n        },\n{\n            'v': 0.22759252305293,\n            'f': \"0.22759252305293\",\n        },\n{\n            'v': 0.9706758304696449,\n            'f': \"0.9706758304696449\",\n        },\n{\n            'v': 0.7009163802978235,\n            'f': \"0.7009163802978235\",\n        },\n{\n            'v': 0.8135473343847958,\n            'f': \"0.8135473343847958\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 48,\n            'f': \"48\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23362354004983912,\n            'f': \"0.23362354004983912\",\n        },\n{\n            'v': 0.22762263073134667,\n            'f': \"0.22762263073134667\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377483941926969,\n            'f': \"0.8377483941926969\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 49,\n            'f': \"49\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 32, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.2353885561330212,\n            'f': \"0.2353885561330212\",\n        },\n{\n            'v': 0.22773463401392974,\n            'f': \"0.22773463401392974\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8377726111607469,\n            'f': \"0.8377726111607469\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 50,\n            'f': \"50\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 1}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23610532127090336,\n            'f': \"0.23610532127090336\",\n        },\n{\n            'v': 0.22759167250898696,\n            'f': \"0.22759167250898696\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8378901998288699,\n            'f': \"0.8378901998288699\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 51,\n            'f': \"51\",\n        },\n\"lstm\",\n\"{'weight_decay': 0.001, 'learning_rate': 0.0007, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 2}\",\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.23536989676925052,\n            'f': \"0.23536989676925052\",\n        },\n{\n            'v': 0.22773557217260407,\n            'f': \"0.22773557217260407\",\n        },\n{\n            'v': 0.9413516609392898,\n            'f': \"0.9413516609392898\",\n        },\n{\n            'v': 0.6781214203894617,\n            'f': \"0.6781214203894617\",\n        },\n{\n            'v': 0.7877237131052639,\n            'f': \"0.7877237131052639\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.7219931271477663,\n            'f': \"0.7219931271477663\",\n        },\n{\n            'v': 0.8385199187778661,\n            'f': \"0.8385199187778661\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 52,\n            'f': \"52\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128}\",\n{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': 0.022594121824800625,\n            'f': \"0.022594121824800625\",\n        },\n{\n            'v': 0.0324072357766407,\n            'f': \"0.0324072357766407\",\n        },\n{\n            'v': 0.9930491495835343,\n            'f': \"0.9930491495835343\",\n        },\n{\n            'v': 0.9888522839808216,\n            'f': \"0.9888522839808216\",\n        },\n{\n            'v': 0.9908897871389477,\n            'f': \"0.9908897871389477\",\n        },\n{\n            'v': 0.9876317632765976,\n            'f': \"0.9876317632765976\",\n        },\n{\n            'v': 0.9820149289352244,\n            'f': \"0.9820149289352244\",\n        },\n{\n            'v': 0.9848132951156789,\n            'f': \"0.9848132951156789\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 53,\n            'f': \"53\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001, 'batch_size': 128}\",\n{\n            'v': 48,\n            'f': \"48\",\n        },\n{\n            'v': 0.0508416385021816,\n            'f': \"0.0508416385021816\",\n        },\n{\n            'v': 0.0645379028369471,\n            'f': \"0.0645379028369471\",\n        },\n{\n            'v': 0.9771973487937176,\n            'f': \"0.9771973487937176\",\n        },\n{\n            'v': 0.9632434459063973,\n            'f': \"0.9632434459063973\",\n        },\n{\n            'v': 0.969978386968445,\n            'f': \"0.969978386968445\",\n        },\n{\n            'v': 0.9772559884770459,\n            'f': \"0.9772559884770459\",\n        },\n{\n            'v': 0.9513458385652597,\n            'f': \"0.9513458385652597\",\n        },\n{\n            'v': 0.9641136834435939,\n            'f': \"0.9641136834435939\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 54,\n            'f': \"54\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128}\",\n{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': 0.02343625199211866,\n            'f': \"0.02343625199211866\",\n        },\n{\n            'v': 0.033765379019414433,\n            'f': \"0.033765379019414433\",\n        },\n{\n            'v': 0.9927152671716448,\n            'f': \"0.9927152671716448\",\n        },\n{\n            'v': 0.9894789600382609,\n            'f': \"0.9894789600382609\",\n        },\n{\n            'v': 0.991033007737149,\n            'f': \"0.991033007737149\",\n        },\n{\n            'v': 0.9895085101265405,\n            'f': \"0.9895085101265405\",\n        },\n{\n            'v': 0.9792537562341926,\n            'f': \"0.9792537562341926\",\n        },\n{\n            'v': 0.9843540435633988,\n            'f': \"0.9843540435633988\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 55,\n            'f': \"55\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0007, 'batch_size': 128}\",\n{\n            'v': 48,\n            'f': \"48\",\n        },\n{\n            'v': 0.041094861244452094,\n            'f': \"0.041094861244452094\",\n        },\n{\n            'v': 0.05397045065447227,\n            'f': \"0.05397045065447227\",\n        },\n{\n            'v': 0.9844684149339925,\n            'f': \"0.9844684149339925\",\n        },\n{\n            'v': 0.9735874425621157,\n            'f': \"0.9735874425621157\",\n        },\n{\n            'v': 0.9789105681946668,\n            'f': \"0.9789105681946668\",\n        },\n{\n            'v': 0.9800461991949599,\n            'f': \"0.9800461991949599\",\n        },\n{\n            'v': 0.9585436117727595,\n            'f': \"0.9585436117727595\",\n        },\n{\n            'v': 0.9691670516937745,\n            'f': \"0.9691670516937745\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 56,\n            'f': \"56\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 64, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 67,\n            'f': \"67\",\n        },\n{\n            'v': 0.031972744692658914,\n            'f': \"0.031972744692658914\",\n        },\n{\n            'v': 0.04161732973102032,\n            'f': \"0.04161732973102032\",\n        },\n{\n            'v': 0.9851698975388765,\n            'f': \"0.9851698975388765\",\n        },\n{\n            'v': 0.9807703180133152,\n            'f': \"0.9807703180133152\",\n        },\n{\n            'v': 0.9827931725537998,\n            'f': \"0.9827931725537998\",\n        },\n{\n            'v': 0.9881414845050731,\n            'f': \"0.9881414845050731\",\n        },\n{\n            'v': 0.973273745558192,\n            'f': \"0.973273745558192\",\n        },\n{\n            'v': 0.9806475821847561,\n            'f': \"0.9806475821847561\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 57,\n            'f': \"57\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 64, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 73,\n            'f': \"73\",\n        },\n{\n            'v': 0.022634717391511554,\n            'f': \"0.022634717391511554\",\n        },\n{\n            'v': 0.030971059518069336,\n            'f': \"0.030971059518069336\",\n        },\n{\n            'v': 0.9919140821295199,\n            'f': \"0.9919140821295199\",\n        },\n{\n            'v': 0.9899790651755933,\n            'f': \"0.9899790651755933\",\n        },\n{\n            'v': 0.9908586166919355,\n            'f': \"0.9908586166919355\",\n        },\n{\n            'v': 0.9881131788241242,\n            'f': \"0.9881131788241242\",\n        },\n{\n            'v': 0.9815515050446331,\n            'f': \"0.9815515050446331\",\n        },\n{\n            'v': 0.984816031418734,\n            'f': \"0.984816031418734\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 58,\n            'f': \"58\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 64, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 60,\n            'f': \"60\",\n        },\n{\n            'v': 0.019841048381326533,\n            'f': \"0.019841048381326533\",\n        },\n{\n            'v': 0.026758116941988673,\n            'f': \"0.026758116941988673\",\n        },\n{\n            'v': 0.9943093931841506,\n            'f': \"0.9943093931841506\",\n        },\n{\n            'v': 0.9902659079188296,\n            'f': \"0.9902659079188296\",\n        },\n{\n            'v': 0.9922054070527241,\n            'f': \"0.9922054070527241\",\n        },\n{\n            'v': 0.9895211908484994,\n            'f': \"0.9895211908484994\",\n        },\n{\n            'v': 0.9834538606897848,\n            'f': \"0.9834538606897848\",\n        },\n{\n            'v': 0.9864772467254864,\n            'f': \"0.9864772467254864\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 59,\n            'f': \"59\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 64, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 66,\n            'f': \"66\",\n        },\n{\n            'v': 0.018083272695558452,\n            'f': \"0.018083272695558452\",\n        },\n{\n            'v': 0.02855252579008181,\n            'f': \"0.02855252579008181\",\n        },\n{\n            'v': 0.995118539802475,\n            'f': \"0.995118539802475\",\n        },\n{\n            'v': 0.9922175876110143,\n            'f': \"0.9922175876110143\",\n        },\n{\n            'v': 0.9935892673341797,\n            'f': \"0.9935892673341797\",\n        },\n{\n            'v': 0.9914042780594218,\n            'f': \"0.9914042780594218\",\n        },\n{\n            'v': 0.9829892481320545,\n            'f': \"0.9829892481320545\",\n        },\n{\n            'v': 0.9871780801632493,\n            'f': \"0.9871780801632493\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 60,\n            'f': \"60\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 128, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 69,\n            'f': \"69\",\n        },\n{\n            'v': 0.03234145330501995,\n            'f': \"0.03234145330501995\",\n        },\n{\n            'v': 0.0426254152841994,\n            'f': \"0.0426254152841994\",\n        },\n{\n            'v': 0.9888446952696524,\n            'f': \"0.9888446952696524\",\n        },\n{\n            'v': 0.9790619541172587,\n            'f': \"0.9790619541172587\",\n        },\n{\n            'v': 0.9838243053056283,\n            'f': \"0.9838243053056283\",\n        },\n{\n            'v': 0.9828989097488331,\n            'f': \"0.9828989097488331\",\n        },\n{\n            'v': 0.9726699625488778,\n            'f': \"0.9726699625488778\",\n        },\n{\n            'v': 0.9777539277857742,\n            'f': \"0.9777539277857742\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 61,\n            'f': \"61\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 128, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 61,\n            'f': \"61\",\n        },\n{\n            'v': 0.026119213750562444,\n            'f': \"0.026119213750562444\",\n        },\n{\n            'v': 0.03453516102175123,\n            'f': \"0.03453516102175123\",\n        },\n{\n            'v': 0.989936652112098,\n            'f': \"0.989936652112098\",\n        },\n{\n            'v': 0.9851828442809493,\n            'f': \"0.9851828442809493\",\n        },\n{\n            'v': 0.9874744217958339,\n            'f': \"0.9874744217958339\",\n        },\n{\n            'v': 0.9843533447632593,\n            'f': \"0.9843533447632593\",\n        },\n{\n            'v': 0.9805485876075037,\n            'f': \"0.9805485876075037\",\n        },\n{\n            'v': 0.9824346291016118,\n            'f': \"0.9824346291016118\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 62,\n            'f': \"62\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 128, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 61,\n            'f': \"61\",\n        },\n{\n            'v': 0.020841396143947987,\n            'f': \"0.020841396143947987\",\n        },\n{\n            'v': 0.031325002557428434,\n            'f': \"0.031325002557428434\",\n        },\n{\n            'v': 0.9940990863369801,\n            'f': \"0.9940990863369801\",\n        },\n{\n            'v': 0.9899450608705461,\n            'f': \"0.9899450608705461\",\n        },\n{\n            'v': 0.991970442142383,\n            'f': \"0.991970442142383\",\n        },\n{\n            'v': 0.9852731489922103,\n            'f': \"0.9852731489922103\",\n        },\n{\n            'v': 0.9856963600166032,\n            'f': \"0.9856963600166032\",\n        },\n{\n            'v': 0.9854805300827998,\n            'f': \"0.9854805300827998\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 63,\n            'f': \"63\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0008753516269381944, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 61,\n            'f': \"61\",\n        },\n{\n            'v': 0.020537101219430618,\n            'f': \"0.020537101219430618\",\n        },\n{\n            'v': 0.02969607963799611,\n            'f': \"0.02969607963799611\",\n        },\n{\n            'v': 0.9933200993385128,\n            'f': \"0.9933200993385128\",\n        },\n{\n            'v': 0.9920391567361662,\n            'f': \"0.9920391567361662\",\n        },\n{\n            'v': 0.992638715347216,\n            'f': \"0.992638715347216\",\n        },\n{\n            'v': 0.9895207389615812,\n            'f': \"0.9895207389615812\",\n        },\n{\n            'v': 0.982485070140583,\n            'f': \"0.982485070140583\",\n        },\n{\n            'v': 0.9859881853664432,\n            'f': \"0.9859881853664432\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 64,\n            'f': \"64\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 64, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 65,\n            'f': \"65\",\n        },\n{\n            'v': 0.04459235037400687,\n            'f': \"0.04459235037400687\",\n        },\n{\n            'v': 0.05334479975229276,\n            'f': \"0.05334479975229276\",\n        },\n{\n            'v': 0.97788728511881,\n            'f': \"0.97788728511881\",\n        },\n{\n            'v': 0.9658741606771407,\n            'f': \"0.9658741606771407\",\n        },\n{\n            'v': 0.9715308600975859,\n            'f': \"0.9715308600975859\",\n        },\n{\n            'v': 0.9814854611373145,\n            'f': \"0.9814854611373145\",\n        },\n{\n            'v': 0.9594866875182908,\n            'f': \"0.9594866875182908\",\n        },\n{\n            'v': 0.970331733304305,\n            'f': \"0.970331733304305\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 65,\n            'f': \"65\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 64, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 66,\n            'f': \"66\",\n        },\n{\n            'v': 0.027518761248028017,\n            'f': \"0.027518761248028017\",\n        },\n{\n            'v': 0.035282769929809665,\n            'f': \"0.035282769929809665\",\n        },\n{\n            'v': 0.988422087740818,\n            'f': \"0.988422087740818\",\n        },\n{\n            'v': 0.9826435988122282,\n            'f': \"0.9826435988122282\",\n        },\n{\n            'v': 0.9853523332525296,\n            'f': \"0.9853523332525296\",\n        },\n{\n            'v': 0.9929148725488274,\n            'f': \"0.9929148725488274\",\n        },\n{\n            'v': 0.9738415929578762,\n            'f': \"0.9738415929578762\",\n        },\n{\n            'v': 0.9832630622680296,\n            'f': \"0.9832630622680296\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 66,\n            'f': \"66\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 64, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 53,\n            'f': \"53\",\n        },\n{\n            'v': 0.022907600279381116,\n            'f': \"0.022907600279381116\",\n        },\n{\n            'v': 0.03131439223680709,\n            'f': \"0.03131439223680709\",\n        },\n{\n            'v': 0.9895789435317425,\n            'f': \"0.9895789435317425\",\n        },\n{\n            'v': 0.9880341159804206,\n            'f': \"0.9880341159804206\",\n        },\n{\n            'v': 0.9886406784960506,\n            'f': \"0.9886406784960506\",\n        },\n{\n            'v': 0.9895461004054994,\n            'f': \"0.9895461004054994\",\n        },\n{\n            'v': 0.9811295384899155,\n            'f': \"0.9811295384899155\",\n        },\n{\n            'v': 0.9853134952850107,\n            'f': \"0.9853134952850107\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 67,\n            'f': \"67\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 64, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 46,\n            'f': \"46\",\n        },\n{\n            'v': 0.027125622651284775,\n            'f': \"0.027125622651284775\",\n        },\n{\n            'v': 0.03133498037752417,\n            'f': \"0.03133498037752417\",\n        },\n{\n            'v': 0.9875084849737935,\n            'f': \"0.9875084849737935\",\n        },\n{\n            'v': 0.9854593871144708,\n            'f': \"0.9854593871144708\",\n        },\n{\n            'v': 0.9862413861309313,\n            'f': \"0.9862413861309313\",\n        },\n{\n            'v': 0.9870810556452144,\n            'f': \"0.9870810556452144\",\n        },\n{\n            'v': 0.9838384281479297,\n            'f': \"0.9838384281479297\",\n        },\n{\n            'v': 0.9854540575230278,\n            'f': \"0.9854540575230278\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 68,\n            'f': \"68\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 128, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 60,\n            'f': \"60\",\n        },\n{\n            'v': 0.03771288329084646,\n            'f': \"0.03771288329084646\",\n        },\n{\n            'v': 0.0473070870252819,\n            'f': \"0.0473070870252819\",\n        },\n{\n            'v': 0.9826115192813438,\n            'f': \"0.9826115192813438\",\n        },\n{\n            'v': 0.9747130608081435,\n            'f': \"0.9747130608081435\",\n        },\n{\n            'v': 0.9784949405193663,\n            'f': \"0.9784949405193663\",\n        },\n{\n            'v': 0.9785976526545951,\n            'f': \"0.9785976526545951\",\n        },\n{\n            'v': 0.9762395192799388,\n            'f': \"0.9762395192799388\",\n        },\n{\n            'v': 0.9774115294274767,\n            'f': \"0.9774115294274767\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 69,\n            'f': \"69\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 128, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 68,\n            'f': \"68\",\n        },\n{\n            'v': 0.026293267063019883,\n            'f': \"0.026293267063019883\",\n        },\n{\n            'v': 0.03469508554005541,\n            'f': \"0.03469508554005541\",\n        },\n{\n            'v': 0.9901640051397432,\n            'f': \"0.9901640051397432\",\n        },\n{\n            'v': 0.9860449941566796,\n            'f': \"0.9860449941566796\",\n        },\n{\n            'v': 0.9880476287314058,\n            'f': \"0.9880476287314058\",\n        },\n{\n            'v': 0.9905030430264912,\n            'f': \"0.9905030430264912\",\n        },\n{\n            'v': 0.9774360827901883,\n            'f': \"0.9774360827901883\",\n        },\n{\n            'v': 0.9839198717123196,\n            'f': \"0.9839198717123196\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 70,\n            'f': \"70\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 128, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 53,\n            'f': \"53\",\n        },\n{\n            'v': 0.022757590199817497,\n            'f': \"0.022757590199817497\",\n        },\n{\n            'v': 0.029775028319907762,\n            'f': \"0.029775028319907762\",\n        },\n{\n            'v': 0.9905578247901032,\n            'f': \"0.9905578247901032\",\n        },\n{\n            'v': 0.9884047787262031,\n            'f': \"0.9884047787262031\",\n        },\n{\n            'v': 0.9893382888889143,\n            'f': \"0.9893382888889143\",\n        },\n{\n            'v': 0.9895211908484994,\n            'f': \"0.9895211908484994\",\n        },\n{\n            'v': 0.9778993212986435,\n            'f': \"0.9778993212986435\",\n        },\n{\n            'v': 0.9836716844776063,\n            'f': \"0.9836716844776063\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 71,\n            'f': \"71\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.001981796079782972, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 59,\n            'f': \"59\",\n        },\n{\n            'v': 0.020704336056031312,\n            'f': \"0.020704336056031312\",\n        },\n{\n            'v': 0.029369806884899992,\n            'f': \"0.029369806884899992\",\n        },\n{\n            'v': 0.9928747692250744,\n            'f': \"0.9928747692250744\",\n        },\n{\n            'v': 0.9906364706857228,\n            'f': \"0.9906364706857228\",\n        },\n{\n            'v': 0.99169744370989,\n            'f': \"0.99169744370989\",\n        },\n{\n            'v': 0.9923705052784237,\n            'f': \"0.9923705052784237\",\n        },\n{\n            'v': 0.9802400997176342,\n            'f': \"0.9802400997176342\",\n        },\n{\n            'v': 0.9862676832958303,\n            'f': \"0.9862676832958303\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 72,\n            'f': \"72\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 64, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 70,\n            'f': \"70\",\n        },\n{\n            'v': 0.033803898255678935,\n            'f': \"0.033803898255678935\",\n        },\n{\n            'v': 0.04308517212976295,\n            'f': \"0.04308517212976295\",\n        },\n{\n            'v': 0.9863781589911245,\n            'f': \"0.9863781589911245\",\n        },\n{\n            'v': 0.9756989742667433,\n            'f': \"0.9756989742667433\",\n        },\n{\n            'v': 0.9808254609738145,\n            'f': \"0.9808254609738145\",\n        },\n{\n            'v': 0.9866933898516264,\n            'f': \"0.9866933898516264\",\n        },\n{\n            'v': 0.9682074433183169,\n            'f': \"0.9682074433183169\",\n        },\n{\n            'v': 0.9773535743876965,\n            'f': \"0.9773535743876965\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 73,\n            'f': \"73\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 64, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 57,\n            'f': \"57\",\n        },\n{\n            'v': 0.0255590647587047,\n            'f': \"0.0255590647587047\",\n        },\n{\n            'v': 0.034222962051834845,\n            'f': \"0.034222962051834845\",\n        },\n{\n            'v': 0.9904027161095676,\n            'f': \"0.9904027161095676\",\n        },\n{\n            'v': 0.9890296462801396,\n            'f': \"0.9890296462801396\",\n        },\n{\n            'v': 0.9895801934777269,\n            'f': \"0.9895801934777269\",\n        },\n{\n            'v': 0.9885857616605589,\n            'f': \"0.9885857616605589\",\n        },\n{\n            'v': 0.9792238695853906,\n            'f': \"0.9792238695853906\",\n        },\n{\n            'v': 0.9838778750886936,\n            'f': \"0.9838778750886936\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 74,\n            'f': \"74\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 64, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 71,\n            'f': \"71\",\n        },\n{\n            'v': 0.018364499195693284,\n            'f': \"0.018364499195693284\",\n        },\n{\n            'v': 0.029131769406836466,\n            'f': \"0.029131769406836466\",\n        },\n{\n            'v': 0.9944530854022698,\n            'f': \"0.9944530854022698\",\n        },\n{\n            'v': 0.9918271568553382,\n            'f': \"0.9918271568553382\",\n        },\n{\n            'v': 0.9930527715233879,\n            'f': \"0.9930527715233879\",\n        },\n{\n            'v': 0.9890798579300444,\n            'f': \"0.9890798579300444\",\n        },\n{\n            'v': 0.9806377111950018,\n            'f': \"0.9806377111950018\",\n        },\n{\n            'v': 0.984837378452415,\n            'f': \"0.984837378452415\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 75,\n            'f': \"75\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 64, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 49,\n            'f': \"49\",\n        },\n{\n            'v': 0.02180186708170797,\n            'f': \"0.02180186708170797\",\n        },\n{\n            'v': 0.029590734591086704,\n            'f': \"0.029590734591086704\",\n        },\n{\n            'v': 0.9921396932857313,\n            'f': \"0.9921396932857313\",\n        },\n{\n            'v': 0.991508951927166,\n            'f': \"0.991508951927166\",\n        },\n{\n            'v': 0.9917089988920693,\n            'f': \"0.9917089988920693\",\n        },\n{\n            'v': 0.9928616573108785,\n            'f': \"0.9928616573108785\",\n        },\n{\n            'v': 0.9811677064513741,\n            'f': \"0.9811677064513741\",\n        },\n{\n            'v': 0.9869792455311013,\n            'f': \"0.9869792455311013\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 76,\n            'f': \"76\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 128, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 66,\n            'f': \"66\",\n        },\n{\n            'v': 0.03237111579033654,\n            'f': \"0.03237111579033654\",\n        },\n{\n            'v': 0.04315400643600631,\n            'f': \"0.04315400643600631\",\n        },\n{\n            'v': 0.9878087183735752,\n            'f': \"0.9878087183735752\",\n        },\n{\n            'v': 0.9794630759684497,\n            'f': \"0.9794630759684497\",\n        },\n{\n            'v': 0.9835246686474806,\n            'f': \"0.9835246686474806\",\n        },\n{\n            'v': 0.9900182313550158,\n            'f': \"0.9900182313550158\",\n        },\n{\n            'v': 0.9696977925333922,\n            'f': \"0.9696977925333922\",\n        },\n{\n            'v': 0.9797430418452011,\n            'f': \"0.9797430418452011\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 77,\n            'f': \"77\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 128, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 53,\n            'f': \"53\",\n        },\n{\n            'v': 0.024942839479784377,\n            'f': \"0.024942839479784377\",\n        },\n{\n            'v': 0.031735684341171765,\n            'f': \"0.031735684341171765\",\n        },\n{\n            'v': 0.9896837237695191,\n            'f': \"0.9896837237695191\",\n        },\n{\n            'v': 0.988168739286128,\n            'f': \"0.988168739286128\",\n        },\n{\n            'v': 0.9888557483422323,\n            'f': \"0.9888557483422323\",\n        },\n{\n            'v': 0.9862017859322532,\n            'f': \"0.9862017859322532\",\n        },\n{\n            'v': 0.9805826512492936,\n            'f': \"0.9805826512492936\",\n        },\n{\n            'v': 0.9833759284616425,\n            'f': \"0.9833759284616425\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 78,\n            'f': \"78\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 128, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 70,\n            'f': \"70\",\n        },\n{\n            'v': 0.020296860184149234,\n            'f': \"0.020296860184149234\",\n        },\n{\n            'v': 0.03174594486599526,\n            'f': \"0.03174594486599526\",\n        },\n{\n            'v': 0.9928090838795219,\n            'f': \"0.9928090838795219\",\n        },\n{\n            'v': 0.9902976716815701,\n            'f': \"0.9902976716815701\",\n        },\n{\n            'v': 0.9914939830719621,\n            'f': \"0.9914939830719621\",\n        },\n{\n            'v': 0.9847600316397859,\n            'f': \"0.9847600316397859\",\n        },\n{\n            'v': 0.9810234786239234,\n            'f': \"0.9810234786239234\",\n        },\n{\n            'v': 0.9828878527093105,\n            'f': \"0.9828878527093105\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 79,\n            'f': \"79\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0009144615755940233, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 65,\n            'f': \"65\",\n        },\n{\n            'v': 0.020406437290134707,\n            'f': \"0.020406437290134707\",\n        },\n{\n            'v': 0.029725743843005695,\n            'f': \"0.029725743843005695\",\n        },\n{\n            'v': 0.9945475061721203,\n            'f': \"0.9945475061721203\",\n        },\n{\n            'v': 0.9921020707503847,\n            'f': \"0.9921020707503847\",\n        },\n{\n            'v': 0.9932908275025076,\n            'f': \"0.9932908275025076\",\n        },\n{\n            'v': 0.9923734495154543,\n            'f': \"0.9923734495154543\",\n        },\n{\n            'v': 0.9816319855252548,\n            'f': \"0.9816319855252548\",\n        },\n{\n            'v': 0.9869672414952848,\n            'f': \"0.9869672414952848\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 80,\n            'f': \"80\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 64, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 72,\n            'f': \"72\",\n        },\n{\n            'v': 0.03173925218696173,\n            'f': \"0.03173925218696173\",\n        },\n{\n            'v': 0.042663133769100886,\n            'f': \"0.042663133769100886\",\n        },\n{\n            'v': 0.9864051453520862,\n            'f': \"0.9864051453520862\",\n        },\n{\n            'v': 0.9813643682543781,\n            'f': \"0.9813643682543781\",\n        },\n{\n            'v': 0.9837151445969321,\n            'f': \"0.9837151445969321\",\n        },\n{\n            'v': 0.9852634125072817,\n            'f': \"0.9852634125072817\",\n        },\n{\n            'v': 0.9741075961123318,\n            'f': \"0.9741075961123318\",\n        },\n{\n            'v': 0.9796422776519915,\n            'f': \"0.9796422776519915\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 81,\n            'f': \"81\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 64, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 63,\n            'f': \"63\",\n        },\n{\n            'v': 0.02438762842465239,\n            'f': \"0.02438762842465239\",\n        },\n{\n            'v': 0.03240532332781664,\n            'f': \"0.03240532332781664\",\n        },\n{\n            'v': 0.9900400254784941,\n            'f': \"0.9900400254784941\",\n        },\n{\n            'v': 0.9885772612206093,\n            'f': \"0.9885772612206093\",\n        },\n{\n            'v': 0.989186335135652,\n            'f': \"0.989186335135652\",\n        },\n{\n            'v': 0.9871748053991528,\n            'f': \"0.9871748053991528\",\n        },\n{\n            'v': 0.9806048055310497,\n            'f': \"0.9806048055310497\",\n        },\n{\n            'v': 0.9838764502083478,\n            'f': \"0.9838764502083478\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 82,\n            'f': \"82\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 64, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 72,\n            'f': \"72\",\n        },\n{\n            'v': 0.01888879258014936,\n            'f': \"0.01888879258014936\",\n        },\n{\n            'v': 0.03072551382878392,\n            'f': \"0.03072551382878392\",\n        },\n{\n            'v': 0.9936511908767084,\n            'f': \"0.9936511908767084\",\n        },\n{\n            'v': 0.9914648586807896,\n            'f': \"0.9914648586807896\",\n        },\n{\n            'v': 0.9924933786870498,\n            'f': \"0.9924933786870498\",\n        },\n{\n            'v': 0.9881102345870936,\n            'f': \"0.9881102345870936\",\n        },\n{\n            'v': 0.9806164112021588,\n            'f': \"0.9806164112021588\",\n        },\n{\n            'v': 0.9843457438411622,\n            'f': \"0.9843457438411622\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 83,\n            'f': \"83\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 64, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 55,\n            'f': \"55\",\n        },\n{\n            'v': 0.020556420512403546,\n            'f': \"0.020556420512403546\",\n        },\n{\n            'v': 0.03066764041730219,\n            'f': \"0.03066764041730219\",\n        },\n{\n            'v': 0.9932526568852024,\n            'f': \"0.9932526568852024\",\n        },\n{\n            'v': 0.9903923154790452,\n            'f': \"0.9903923154790452\",\n        },\n{\n            'v': 0.9917254732961517,\n            'f': \"0.9917254732961517\",\n        },\n{\n            'v': 0.9957299928237415,\n            'f': \"0.9957299928237415\",\n        },\n{\n            'v': 0.9775469534821435,\n            'f': \"0.9775469534821435\",\n        },\n{\n            'v': 0.9865511771519465,\n            'f': \"0.9865511771519465\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 84,\n            'f': \"84\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 128, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 68,\n            'f': \"68\",\n        },\n{\n            'v': 0.03335705410906744,\n            'f': \"0.03335705410906744\",\n        },\n{\n            'v': 0.04264182039501331,\n            'f': \"0.04264182039501331\",\n        },\n{\n            'v': 0.987131179785267,\n            'f': \"0.987131179785267\",\n        },\n{\n            'v': 0.9778778624206931,\n            'f': \"0.9778778624206931\",\n        },\n{\n            'v': 0.9823751364535345,\n            'f': \"0.9823751364535345\",\n        },\n{\n            'v': 0.986709014810616,\n            'f': \"0.986709014810616\",\n        },\n{\n            'v': 0.9709424323135254,\n            'f': \"0.9709424323135254\",\n        },\n{\n            'v': 0.9787546813620862,\n            'f': \"0.9787546813620862\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 85,\n            'f': \"85\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 128, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 57,\n            'f': \"57\",\n        },\n{\n            'v': 0.025261819763333117,\n            'f': \"0.025261819763333117\",\n        },\n{\n            'v': 0.03323419678610625,\n            'f': \"0.03323419678610625\",\n        },\n{\n            'v': 0.9894319626337144,\n            'f': \"0.9894319626337144\",\n        },\n{\n            'v': 0.987178686690833,\n            'f': \"0.987178686690833\",\n        },\n{\n            'v': 0.9882213005148381,\n            'f': \"0.9882213005148381\",\n        },\n{\n            'v': 0.9895334196835404,\n            'f': \"0.9895334196835404\",\n        },\n{\n            'v': 0.976967621873028,\n            'f': \"0.976967621873028\",\n        },\n{\n            'v': 0.9832085063804747,\n            'f': \"0.9832085063804747\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 86,\n            'f': \"86\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 128, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': 0.022497781362909426,\n            'f': \"0.022497781362909426\",\n        },\n{\n            'v': 0.03217587433115313,\n            'f': \"0.03217587433115313\",\n        },\n{\n            'v': 0.9917122998372047,\n            'f': \"0.9917122998372047\",\n        },\n{\n            'v': 0.9884129908491348,\n            'f': \"0.9884129908491348\",\n        },\n{\n            'v': 0.9899886965892487,\n            'f': \"0.9899886965892487\",\n        },\n{\n            'v': 0.9857296549827368,\n            'f': \"0.9857296549827368\",\n        },\n{\n            'v': 0.9861914288202919,\n            'f': \"0.9861914288202919\",\n        },\n{\n            'v': 0.9859597109839189,\n            'f': \"0.9859597109839189\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 87,\n            'f': \"87\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0012742905281420724, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 68,\n            'f': \"68\",\n        },\n{\n            'v': 0.019063930205663694,\n            'f': \"0.019063930205663694\",\n        },\n{\n            'v': 0.029558685352814567,\n            'f': \"0.029558685352814567\",\n        },\n{\n            'v': 0.994666113903744,\n            'f': \"0.994666113903744\",\n        },\n{\n            'v': 0.9919930373465932,\n            'f': \"0.9919930373465932\",\n        },\n{\n            'v': 0.9932878000148903,\n            'f': \"0.9932878000148903\",\n        },\n{\n            'v': 0.9947701059657191,\n            'f': \"0.9947701059657191\",\n        },\n{\n            'v': 0.9798069556338956,\n            'f': \"0.9798069556338956\",\n        },\n{\n            'v': 0.9872270600185536,\n            'f': \"0.9872270600185536\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 88,\n            'f': \"88\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 64, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 72,\n            'f': \"72\",\n        },\n{\n            'v': 0.04307244685729208,\n            'f': \"0.04307244685729208\",\n        },\n{\n            'v': 0.0521123243130974,\n            'f': \"0.0521123243130974\",\n        },\n{\n            'v': 0.9761655142145956,\n            'f': \"0.9761655142145956\",\n        },\n{\n            'v': 0.9652731484445286,\n            'f': \"0.9652731484445286\",\n        },\n{\n            'v': 0.9704128224631444,\n            'f': \"0.9704128224631444\",\n        },\n{\n            'v': 0.9843098660102388,\n            'f': \"0.9843098660102388\",\n        },\n{\n            'v': 0.9569302978807798,\n            'f': \"0.9569302978807798\",\n        },\n{\n            'v': 0.9704189261368995,\n            'f': \"0.9704189261368995\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 89,\n            'f': \"89\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 64, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 59,\n            'f': \"59\",\n        },\n{\n            'v': 0.030040605852694035,\n            'f': \"0.030040605852694035\",\n        },\n{\n            'v': 0.03793496515896312,\n            'f': \"0.03793496515896312\",\n        },\n{\n            'v': 0.9867155911250655,\n            'f': \"0.9867155911250655\",\n        },\n{\n            'v': 0.981954771194292,\n            'f': \"0.981954771194292\",\n        },\n{\n            'v': 0.9841632944622041,\n            'f': \"0.9841632944622041\",\n        },\n{\n            'v': 0.990493758428481,\n            'f': \"0.990493758428481\",\n        },\n{\n            'v': 0.9756182817263758,\n            'f': \"0.9756182817263758\",\n        },\n{\n            'v': 0.9829893603788756,\n            'f': \"0.9829893603788756\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 90,\n            'f': \"90\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 64, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 69,\n            'f': \"69\",\n        },\n{\n            'v': 0.01947353778648076,\n            'f': \"0.01947353778648076\",\n        },\n{\n            'v': 0.030819157874563716,\n            'f': \"0.030819157874563716\",\n        },\n{\n            'v': 0.9939745491161102,\n            'f': \"0.9939745491161102\",\n        },\n{\n            'v': 0.99041226444162,\n            'f': \"0.99041226444162\",\n        },\n{\n            'v': 0.9920828913755969,\n            'f': \"0.9920828913755969\",\n        },\n{\n            'v': 0.9900216274789646,\n            'f': \"0.9900216274789646\",\n        },\n{\n            'v': 0.982493323413522,\n            'f': \"0.982493323413522\",\n        },\n{\n            'v': 0.9862372847437672,\n            'f': \"0.9862372847437672\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 91,\n            'f': \"91\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 64, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 71,\n            'f': \"71\",\n        },\n{\n            'v': 0.019795069416701144,\n            'f': \"0.019795069416701144\",\n        },\n{\n            'v': 0.029030802904842647,\n            'f': \"0.029030802904842647\",\n        },\n{\n            'v': 0.9931955240622338,\n            'f': \"0.9931955240622338\",\n        },\n{\n            'v': 0.9908098924086873,\n            'f': \"0.9908098924086873\",\n        },\n{\n            'v': 0.9919120250677361,\n            'f': \"0.9919120250677361\",\n        },\n{\n            'v': 0.9919232838859073,\n            'f': \"0.9919232838859073\",\n        },\n{\n            'v': 0.9825278301679967,\n            'f': \"0.9825278301679967\",\n        },\n{\n            'v': 0.9871981981102582,\n            'f': \"0.9871981981102582\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 92,\n            'f': \"92\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 128, 'start_layer': 'conv2.1_weight'}\",\n{\n            'v': 68,\n            'f': \"68\",\n        },\n{\n            'v': 0.043721206787064724,\n            'f': \"0.043721206787064724\",\n        },\n{\n            'v': 0.051468710040616004,\n            'f': \"0.051468710040616004\",\n        },\n{\n            'v': 0.9795785473219948,\n            'f': \"0.9795785473219948\",\n        },\n{\n            'v': 0.9662991859592059,\n            'f': \"0.9662991859592059\",\n        },\n{\n            'v': 0.9727315315877391,\n            'f': \"0.9727315315877391\",\n        },\n{\n            'v': 0.9828866809137922,\n            'f': \"0.9828866809137922\",\n        },\n{\n            'v': 0.9577620900696097,\n            'f': \"0.9577620900696097\",\n        },\n{\n            'v': 0.9701544912050144,\n            'f': \"0.9701544912050144\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 93,\n            'f': \"93\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 128, 'start_layer': 'conv3.1_weight'}\",\n{\n            'v': 64,\n            'f': \"64\",\n        },\n{\n            'v': 0.026477844012476597,\n            'f': \"0.026477844012476597\",\n        },\n{\n            'v': 0.0352039636564009,\n            'f': \"0.0352039636564009\",\n        },\n{\n            'v': 0.9875886881691769,\n            'f': \"0.9875886881691769\",\n        },\n{\n            'v': 0.9839115216691044,\n            'f': \"0.9839115216691044\",\n        },\n{\n            'v': 0.985648570217583,\n            'f': \"0.985648570217583\",\n        },\n{\n            'v': 0.9919545338038867,\n            'f': \"0.9919545338038867\",\n        },\n{\n            'v': 0.9729272514464115,\n            'f': \"0.9729272514464115\",\n        },\n{\n            'v': 0.982331143481537,\n            'f': \"0.982331143481537\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 94,\n            'f': \"94\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 128, 'start_layer': 'conv4.1_weight'}\",\n{\n            'v': 67,\n            'f': \"67\",\n        },\n{\n            'v': 0.019458977887481622,\n            'f': \"0.019458977887481622\",\n        },\n{\n            'v': 0.03011886373520717,\n            'f': \"0.03011886373520717\",\n        },\n{\n            'v': 0.9945429184547366,\n            'f': \"0.9945429184547366\",\n        },\n{\n            'v': 0.9914561523154032,\n            'f': \"0.9914561523154032\",\n        },\n{\n            'v': 0.9929578206187107,\n            'f': \"0.9929578206187107\",\n        },\n{\n            'v': 0.9824390076343575,\n            'f': \"0.9824390076343575\",\n        },\n{\n            'v': 0.9861377405571846,\n            'f': \"0.9861377405571846\",\n        },\n{\n            'v': 0.9842780795098807,\n            'f': \"0.9842780795098807\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }],\n [{\n            'v': 95,\n            'f': \"95\",\n        },\n\"deep_rescnn\",\n\"{'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\",\n{\n            'v': 62,\n            'f': \"62\",\n        },\n{\n            'v': 0.019647994349878132,\n            'f': \"0.019647994349878132\",\n        },\n{\n            'v': 0.029902966620074106,\n            'f': \"0.029902966620074106\",\n        },\n{\n            'v': 0.9942470512578548,\n            'f': \"0.9942470512578548\",\n        },\n{\n            'v': 0.9917314704256792,\n            'f': \"0.9917314704256792\",\n        },\n{\n            'v': 0.9929521535097453,\n            'f': \"0.9929521535097453\",\n        },\n{\n            'v': 0.9957021390297108,\n            'f': \"0.9957021390297108\",\n        },\n{\n            'v': 0.9793811045228158,\n            'f': \"0.9793811045228158\",\n        },\n{\n            'v': 0.9874734184320075,\n            'f': \"0.9874734184320075\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        },\n{\n            'v': null,\n            'f': \"null\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"model_name\"], [\"string\", \"hyperparameter\"], [\"number\", \"best_epoch\"], [\"number\", \"best_train_cost\"], [\"number\", \"best_val_cost\"], [\"number\", \"best_train_recall\"], [\"number\", \"best_train_precision\"], [\"number\", \"best_train_f1\"], [\"number\", \"best_val_recall\"], [\"number\", \"best_val_precision\"], [\"number\", \"best_val_f1\"], [\"number\", \"best_test_recall\"], [\"number\", \"best_test_precision\"], [\"number\", \"best_test_f1\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: \"0\",\n      });\n    "
          },
          "metadata": {},
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best hyperparameter setting was determined based on the validation F1 score, and the following combination of hyperparameters was found to be optimal.\n",
        "\n",
        "    {'weight_decay': 0.01, 'learning_rate': 0.0023290654300914477, 'batch_size': 128, 'start_layer': 'conv5.1_weight'}\n"
      ],
      "metadata": {
        "id": "o9Ws0hyfccxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is clear that the top 5 performing models are those in which only the last residual block is frozen during training (i.e., with a start_layer of conv5.1_weight). This suggests that the feature extraction step from our previous task (Arithmetic) has been highly effective in providing the necessary information for this task.\n",
        "\n",
        "By leveraging the pre-trained weights from the previous task, we are able to fine-tune the model's performance on the current task more effectively. This highlights the benefits of transfer learning, where knowledge gained from one task can be applied to another related task to improve performance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NmscDrOIbItS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark.to_csv(PATH + '/benchmark.csv', index=False)"
      ],
      "metadata": {
        "id": "a0pRVqADD70F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_model, PATH + '/best_deep_rescnn_model_state_dict.pt')"
      ],
      "metadata": {
        "id": "PlHCi7cOEc0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we did not have the opportunity to explore further hyperparameter settings, such as the number of hidden units in the deep_rescnn's hidden layer, there is always the possibility of doing so in the future.\n",
        "\n",
        "However, given the time constraints of the current project, we have moved on to the next task, which involves finding the optimal threshold that can achieve the best validation F1 score for our best model. By focusing on this specific task, we can ensure that we are using the best possible threshold for our model's performance without having to invest further time into exploring additional hyperparameters."
      ],
      "metadata": {
        "id": "sjVmPDwfcC9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔺 Model Evaluation\n",
        "\n",
        "\n",
        "\n",
        "- This section we are going to check the test set score, confusion matrix, demonstrating how to predict\n",
        "\n",
        "We have chosen to use the F1 score as our primary evaluation metric because our main objective is to ensure that the model does not miss any patient with Myocardial Infarction, as misclassifying such a patient as not having the disease could have severe consequences, including loss of life.\n",
        "\n",
        "By prioritizing recall, we can minimize the risk of false negatives, where the model incorrectly predicts that a patient does not have the disease when they actually do. However, relying solely on recall can result in a large number of false positives, where the model incorrectly predicts that a patient has the disease when they do not. This is where precision comes in, as it measures the proportion of predicted positive cases that are actually true positives.\n",
        "\n",
        "To balance the trade-off between recall and precision, we use the F1 score, which is a harmonic mean of precision and recall. The F1 score provides a balanced assessment of the model's ability to correctly identify true positive cases while minimizing false positives and false negatives.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G3ynpeLxWBmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try load the best model by architecture\n",
        "PATH = '/content/drive/MyDrive/mi_classification'\n",
        "\n",
        "model = Deep_ResCNN(num_classes = 1).to(device)\n",
        "model.load_state_dict(torch.load(PATH + '/best_deep_rescnn_model_state_dict.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRU2ZgzX4jXD",
        "outputId": "450242b2-a14f-44a4-f41c-aa6a3463d850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weight = get_weighted_for_bce(y_train)\n",
        "criterion = nn.BCEWithLogitsLoss(weight=pos_weight.to(device))\n",
        "\n",
        "train_loader, val_loader, test_loader = get_loader(train_set, val_set, test_set, train_batch_size=64)\n",
        "test_loss, test_acc, test_recall,test_precision,test_f1 = eval_epoch(model, criterion, test_loader,threshold=0.5)"
      ],
      "metadata": {
        "id": "bahvlDcNdQ1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f' weight recall score of the best model on test set is {test_recall : .4f}')\n",
        "print(f' weight precision score of the best model on test set is {test_precision : .4f}')\n",
        "print(f' weight f1 score of the best model on test set is {test_f1 : .4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095ef114-497c-42a3-aada-400f4fafeff2",
        "id": "L-9bBNcBdQ1A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " weight recall score of the best model on test set is  0.9929\n",
            " weight precision score of the best model on test set is  0.9761\n",
            " weight f1 score of the best model on test set is  0.9844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to the benchmark\n",
        "\n",
        "benchmark.iloc[95,-3:] = [test_recall,test_precision,test_f1]"
      ],
      "metadata": {
        "id": "zlYG2KyA_qay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performed exceptionally well on the test set, as it was able to predict patients with actual Myocardial Infarction disease as having the disease with a very low rate of false negatives.\n",
        "\n",
        "While the precision metric suggests that the model may produce some false alarms, it is more important to minimize the risk of missing patients with Myocardial Infarction, as this can have serious consequences for their health. Therefore, the model's ability to accurately identify patients with the disease is of paramount importance, even if it leads to some false alarms.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UFihXYvwfnL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#benchmark.to_csv('/content/drive/MyDrive/arrhythmia_classification/benchmark.csv',index=False)"
      ],
      "metadata": {
        "id": "rgRVBfAaBA4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Area under Precision-Recall curve"
      ],
      "metadata": {
        "id": "OwaUzZ1ThNus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we aim to select the best threshold that minimizes the F1 score on the validation set, it is important to determine whether our model heavily relies on the threshold to minimize errors in its predictions.\n",
        "\n",
        "To assess this, we can use the Area Under Precision-Recall Curve (AUPRC), which measures the model's ability to rank positive examples higher than negative examples across a range of thresholds. If the AUPRC is high, it suggests that the model is able to make accurate predictions across a range of thresholds, indicating that the model's performance is not heavily reliant on the threshold.\n",
        "\n",
        "Therefore, by examining the AUPRC curve, we can gain insights into the model's robustness and generalizability, which can guide our selection of the optimal threshold."
      ],
      "metadata": {
        "id": "1c3YNE4zhQ4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_loader(model, loader, threshold : int = None):\n",
        "\n",
        "    model.eval()\n",
        "    probs_all = []\n",
        "    targets_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            probs = F.sigmoid(output) #prob\n",
        "\n",
        "            probs_all.append(probs)\n",
        "            targets_all.append(target)\n",
        "\n",
        "    probs = torch.cat(probs_all, dim=0)\n",
        "    targets= torch.cat(targets_all, dim=0)\n",
        "\n",
        "    if threshold is not None:\n",
        "        preds = torch.where(probs > threshold, 1.0, 0.0)\n",
        "        return preds.cpu(), targets.cpu()\n",
        "    else:\n",
        "        return probs.cpu(), targets.cpu()"
      ],
      "metadata": {
        "id": "YrT3xtAUsF89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_probs, val_targets = predict_loader(model, val_loader)"
      ],
      "metadata": {
        "id": "GhcvSXtPs-tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "\n",
        "average_precision = average_precision_score(val_targets, val_probs) # AP : Area under Precision-Recall curve\n",
        "print(f'average_precision of this model: {average_precision : .4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C021uuA0u10P",
        "outputId": "25facf95-a983-4953-ed3c-4441aeaabb4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average_precision of this model:  0.9976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The area under the precision-recall curve is very high, close to 1. This suggests that the model's predicted output is highly confident in its probabilities, and does not rely heavily on the threshold.\n",
        "\n",
        "In cases where an actual positive observation is present, the model is able to predict probabilities that are much higher, such as 0.99999, as opposed to just above the threshold of 0.5 (say 0.5001). This high level of confidence in the model's predictions is indicative of its strong performance and reliability in identifying positive cases.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4NNHeOcAxE_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(val_targets, val_probs)"
      ],
      "metadata": {
        "id": "R1hva2sAwXQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_threshold(targets,probs):\n",
        "    precision, recall, thresholds = precision_recall_curve(targets, probs) \n",
        "    print(f'try computing precision and recall for {len(thresholds)} thresholds')\n",
        "\n",
        "    f1 = (2 * precision * recall)/(precision + recall) #harmonic mean\n",
        "    index = np.argmax(f1) # Obtain the index that has highest f1\n",
        "    return thresholds[index], f1[index]"
      ],
      "metadata": {
        "id": "ZKj4mvlVw5Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, computing Area under Precision and recall curve OR Average Precision\n",
        "\n",
        "optimal_threshold, optimal_f1 = find_optimal_threshold(val_targets, val_probs)\n",
        "print(f'optimal threshold is : {optimal_threshold}, give f1 = {optimal_f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPWfjJ8ewsSX",
        "outputId": "abce8744-c931-4fb1-81f2-c0662dd4a2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "try computing precision and recall for 2724 thresholds\n",
            "optimal threshold is : 0.46949878334999084, give f1 = 0.9874970511913186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our best model achieved a validation F1 score of 0.987473 with the default threshold of 0.5. By changing the threshold to 0.469498, we were able to slightly increase the score by 2.4e-5.\n",
        "\n",
        "As a result, we will use this optimal threshold during inference, as it provides the best trade-off between precision and recall for our model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RWLAn4KpzK6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix"
      ],
      "metadata": {
        "id": "Iwzd2LaQAT7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix plotter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def cm_plotter(model,loader,threshold = 0.5,set_name = 'train'):\n",
        "\n",
        "    preds, targets = predict_loader(model, loader, threshold = threshold)\n",
        "    preds, targets = preds.numpy(), targets.numpy()\n",
        "\n",
        "    conf_mat = confusion_matrix(targets, preds)\n",
        "    class_total = np.sum(conf_mat, axis=1)\n",
        "    conf_mat_percent = conf_mat / class_total[:, np.newaxis] * 100\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    heatmap = sns.heatmap(conf_mat_percent, annot=True, fmt='.2f',\n",
        "                      xticklabels=['Do not Have MI (0)', 'Have MI (1)'], \n",
        "                      yticklabels=['Do not Have MI (0)', 'Have MI (1)'],\n",
        "                      cmap=\"Reds\")\n",
        "    cbar = heatmap.collections[0].colorbar\n",
        "    cbar.set_label('Percentage (%)')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title(f'Confusion matrix of {set_name} set')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nzEw9BmcBdsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for set_,loader_ in zip(['train','val','test'],[train_loader,val_loader,test_loader]):\n",
        "    cm_plotter(model,loader_,optimal_threshold, set_)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9Xhh4of9FpIy",
        "outputId": "af18a58a-1a5e-4176-8ba3-64c79e65e03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAGDCAYAAACMZdGQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzK0lEQVR4nO3dd5yU1d3//9ebJqKCIogFFBUworktscUW1KixRZOoiS1YsUVsiSW3P0ti8o3eJhG7iCJ2Ym+xRA3YYsHeS2yoNAtFRViWz++P61ocyu7O7uzZ2Rnezzzm4VzlXOdcw2Q/c8p1jiICMzMzS6NduQtgZmZWzRxozczMEnKgNTMzS8iB1szMLCEHWjMzs4QcaM3MzBJyoLVWIWlJSXdLmibp5hKus5+kB1uybOUiaStJbyW4bot81o3kkaTsZtXIgdbmI2lfSeMkfSVpgqT7JG3ZApfeE+gFLB8RezX3IhFxfUTs0ALlSUpSSOrX0DkR8VhErJUg+wY/a0lnSrqulAwSlr1ekq6WdHZr5mnWEhxobR5JJwDnA38m+0O9KnAJsHsLXH414O2ImNMC16p4kjokvHxJn7Uy/ttg1lIiwi+/ALoBXwF7NXDOEmSB+NP8dT6wRH5sEPAxcCIwGZgAHJQfOwuYDdTkeRwCnAlcV3DtvkAAHfLtA4H3gBnA+8B+BfsfL0i3OfAsMC3/7+YFx8YAfwSeyK/zINCjnnurK/9JBeXfA9gZeBv4Avh9wfmbAP8BpubnXgR0yo89mt/L1/n9/rLg+icDE4Fr6/bladbM89gw314ZmAIMqqe8a+f3NxV4DfhpfZ/1Aul+ssDxlwo+qz/ln9VMoB9wEPBG/tm9Bxy+4OdVsP0B8Fvg5fzfYjTQuZ6y9wPG5ud9BowuOPY94F/5Z/EWsHe+f0he5tl5ue8u9/9n/PKr2FfZC+BX23jlf4DnkAe6es75A/AUsALQE3gS+GN+bFCe/g9AxzxAfQMslx8/k/kD64LbffPg1AFYCpgOrJUfWwlYJ39/IHmgBboDXwIH5On2ybeXz4+PAf4LDACWzLf/Us+91ZX/9Lz8h5EFuhuAZYB18gC0en7+D4DN8nz75gHpuILrBdBvEdc/h+wHy5KLCFaHAa8DXYAHgPPqKWtH4F3g90AnYFuyYLjWoj7bRaRf6Hj+2XyU32eHPI9dyH4ACPhR/u+5YcH9LBhonyH7gdA9/zyOqCf/G4H/JWtR6wxsme9fChhPFuA7ABuQBeKB+fGrgbPL/f8Vv/xq6svNQ1ZneeCzaLi5cT/gDxExOSKmkNWeDig4XpMfr4mIf5LVPJrbjzcXWFfSkhExISJeW8Q5uwDvRMS1ETEnIm4E3gR2KzhnZES8HREzgX8A6zeQZw3wp4ioAW4CegDDImJGnv/rwHoAEfFcRDyV5/sBcDlZMGrsns6IiFl5eeYTEVeQBdCnyX5c/G8919kMWJrsR8PsiHgEuIfsh0Ypro6I1/J7qomIeyPiv5EZS9YisFUD6S+IiE8j4gvgbur/rGvImrdXjohvI+LxfP+uwAcRMTIvwwvArUCz+/TN2gIHWqvzOdCjkb7DlYEPC7Y/zPfNu8YCgfobsoDQJBHxNVlz6xHABEn3SvpeEeWpK9MqBdsTm1CezyOiNn9fFwgnFRyfWZde0gBJ90iaKGk6Wb92jwauDTAlIr5t5JwrgHWBCyNiVj3nrAyMj4i5BfsWvO/mGF+4IWknSU9J+kLSVLJWiobusdjP+iSyWvIzkl6TdHC+fzVgU0lT615kP+5WbPqtmLUdDrRW5z/ALLJ+yfp8SvbHsM6q+b7m+JqsibTOfH9MI+KBiNierGb3JlkAaqw8dWX6pJllaopLycrVPyK6kjXjqpE0DS6VJWlpsn7vK4EzJXWv59RPgT4LDFhqyn3XV455+yUtQVabPA/oFRHLAv+k8XtsPPOIiRFxWESsDBwOXJKP0B4PjI2IZQteS0fEkY2U26xNc6A1ACJiGln/5MWS9pDURVLHvFZzbn7ajcBpknpK6pGf39zHRF4Etpa0qqRuwKl1ByT1krS7pKXIgv9XZM2uC/onMCB/JKmDpF8CA8maUVNbhqwf+au8tn3kAscnAWs08ZrDgHERcShwL3BZPec9TVZjPCn/NxpE1lx+U5H5TAL6NjKyuBNZX/IUYI6knYAWeaxK0l6SeuebX5IF0Llk/24DJB2Q31dHSRtLWrug3E39TM3KzoHW5omIvwInAKeR/YEdD/wGuCM/5WxgHNnI0leA5/N9zcnrX2QjU18GnmP+4NguL8enZKNPf8TCgYyI+JysX+9Esqbvk4BdI+Kz5pSpiX4L7Es2COkKsnspdCYwKm8C3buxi0nanWxAWt19ngBsKGm/Bc+NiNlkgXUnssFClwC/jog3iyx73SQWn0t6flEnRMQMYChZv/aXZPd6V5HXb8zGwNOSvsqveWxEvJfnuQPwK7J/+4l8N3gMspr+wPwzvaOFymKWnCLcGmNmZpaKa7RmZmYJOdCamZkl5EBrZmaWkAOtmZlZQg60ZmZmCaVcQaQkp3Rc1sOhrSr85TMv22pVoFuvkicrqc8R6lrS3/vLYnqysrWENhtozcxs8VDtTasOtGZmVlbt1KYrpCVzoDUzs7Kq9hpttd+fmZlZWblGa2ZmZdWuuluOHWjNzKy8qr1p1YHWzMzKqtoHQ1X7DwkzM7Oyco3WzMzKqtprfA60ZmZWVh4MZWZmlpBrtGZmZgnJg6HMzMysuVyjNTOzsqr2Gp8DrZmZlZUHQ5mZmSXkGq2ZmVlCnhnKzMzMms01WjMzK6tqr/E50JqZWVl5MJSZmVlC1V6jrfb7MzMzKyvXaM3MrKzaUd1txw60ZmZWVu6jNTMzS6ja+zAdaM3MrKyqvUZb7T8kzMzMyso1WjMzKysPhjIzM0uo2puOHWjNzKysqr0P04HWzMzKqtprtNX+Q8LMzKysXKM1M7Oy8mAoMzOzhKq96diB1szMyqrK46z7aM3MzFJKVqOV1Bv4FbAVsDIwE3gVuBe4LyLmpsrbzMwqh5uOm0HSSGAV4B7gHGAy0BkYAPwE+F9Jp0TEoynyNzOzyuHBUM3z14h4dRH7XwVuk9QJWDVR3mZmVkFco22GwiArqXu+74uC47OBd1PkbWZmlaXaBwsluT9Jq0q6SdIU4GngGUmT8319U+RpZmbWFqX6ITEauB1YMSL6R0Q/YCXgDuCmRHmamVkFUomvti5VoO0REaMjorZuR0TURsRNwPKJ8jQzswrUTirp1dalGgz1nKRLgFHA+HxfH2Aw8EKiPM3MrAK1/VBZmlSB9tfAIcBZZI/5AHwM3A1cmShPMzOzNifVqOPZwKX5y8zMrF7VXqNNNer4tLrHeuo5vq2kXVPkbWZmlaXaB0Olajp+Bbhb0rfA88AUspmh+gPrAw8Bf06Ut5mZVRBVwICmUqRqOr4TuFNSf2ALskd7pgPXAUMiYmaKfM3MrPJUd5hNvExeRLwDvJMyDzMzs7bM69GamVlZVfsUjA60ZmZWVlXeRetAa2Zm5aUq76VNtR7thUDUdzwihqbI18zMKk91h9l0Ndpxia5rZmZWUVI93jMqxXXNzKz6uEbbDJLuauh4RPw0Rb5mZlZ52lV5pE3VdPxDslV7biRb+L3KP0YzM2suD4ZqnhWB7YF9gH2Be4EbI+K1RPmZmZm1SUmeE84Xeb8/IgYDmwHvAmMk/SZFfmZmVrm8qEAzSVoC2IWsVtsXuAC4PVV+ZmZWmVpjwgpJxwOHkj16+gpwENk8/DcBywPPAQfky7y2qFTL5F0D/AfYEDgrIjaOiD9GxCcp8jMzs8qVukYraRVgKLBRRKwLtAd+BZwD/D0i+gFfAoe03F19J9UUk/uTLYl3LPCkpOn5a4ak6YnyNDOzCtQOlfQqUgdgSUkdgC7ABGBb4Jb8+Chgj5a+t7qMW1xEVPsc0WZm1kZIGgIMKdg1PCKG121ExCeSzgM+AmYCD5I1FU+NiDn5aR8Dq6Qon+c6NjOzsiq1izYPqsPrOy5pOWB3YHVgKnAz8JMSsy2aA62ZmZVVKwyG+jHwfkRMyfLTbcAWwLKSOuS12t5AknFEbuI1M7OyaoXHez4CNpPURZKA7YDXgX8De+bnDAbubIn7WVDyQCtpNUk/zt8vKWmZ1HmamVnlUIn/a0xEPE026Ol5skd72pE1NZ8MnCDpXbJHfK5McX9Jm44lHUbWQd0dWJOsan4Z2a8JMzOzVhERZwBnLLD7PWCT1Hmn7qM9muwmngaIiHckrZA4TzMzqyBeVKA0syJitvKe7vz5pXoXhDczs8VPlcfZ5H20YyX9nuwh4e3JhlTfnThPMzOrINU+13HqQHsKMIWs8/lw4J/AaYnzNDMzazNSNx3vAVwTEVckzsfMzCpUta9Hm7pGuxvwtqRrJe2a99GamZnNI5X2auuSBtqIOAjoR9Y3uw/wX0kjUuZpZmaVpV2Jr7YueQ0zImok3Uc22nhJsubkQ1Pnuzja4pgj2PjgXyOJZ666hicuuBSAzY8ewmZHHErU1vLmfQ9y36kLPkoGJ7/zMrO+msHc2rnMnTOHizbbBoAll1uWfW8YyXKrrcqXH37EDfscyMyp01r1vmzxNWvWLPY7/Bhmz66htraWHbcbxNAhB893zm333Me5F1xCr549Adh/r5+z1x67AnD7Pfdx6chrADjyoF/zs113at0bsKJUQKW0JKknrNgJ+CUwCBgDjAD2Tpnn4qrXOmuz8cG/5uLNt6N29mwOuvdW3rz3frr17s3au+3MsB9sSe3s2SzVs0e91xj+49345vMv5ts36KTjefeRsYz9v/P50e+O40cnHc/9vz8z8d2YZTp16sSoS85nqS5dqJkzh30PO5qtf7gp639/nfnO23n7bTn9d8fPt2/qtOlcNOJqbh11BZL4+a8PZdutt6RbV09OZ60rda3718AdwFoRcWBE/LNgSSJrQSt8bwDjn32OmpkzmVtby/uPPsE6e+zGZocfzNhz/07t7NkAfD3lsyZdd+BuO/P8tTcC8Py1N7LOT3dp8bKb1UcSS3XpAsCcOXOYM2cOKrJT7vGnnmGLTTdi2W5d6dZ1GbbYdCMe+8/TKYtrzSSppFdbl7qPdp+IuCMiZqXMx2Dia2/Qd4sf0qX7cnRccknW2ml7lu3Tmx4D+tF3y8056omHGPLwvfTeaINFpo8IDrnvdn7z9Bg2OXTwvP1L91qBGRMnATBj4iSW7uWJvax11dbWsvt+B7P5jruz+SYbsd66Axc658FHxrLbvgcy9JT/jwmTsu/rpClTWHGF776vvVZYgUlTprRaua141f4cbeqm482AC4G1gU5Ae+DriOhaz/nzFu/dsd2SrN+uU8riVZUpb77N2POGcfB9t1Pz9TdMeOkV5tbW0q59e7p0X45LtvgxvTfekH1vuJpzB6y3UPrLBv2E6Z9OYKmePTj0/juY8uY7vP/4kwtnFJ7Yy1pX+/btufP6q5g+YwZHn3Qab//3PQasuca849tsuTm77rAdnTp14qbb7uTkM//MNZcOK2OJrakqIViWInXT8UVko43fIRsIdShwcX0nR8TwiNgoIjZykG26cSOv5aJNB3H5tjsz88upfPbOu0z75FNevT2bjOvjZ58n5s5lqR7LL5R2+qcTgKxp+bU77qH3xhsC8NWkySyzYi8AllmxF19Ndo3AyqPrMsuw6Q82WKj5d7llu9GpU/b3Yq/dd+W1N98GoFfPnkycPHneeZMmT543YMraFjcdlygi3gXaR0RtRIykFVe1X9zUDXTq1qc36+yxGy/eeAuv33Uvaw7aCoAe/dekfaeOfP3Z5/Ol69ilC52WXnre+/7bb8Ok194A4PV77mPDA/YBYMMD9uH1u//ZWrdjxhdfTmX6jBkAfPvtLJ58ehxrrLbafOdM/uy7cQePPPoEa66eHd9ys014/KlnmTZ9BtOmz+Dxp55ly82SL9RitpDUj/d8I6kT8KKkc4EJVMZjTxVp/39cQ5fu3Zk7Zw53Dv0t306bxriR17HniIs47oUnqa2p4eaDjwJgmZVW5BeXX8DVP92bZXr15IBbrgegXfv2vHjTLbz94MMAjD337+x749VsfNABfPnReG7Y58By3Z4thiZ/9jmnnPVnaufWEnODn/x4G7bZanOGXX4l6669FtttvSXXjr6VRx59gvbt29OtW1f+3+mnArBst64cdchg9jxwCABHH3ogy3ZbZK+VlVm1r96jSNjnJmk1YDLQETge6AZcktdyG3RKx2XdGWhV4S+fvVXuIpiVrluvZOHwxT59S/p7v/74D9p0qE5ao42ID/O3M4GzUuZlZmaVqQK6WUuSJNBKeoUG1p2NiP9Jka+ZmVlbk6pGu2v+XwH3AjsnysfMzCqca7TNUNBkjKRZhdtmZmaFKuERnVJ42TozMyurKo+zyfpoNyzYXFLSBhRM/hERz6fI18zMKo9rtM3z14L3E4G/FWwHsG2ifM3MzNqUVH2026S4rpmZVZ8qr9C6j9bMzMqrXZVHWgdaMzMrqyqPsw60ZmZWXtU+GCrpBP+SHi5mn5mZWbVK9XhPZ6AL0EPScnz3aE9XYJUUeZqZWWVSla/plqrp+HDgOGBloPCZ2elki8GbmZkB1d90nOrxnmHAMEnHRMSFKfIwM7PqUOVxNvlgqMslDQW2zrfHAJdHRE3ifM3MzNqE1IH2ErJF3y/Jtw8ALgUOTZyvmZlVCDcdl2bjiFivYPsRSS8lztPMzCpIlcfZ5IG2VtKaEfFfAElrALWJ8zQzswrimaFK8zvg35LeI3vEZzXgoMR5mplZBanyOJs20EbEw5L6A2vlu96KiFkp8zQzM2tLWmMKxh8AffO81pdERFzTCvmamVkF8GCoEki6FlgTeJHv+mYDcKA1MzPATcel2ggYGBGROB8zM6tQDrSleRVYEZiQOB8zM6tQalfdkTZ1oO0BvC7pGWDeIKiI+GnifM3MzNqE1IH2zMTXNzOzCuem4xJExNiU1zczs8rnCSvMzMwSqvI4S5Uvt2tmZlZeyWu0kjoBA/LNt7xEnpmZFfKEFSWQNAgYBXxANtdxH0mDI+LRlPmamVnlqPI4m7xG+1dgh4h4C0DSAOBGsmkZzczMXKMtUce6IAsQEW9L6pg4TzMzqyBVHmeTB9pxkkYA1+Xb+wHjEudpZmbWZqQOtEcCRwND8+3HgEsS52lmZhXETccliIhZ+Qo+10bElJR5mZlZZVKVP2ia5PaUOVPSZ8BbwFuSpkg6PUV+ZmZWuSSV9GrrUv2OOB7YAtg4IrpHRHdgU2ALSccnytPMzCpRO5X2auNSBdoDgH0i4v26HRHxHrA/8OtEeZqZmbU5qfpoO0bEZwvujIgpfrzHzMzm0wrNv5KWBUYA6wIBHEzWtTka6Es2sdLeEfFlPek3ArYCVgZmkq23/q/6zi+UqkY7u5nHzMxsMdNKfbTDgPsj4nvAesAbwCnAwxHRH3g4316wbAdJeh44FViSLDhPBrYEHpI0StKqDWWcqka7nqTpi9gvoHOiPM3MrBIl7meV1A3YGjgQICJmA7Ml7Q4Myk8bBYwBTl4geRdgi4iYWc+11wf6Ax/Vl3+SQBsR7VNc18zMrBlWB6YAIyWtBzwHHAv0iogJ+TkTgV4LJoyIixu6cES82FjmVf70kpmZtXlSSS9JQySNK3gNWSCHDsCGwKURsQHwNQs0E0dEkPXdNlJU7SZpjKSnJB1VzO154XczMysrldh0HBHDgeENnPIx8HFEPJ1v30IWaCdJWikiJkhaiazvdf6ySesvUGs9ANiGrCv0JYqY7dA1WjMzK68Sa7SNiYiJwHhJa+W7tgNeB+4CBuf7BgN3LiL5kZKukLRivj0eOI1scNSnxdyea7RmZlZWpdZoi3QMcL2kTsB7wEFklc1/SDoE+BDYe8FEEXF43q97uaTngNOBH5INkjqvmIwdaM3MrOrlzb8bLeLQdkWkfQnYXdJuZLXeayLimmLzdtOxmZmVV+Km49KKpiMkPSnpSWAp4CfAspIekLR1MddwoDUzs/Jq23MdHxURm5MNgPpdRMyJiAuAXwF7FHMBNx2bmVlZtfEVeD6R9HuyPtk363bmUy+eUMwFXKM1MzOr3+7AK8DjNHNRHNdozcysvNr2UncrR8Td9R1UVh1fJSI+ru8cB1ozMyuvtt10/H+S2pGNNn6ObCrHzkA/sn7b7YAzyCbFWCQHWjMzKyu14U7MiNhL0kBgP7Kl9VYCviFb/eefwJ8i4tuGruFAa2Zm5dW2a7RExOvA/zY3fRv+HWFmZlb5XKM1M7OyaqUpGMvGgdbMzMqrjTcdl8pNx2ZmVl5te2YoIHuMR9L+kk7Pt1eVtEkxaR1ozcysrJQt3t7sVyu5hGzVnn3y7RnAxcUkdNOxmZlZ4zaNiA0lvQDZFIz5knuNcqA1M7PyqozBUDWS2gMBIKknMLeYhPUGWkkX1l1wUSJiaBMLaWZmtrDKGAx1AXA7sIKkPwF7AqcVk7ChGu24FiiYmZlZg9r46j0ARMT1kp4jm3JRwB4R8UYxaesNtBExqoXKZ2ZmVtEkdQcmAzcW7OsYETWNpW20jzZvhz4ZGEg2kTIAEbFts0prZmZWqDL6aJ8H+gBfktVolwUmSpoEHBYRz9WXsJjHe64nmzx5deAs4APg2dLKa2ZmlqmQx3v+BewcET0iYnlgJ+Ae4CiyR3/qVUygXT4irgRqImJsRBwMuDZrZmYtowImrAA2i4gH6jYi4kHghxHxFLBEQwmLebynrv15gqRdgE+B7s0tqZmZ2XwqYDAUWQw8Gbgp3/4lMCl/5KfBx3yKCbRnS+oGnAhcCHQFji+hsGZmZpVmX7IF3u/It5/I97UH9m4oYaOBNiLuyd9OI1tN3szMrMVUwuo9EfEZcEw9h99tKG0xo45HsoiJK/K+WjMzs9JUQNNx/gTOScA6NPEJnGKaju8peN8Z+BlZP62ZmVnpKqBGS/YEzmhgV+AIYDAwpZiExTQd31q4LelG4PGml9HMzGxhlTAzFPkTOJKOjYixwFhJRT3q2pxFBfoDKzQjnZmZWaVq9hM4xfTRzmD+PtqJZDNFJfWXaR+mzsKsVRyxVJ9yF8GsZJfF9HQXr4ym40U9gXNcMQmLaTpepqSimZmZNaQymo6/jIhpFDyBI2mLYhI2OjOUpIeL2WdmZtYsUmmv1nFhkfsW0tB6tJ2BLkAPScuRTaIMWXV5laaW0MzMrNJI+iGwOdBT0gkFh7qSTVbRqIaajg8na39eGXiO7wLtdOCiphbWzMxskdp203EnYGmyeFnYlTqdbPH3RjW0Hu0wYJikYyKiqOqxmZlZk7UrZn2b8ih4lOfqiGjWKN1iHu+ZK2nZiJgKkDcj7xMRDS4LZGZmVpS2XaOts4Sk4UBfCmJnS80MdVhEXFxw0S8lHUYj6++ZmZkVpTIC7c3AZcAIoLYpCYsJtO0lKSICIF8SqFOTi2hmZla55kTEpc1JWEygvR8YLenyfPtw4L7mZGZmZraQyqjR3i3pKOB2YFbdzoj4orGExQTak4EhZJMoA7wMrNiMQpqZmS2sDQ+GKjA4/+/vCvYFsEZjCYuZGWqupKeBNckWt+0B3NpwKjMzsyJVQI02IlZvbtqGJqwYAOyTvz4jWx6IiPDi72Zm1nIqINBK6gKcAKwaEUMk9QfWioh7Gkna4BSMbwLbArtGxJb5s7RNGmllZmZWJUYCs8lmiQL4BDi7mIQNBdqfAxOAf0u6QtJ2fDc7lJmZWcuojLmO14yIc8mXy4uIbygyJjY0M9QdwB2SlgJ2J5uOcQVJlwK3R8SDJRbazMysUgZDzZa0JPmysZLWpGD0cUMavbuI+DoiboiI3YDewAu0wnq0Zma2mKiMGu0ZZI+79pF0PfAwcFIxCYt5vGeeiPgSGJ6/zMzMFgsR8S9JzwObkTUZHxsRnxWTtiLq62ZmVsUqoEYr6Wdks0Pdm480niNpj2LSOtCamVl5VUCgBc6IiGl1G/lCO2cUk7BJTcdmZmYtTZUxGGpRhSwqhjrQmplZeVXAhBXAOEl/A+pWszsaeK6YhBXxM8LMzKzMjiGbsGI0cBPwLVmwbZRrtGZmVl5tvEabLw97T3OnIHagNTOz8mrjgTYiaiXNldStcEBUsRxozcysvCpjMNRXwCuS/gV8XbczIoY2ltCB1szMyquN12hzt+WvJnOgNTMza0REjMrnOl41It5qStqKqK+bmVkVq4AJKyTtBrxINt8xktaXdFcxaR1ozcysvFoh0EpqL+kFSffk26tLelrSu5JGS+rUyCXOBDYBpgJExIvAGsXk7UBrZmbl1a5daa/iHAu8UbB9DvD3iOgHfAkc0kj6mkWMOJ5b1O0VW0IzM7NKJKk3sAswIt8WsC1wS37KKGCPRi7zmqR9gfaS+ku6EHiymPwdaM3MrLzSNx2fT7Z2bF0NdHlgakTMybc/BlZp5BrHAOuQLfZ+AzANOK6YzD3q2MzMyqvEAU2ShgBDCnYNj4jh+bFdgckR8ZykQc24dmfgCKAf8Arww4IAXRQHWjMzK68SJ6zIg+rweg5vAfxU0s5AZ6ArMAxYVlKHPGj2Bj6pJ/0ooAZ4DNgJWJsia7J13HRsZmbllbDpOCJOjYjeEdEX+BXwSETsB/wb2DM/bTBwZz2XGBgR+0fE5fn5Wzf19hxozcxscXQycIKkd8n6bK+s57yaujdNbTKu46ZjMzMrr1aadCIixgBj8vfvkT0X25j1JE3P3wtYMt9Wdpno2tgFHGjNzKy82vBcxxHRvtRrONCamVl5VcbqPc3mQGtmZuXVhmu0LaG6f0aYmZmVmWu0ZmZWXlVeo3WgNTOz8lJ1N6460JqZWXm1q+4abXX/jDAzMysz12jNzKy83HRsZmaWkAdDmZmZJeQJK8zMzBKq8hptdf+MMDMzKzPXaM3MrLw8GMrMzCyhKm86dqA1M7PyqvLBUNV9d2ZmZmWWrEYrqTOwK7AVsDIwE3gVuDciXkuVr5mZVRg3HTedpLPIguwY4GlgMtAZGAD8JQ/CJ0bEyynyNzOzCuLBUM3yTEScUc+xv0laAVg1Ud5mZlZJqnxRgSSBNiLubeT4ZLJarpmZLe6qvEbb6ncnaXhr52lmZlYuqfpou9d3CNg5RZ5mZlahPBiqWaYAH5IF1jqRb6+QKE8zM6tEVd50nCrQvgdsFxEfLXhA0vhEeZqZWSWq8sFQqX5GnA8sV8+xcxPlaWZmlUgq7dXGpRp1fHEDxy5MkaeZmVlblKRGK2nLRo53lbRuirzNzKzCqF1przYuVR/tLySdC9wPPEc2OKoz0A/YBlgNODFR3mZmVkmqvI82VdPx8fkjPr8A9gJWIpvr+A3g8oh4PEW+ZmZWgSqgVlqKZIsKRMQXwBX5y8zMbLHk9WjNzKy8KmDkcCkcaM3MrLzcdGxmZpaQB0M1naSfN3Q8Im5Lka+ZmVUg12ibZbcGjgXgQGtmZouFVI/3HJTiumZmVoU8GKrpJJ3Q0PGI+FuKfM3MrAK1c9Nxc5wHvAjcB8xi/uXyzMzMvuMabbNsAOwD7EI2BeONwMMREYnyswU8+sR/+NP//ZW5c+ey1x67M+TgwfMdv/HmW7nhH7fQrl07unTpwh9PO5V+a67BE089zV8vuJiamho6duzI7447hh9usnGZ7sIWR9sOPZItDhuMJB6/YhSPDLuEVf5nXfa77HyWWHopPv/gI67a71C+nTFjobRLduvGASMuZOV1BxIRXHPw0bz/1DP0Xu/77HvZ+XTsvARz58zhxqNO5INnnyvD3dkiVflgKKWOfZI2Jwu6PwZOjoi7ikr4zTQH5Waqra1lxz32ZOSlF9Gr1wrsud9g/vb/zqbfmmvMO+err75i6aWXBuDhMY9yw823cOXFF/D6m2+xfPfu9FqhJ2+/+18OOWoojz14b7lupSocsVSfchehYqy8ztocctNI/rLJNtTOns0x99/GDUcczyE3XsWtv/1f3nn0CTY/aH+WX70vd59+9kLpB199Ge8+9iRPXHkN7Tt2pFOXLsycNo2hD9zBw3+/mNfu/xfr7rQDO5x0LH/bZpcy3GHluiymJ6t21j4wsqS/9+13PKhNV4mT/oyQ1JOsdvt94GNgcsr8LPPyq6+xWp/e9Om9Cp06dmSXHXfg4TGPzndOXZAFmDlzJspb9wd+by16rdATgP5rrsGsWbOYPXt26xXeFmsrrr0WHzw9jpqZM5lbW8s7Y59gg5/vRq8Ba/LOo08A8Ma//s2Gv/jpQmk7d+1K/60354krrwGgtqaGmdOmARARdO66THZet65M/XRiK92RFcXr0TadpIOBvclW7LkF2DsiHGRbyaTJU1ixV6952716rcDLr7620HnXj76ZkdfdQE1NDaMuv2Sh4w889AgDv7cWnTp1Slpeszqfvvo6u//pdJbq3p3ZM2ey7s478OG4F/j0tTdZb/ddeOnOe9lwrz1Yrs8qC6XtsfpqfDXlcwaPvJRV1luXj557kX8cezKzv/mGm487maEP3M4vzjubdu3ace7m25fh7qxeVT4YKtXdjQBWBmYAOwIjJN1V96ovkaQhksZJGjf8qqsTFc3q7PfLvXjo7tv57bG/4dIRV8137J3//pfzLriIP5x2aplKZ4ujiW++zQPn/J2hD97O0PtvY/yLLzO3tpZrDj6KHx11GKeOG0vnZZZhzuyahdK269CBPhuux9hLr+TPG27F7K+/YcdTsgcgtj7yUG4+/lR+v+pAbj7+VA648qLWvjVriGu0zbJNcxJFxHBgOOA+2hL0WqEnEydNmrc9adJkevXsWe/5u+y4A2f++Zx52xMnTeI3J5zEOX88k1X79E5aVrMFPXnVtTx51bUA7P6n05n68adMeusdLthxDwBW6N+P7++y40Lppn78CVM//oQPnhkHwPO33DEv0P5w8D7849iTAHju5tvZf8SFrXAnZpkkNdqIGNvQK0We9p3vrzOQDz4az/hPPmF2TQ33PvAg2w7aar5zPvjwo3nvxzz2BKv1yQbsTJ8xgyHHHM+JQ3/DD9Zfr1XLbQawTM8eACzXpzcb/PynPHPDzfP2SWLn037Ho5dduVC66ZMm88X4T+g1oB8A39tuEBNefxOAqZ9OZMCPtgRgrW1/xOR3/tsat2LFUrvSXm2cFxWoQh06dOD0k3/HoUcNpXbuXH6x+270X3NNhl1yOesOXJvtBm3NdaNv5j9PP0OHDh3o2rUr5/zxDACuu+kffDT+Yy4ePoKLh48A4KpLL2T57t3LeUu2GBly63UsvXx3amtquPHoE5k5bRrbDj2SHx19GAAv3HYXT468DoBuK63IASMu4qJd9gRg9DG/4+DrR9C+Uyc+e+8DrjnoKACuO+wY9h52Du07dKDm21lcP+TY8tycLVoFNP+WIvnjPc3mpmOrEn68x6pB0sd7xtxU2uM9g37VpiN1q9RoJXWJiG9aIy8zM6swVb5MXurnaDeX9DrwZr69nqSFnyMxMzOrUql7kf9O9njP5wAR8RKwdeI8zcyskngwVGkiYrzm7+iuTZ2nmZlVkCofDJU60I7P5zoOSR2BY4E3EudpZmaVpAJqpaVIHWiPAIYBqwCfAA8CRyfO08zMKohcoy2JImK/xHmYmZm1WakD7ROSPgBGA7dGxNTE+ZmZWaWp8qbjpHcXEQOA04B1gOcl3SNp/5R5mplZhanyUcfJSxgRz0TECcAmwBfAqNR5mplZBWmn0l6NkNRH0r8lvS7pNUnH5vu7S/qXpHfy/y6X5PZSXLSOpK6SBku6D3gSmEAWcM3MzFrLHODEiBgIbAYcLWkgcArwcET0Bx7Ot1tc6j7al4A7gD9ExH8S52VmZpUocfNvREwgq+gRETMkvUH2NMzuwKD8tFHAGODkls4/daBdI9rsqgVmZtYmlPh4j6QhwJCCXcPz9c0XdW5fYAPgaaBXHoQBJgK9SipIPVIH2h6STiIbDNW5bmdEbJs4XzMzqxQl1mjzoLrIwDpfNtLSwK3AcRExvfD53YgISUkqhqkHQ11PtqDA6sBZwAfAs4nzNDOzSiKV9ioqC3UkC7LXR8Rt+e5JklbKj68ETE5xe6kD7fIRcSVQExFjI+JgwLVZMzNrNcqqrlcCb0TE3woO3QUMzt8PBu5MkX/qpuOa/L8TJO0CfAp0T5ynmZlVkvTPwm4BHAC8IunFfN/vgb8A/5B0CPAhsHeKzFMH2rMldQNOBC4EugLHJ87TzMwqSeKF3yPicaC+TLZLmjmJA21E3JO/nQZskzIvMzOrUBUwu1MpkgRaSRcC9Y7eioihKfI1M7MK5NV7mmVcwfuzgDMS5WNmZtamJQm0ETFvPmNJxxVum5mZzcdNxyXzzFBmZlY/Nx2bmZkl5Bpt00mawXc12S6SptcdIpvpqmuKfM3MzNqaVH20y6S4rpmZVaF2rtGamZklI/fRmpmZJeQ+WjMzs4SqvEZb3T8jzMzMysw1WjMzKy83HZuZmSVU5U3HDrRmZlZefrzHzMwsoSqv0Vb3zwgzM7Myc43WzMzKy4OhzMzMEqrypmMHWjMzK7PqDrTVXV83MzMrM9dozcysvNx0bGZmlpADrZmZWUoOtGZmZulUeY3Wg6HMzMwSco3WzMzKq7ortA60ZmZWbtUdaR1ozcysvKq8j9aB1szMyqvKA60HQ5mZmSXkGq2ZmZVZdddoHWjNzKy8qrzp2IHWzMzKrLoDrftozczMEnKN1szMystNx2ZmZgk50JqZmaXkQGtmZpaMqrxG68FQZmZmCblGa2Zm5VXlNVoHWjMzKzMHWjMzs3SqvEbrPlozM7OEXKM1M7PyqvIarQOtmZmVmQOtmZlZOq7RmpmZJVTdcdaDoczMzFJyjdbMzMqsuqu0DrRmZlZe7qM1MzNLyIHWzMwspeoOtB4MZWZmlpBrtGZmVl5uOjYzM0vIgdbMzCyl6g607qM1MzNLyDVaMzMrrypvOlZElLsMViaShkTE8HKXw6xU/i5bW+am48XbkHIXwKyF+LtsbZYDrZmZWUIOtGZmZgk50C7e3Kdl1cLfZWuzPBjKzMwsIddozczMEnKgNTMzS8iBtoVIqpX0oqTXJL0k6URJST9fScdJ6lLPsTGSNirY7ivp1cTlCUnXFWx3kDRF0j359oGSLqon7R6STs/fLyFptKR3JT0tqW++//uSrk55D9YyJH21wHa9//YtlN+g/Pt3aMG+9fN9v823r5a0Zz3pz5e0df7+N/l3LyT1KDhnV0l/SHUPVr0caFvOzIhYPyLWAbYHdgLOSJznccAiA22ZfA2sK2nJfHt74JMi054EXJK/PwT4MiL6AX8HzgGIiFeA3pJWbbkiWxV5Fdi7YHsf4KXGEklaHtgsIh7Ndz0B/Bj4cIFT7wV2q+/HrVl9HGgTiIjJZA/Q/0aZzpJGSnpF0guStlkwTf6LfIykWyS9Kel6KZuXTNJ2ebpXJF2V1/iGAisD/5b076aUL6/dPibp+fy1eb7/Jkm7FJx3taQ9JbWX9H+SnpX0sqTDG7j8P4G6a+wD3FhEeQYAsyLis3zX7sCo/P0twHZ1nwVwN/Crom/W2hxJu+UtFS9IekhSL0ntJH0gadmC897Jj/WUdGv+/XtW0hb1XPpDoHOeRsBPgPuKKNIvgPvrNiLihYj4YMGTIhs5OgbYteibNcOBNpmIeA9oD6wAHJ3tiu+TBZ9RkjovItkGZLXUgcAawBb5eVcDv8zTdwCOjIgLgE+BbSJiocCduz5vzn6RLADWmQxsHxEbAr8ELsj3jyavEUjqBGxH9iv+EGBaRGwMbAwcJmn1evK8CfhVXu7/AZ6u57xCWwDPF2yvAowHiIg5wDRg+fzYOGCrIq5p5bVk3Xcv//4VNrk+TlaD3IDs+3JSRMwF7gR+BiBpU+DDiJgEDAP+nn//fgGMaCDfW4C9gM3JvlOziijrFsBzRd6Xv3/WZF5UoHVsCVwIEBFvSvoQGAC8vMB5z0TExwD5H6e+wAzg/Yh4Oz9nFFngPr+IfPeLiHH59foC9+T7OwIXSVofqM3LAtmv/2GSliCrDTwaETMl7QD8T0H/VjegP/D+ghlGxMt5Xvswf3BvyErAlCLPnUxWk7e2bWZErF+3IelAoG7MQG9gtKSVgE589z0aDZwOjCRrtRid7/8xMPC7Rg26Slo6IubrB879I0/3PbLWlM2LKKu/f5aUA20iktYgC2KTm5Cs8Nd3Len+fY4HJgHrkbVqfAsQEd9KGgPsSFbTvSk/X8AxEfFAkde/CzgPGMR3NdGGzCQL3nU+AfoAH0vqkB/7PD/WOT/fKteFwN8i4i5Jg4Az8/3/AfpJ6gnsAZyd729HVgP+trELR8RESTVk4wOOpbhAO5Pse1UMf/+sydx0nED+h+Iy4KK8X+cxYL/82ABgVeCtIi/3FtBXUr98+wBgbP5+BrBMM4rYDZiQN9cdQNbEXWc0cBBZ81hdv9UDwJGSOtbdg6SlGrj+VcBZ+eClYrwB9CvYvgsYnL/fE3gkvptZZQDZoBerXN34bpBc3b9zXR/o7cDfgDciou7H1YPAMXXn5S0xDTkdODkiaossz4Lfv4b4+2dN5kDbcur6pF4DHiL743BWfuwSoJ2kV8gC2YERUUzfEfmv+IOAm/P0c8mCOGTTzt3f1MFQeXkGS3qJrInt64JjDwI/Ah6KiNn5vhHA68Dzyh4RupwGatsR8XHeh1ysR4ENCgY8XQksL+ld4ATglIJztyHrN7bKdSbZ9/k54LMFjo0G9ue7ZmOAocBG+UC814EjGrp4RDwZEXc0oTz3krW+ACBpqKSPyZq4X5ZU2Cfs7581madgtDZB0jDg7oh4qIFzliCrzW+ZD5IyaxGSHgd2jYipDZzTC7ghIrZrtYJZVXCgtTYh/yO2aUTc1cA5/YFVImJMqxXMFgv5KOeZEbHgAMXCczYGaiLixVYrmFUFB1ozM7OE3EdrZmaWkAOtmZlZQg60Zsy3KMSrkm4uZT5bFUxeL2mEpIENnDuobgrMJubxgQomvDeztsuB1ixTtyjEusBsFniEJJ84o8ki4tCIeL2BUwZR3KQKZlahHGjNFvYY2QxFg/LFF+4CXq9vcQVlLpL0lqSHyOa3Jj82b7lCST/JF3F4SdLD+VSVRwDH57XpreqbQF/S8pIeVLYM4wiy2brMrAJ4CkazAnnNdSe+mxVrQ2DdiHhf0hDyxRXyZ3qfkPQg2WIQa5EtBtGLbHKPqxa4bk/gCmDr/FrdI+ILSZcBX0XEefl5N5BNoP+4suUAHwDWJlty8fGI+IOyFZYOSfpBmFmLcaA1yyyZL+QAWY32SrIm3Wciom7S+/oWV9gauDGf8u9TSY8s4vqbkS3S8D5ARHxRTzkWOYF+nsfP87T3SvqyebdpZq3NgdYsM99qMwB5sCucnnKRiytI2rkFy7HICfQLAq+ZVRj30ZoVr77FFR4Ffpn34a5ENh/ugp4Ctla+jq+k7vn+BReGqG8C/UeBffN9OwHLtdRNmVlaDrRmxatvcYXbgXfyY9eQLfc2n4iYAgwBbssXc6ibNP9u4Gd1g6GofwL9s8gC9WtkTcgfJbpHM2thnoLRzMwsIddozczMEnKgNTMzS8iB1szMLCEHWjMzs4QcaM3MzBJyoDUzM0vIgdbMzCwhB1ozM7OE/n/ugn055MZ9DAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has performed exceptionally well, as it consistently predicts patients with Myocardial Infarction as having Myocardial Infarction across all the train, validation, and test sets. This is a highly encouraging outcome, as it suggests that if the model were to be deployed, it would be highly accurate in identifying patients with Myocardial Infarction, with a low risk of misclassification (< ~0.5%)"
      ],
      "metadata": {
        "id": "6b5fErg4111R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstration of prediction"
      ],
      "metadata": {
        "id": "3vesejtHKYnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demonstrate how to predict the input from each set, and also some random generated input with does not satisfy the shape (1,1,187)"
      ],
      "metadata": {
        "id": "E8gR4UvZnro4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "map_dct = {0:'normal',\n",
        "           1:'myocardial infarction'}\n",
        "\n",
        "optimal_threshold = 0.46949878334999084\n",
        "\n",
        "def predict(model, data, threshold = optimal_threshold,map_dct = map_dct):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(data)\n",
        "        probs = F.sigmoid(output)\n",
        "        preds = torch.where(probs > threshold, 1, 0)\n",
        "        preds = map_dct[preds.item()]\n",
        "\n",
        "    return probs.item(), preds"
      ],
      "metadata": {
        "id": "YK9DBtAFkm83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0 #@param {type:\"slider\", min:0, max:2909, step:1}\n",
        "set_ = val_set #@param [\"train_set\", \"val_set\", \"test_set\"] {type:\"raw\"}\n",
        "\n",
        "def predict_on_set(index,set_ = set_, map_dct = map_dct):\n",
        "\n",
        "    data = set_[index]\n",
        "\n",
        "    # X\n",
        "    X = data[0].to(device)\n",
        "\n",
        "    # Y_t\n",
        "    target = data[1].item()\n",
        "    target = map_dct[target]\n",
        "\n",
        "    plt.plot(X.cpu()[0])\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.title(f\"Truth : {target}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Y_pred\n",
        "    prob, pred = predict(model,X.unsqueeze(0))\n",
        "\n",
        "    if target == 'normal':\n",
        "        print(f'predict this as class {pred} with probability {1-prob}')\n",
        "    else:\n",
        "        print(f'predict this as class {pred} with probability {prob}')\n",
        "\n",
        "# Try predict\n",
        "predict_on_set(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "pHFqMos36Y-_",
        "outputId": "2e32c44e-bbf4-41ca-f620-2cdd2b0e94e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict this as class myocardial infarction with probability 0.9999949932098389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IIfF_e8X7_In"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can predict on the generated data which may or may not have the same shape"
      ],
      "metadata": {
        "id": "9aaUAx6o8Jpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X,fix_seq_len = 187):\n",
        "    \"\"\"\n",
        "    Fill to make the input tensor satisfy with tensor shape (1,1,187) bu padding or truncate (last)\n",
        "    \"\"\"\n",
        "    seq_len = X.shape[2]\n",
        "\n",
        "    if seq_len < fix_seq_len:\n",
        "        diff = fix_seq_len - seq_len\n",
        "        X = torch.nn.functional.pad(X, (0, diff)) #pad the sample\n",
        "\n",
        "    elif seq_len > fix_seq_len:\n",
        "        X = X[:, :, :fix_seq_len] # truncate-last on the sample\n",
        "    \n",
        "    assert X.shape == (1,1,187)\n",
        "    return X"
      ],
      "metadata": {
        "id": "09tZP6Ts79bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def predict_unknown(data=None):\n",
        "    if data is None:\n",
        "        random_len = random.randrange(1,187)\n",
        "        data = torch.rand(1,1,random_len)\n",
        "\n",
        "    processed_data = preprocess(data)\n",
        "    processed_data = processed_data.to(device)\n",
        "\n",
        "    plt.plot(processed_data.cpu()[0][0],color=\"red\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.title(f\"Truth : unknown\")\n",
        "    plt.show()\n",
        "\n",
        "    prob, pred = predict(model,processed_data)\n",
        "    if pred == 'normal':\n",
        "        print(f'predict this as class {pred} with probability {1-prob}')\n",
        "    else:\n",
        "        print(f'predict this as class {pred} with probability {prob}')"
      ],
      "metadata": {
        "id": "fbj4BPG9-RsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try random\n",
        "# Always expect positive, sine it is unusual to have this fluctuate\n",
        "predict_unknown()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "O5-cnukQ-3qQ",
        "outputId": "ca8da049-4565-4ebc-c0c1-28981f0cb670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict this as class myocardial infarction with probability 0.9999997615814209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = test_set[1][0]\n",
        "x = x[:,:100] # random cut the first 100 timestep\n",
        "\n",
        "std = 0.02\n",
        "noise = torch.randn(x.shape) * std\n",
        "noisy_x = x + noise\n",
        "\n",
        "# Try on noisy real data\n",
        "predict_unknown(noisy_x.unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "vEKGGhdZAHsh",
        "outputId": "8da83cbd-c7c7-4aef-c601-0acb6070b791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict this as class myocardial infarction with probability 0.5156314373016357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⏭ Deployment"
      ],
      "metadata": {
        "id": "xLMkfYLDEI9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating the trained model into a system that connects with a cardiac monitoring device can aid in the early detection and diagnosis of myocardial infarction. The model can quickly analyze electrocardiogram signals to determine if a patient is experiencing a heart attack, and provide early alerts for medical professionals to initiate timely interventions.\n",
        "\n",
        "Before deployment in a clinical setting, the system will undergo rigorous review and testing by medical experts to ensure its accuracy and reliability. This approach can potentially reduce healthcare costs by minimizing the need for invasive procedures and hospitalizations, while also improving patient outcomes.\n",
        "\n",
        "Overall, the deployment of this model has the potential to significantly enhance the efficiency of healthcare delivery by enabling early detection and diagnosis of myocardial infarction, leading to better health outcomes and potentially saving lives.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xeUhK3EvEFnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/v3bsAbA.png)"
      ],
      "metadata": {
        "id": "e9otiCmV9nR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is my proposal for deploying the project in real-life, despite my limitations in obtaining the actual ECG heart signal. Instead, I plan to develop a Flask API server that can receive data from an ECG machine, and then notify the patient's caregiver, doctor, or nurse. Although I won't be able to perform preprocessing in real-time, I will assume that the data has already been preprocessed as outlined in the research paper (with the exception of zero-padding) which involves approximately 7 steps.\n",
        "\n",
        "\n",
        "The Flask API is hosted here : https://ecg-heartbeat-ai.onrender.com\n",
        "\n"
      ],
      "metadata": {
        "id": "bQJwwnCw9t21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2 ways to obtain the output given the heartbeat signal data: POST and GET.\n",
        "\n",
        "Although POST is more efficient, but for simplicity, I am going to demonstrate using GET method\n",
        "\n",
        "Suppose you have a heartbeat signal with 5 timesteps : [0.43,0.56,0.57,0.46,0.461], then to obtain the output via GET method will be the following\n",
        "\n",
        "https://ecg-heartbeat-ai.onrender.com/predict_ar?beat_input=[0.43,0.56,0.57,0.46,0.461]\n",
        "\n",
        "then you will expect the output in form of dictionary like\n",
        "\n",
        "{'myocardial infarction' : 0.9993}"
      ],
      "metadata": {
        "id": "KKdlN2F7-5S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have a real test case for this task, which the data is actually from the test set, you can check out here : https://github.com/saranpan/ECG-Heartbeat-classifier/blob/main/real_test_case_mi.txt, at the end of the data, you will the actual label of that"
      ],
      "metadata": {
        "id": "g3CQxhjlBrkE"
      }
    }
  ]
}